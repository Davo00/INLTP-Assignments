{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be9f7653",
   "metadata": {
    "id": "be9f7653"
   },
   "source": [
    "**Heidelberg University**\n",
    "\n",
    "**Data Science  Group**\n",
    "    \n",
    "Prof. Dr. Michael Gertz  \n",
    "\n",
    "Ashish Chouhan, Satya Almasian, John Ziegler, Jayson Salazar, Nicolas Reuter\n",
    "    \n",
    "November 13, 2023\n",
    "    \n",
    "Natural Language Processing with Transformers\n",
    "\n",
    "Winter Semster 2023/2024     \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258e9648",
   "metadata": {
    "id": "258e9648"
   },
   "source": [
    "# **Assignment 2: “Sequence Models”**\n",
    "**Due**: Monday, November 27, 2pm, via [Moodle](https://moodle.uni-heidelberg.de/course/view.php?id=19251)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc27ad9e",
   "metadata": {
    "id": "fc27ad9e"
   },
   "source": [
    "### **Submission Guidelines**\n",
    "\n",
    "- Solutions need to be uploaded as a **single** Jupyter notebook. You will find several pre-filled code segments in the notebook, your task is to fill in the missing cells.\n",
    "- For the written solution, use LaTeX in markdown inside the same notebook. Do **not** hand in a separate file for it.\n",
    "- Download the .zip file containing the dataset but do **not** upload it with your solution.\n",
    "- It is sufficient if one person per group uploads the solution to Moodle, but make sure that the complete names of all team members are given in the notebook.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e322e8b0",
   "metadata": {
    "id": "e322e8b0"
   },
   "source": [
    "## **Task 1: Part-of-Speech Tagging with a Bidirectional LSTM**  (2+4+5=11 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ca26ac",
   "metadata": {
    "id": "b4ca26ac"
   },
   "source": [
    "In this task we will be building a sequence tagger that produces an output for every element in an input sequence, using `PyTorch` and `TorchText`, where `TorchText` consists of data processing utilities and popular datasets for natural language.\n",
    "\n",
    "\n",
    "*   **input:** a sequence of text\n",
    "*   **output:** part-of-speech (POS) tag for each token in the input text\n",
    "\n",
    "We tackle this task using a multi-layer bi-directional LSTM (BiLSTM) to predict POS tags using the [Universal Dependencies](https://universaldependencies.org/) English Web Treebank (UDPOS) dataset. This dataset is contained in the `TorchText` library and we do not require an external file for it. The dataset in  `TorchText`  has two different sets of tags, universal dependency (UD) tags and Penn Treebank (PTB) tags. We only focus on the UD tags for this assignment."
   ]
  },
  {
   "cell_type": "raw",
   "id": "169ebf80-f75f-4c3b-a457-4aa88598c58a",
   "metadata": {
    "id": "nnBfLC1P5QXw"
   },
   "source": [
    "%pip install portalocker"
   ]
  },
  {
   "cell_type": "raw",
   "id": "51ff08a8-6195-4550-bd15-84b4ae71badc",
   "metadata": {
    "id": "nT5geerO7d1P"
   },
   "source": [
    "%pip install torchdata"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9cfea99f-dfe6-4d6c-a4a1-795fa3ca0495",
   "metadata": {
    "id": "_pIBtTh3sOQe"
   },
   "source": [
    "%pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "787e2059",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T12:38:33.568458200Z",
     "start_time": "2023-11-27T12:38:28.298408800Z"
    },
    "id": "787e2059"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchtext.datasets import UDPOS\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import random\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# for reproducibility\n",
    "random.seed(77)\n",
    "np.random.seed(77)\n",
    "torch.manual_seed(77)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc29cb37",
   "metadata": {
    "id": "fc29cb37"
   },
   "source": [
    "### Subtask 1: Data Analysis\n",
    "The very basic components of the torchtext library include `vocab`, `word vectors`, and `tokenizer`. Those are the basic data processing building blocks for the raw text string.\n",
    "In this case, we use the tokenizer and the vocabulary. Use the `build_vocab_from_iterator` to create the vocabulary for the text field and add the `<unk>` and `<pad>` tokens to it. Use a minimal frequency of `2`.\n",
    "Also create a vocabulary for the labels (tag field). However, since the tags are predefined elements, you will not need an `<unk>` token.\n",
    " This dataset actually has two different sets of tags, universal dependency (UD) tags and Penn Treebank (PTB) tags. We train our model on the UD tags, which is the second element on the list of outputs (example below).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "IlEFywat4Gva",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T12:38:33.639955500Z",
     "start_time": "2023-11-27T12:38:33.561448700Z"
    },
    "id": "IlEFywat4Gva"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Al', '-', 'Zaman', ':', 'American', 'forces', 'killed', 'Shaikh', 'Abdullah', 'al', '-', 'Ani', ',', 'the', 'preacher', 'at', 'the', 'mosque', 'in', 'the', 'town', 'of', 'Qaim', ',', 'near', 'the', 'Syrian', 'border', '.'], ['PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'ADJ', 'NOUN', 'VERB', 'PROPN', 'PROPN', 'PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'PROPN', 'PUNCT', 'ADP', 'DET', 'ADJ', 'NOUN', 'PUNCT'], ['NNP', 'HYPH', 'NNP', ':', 'JJ', 'NNS', 'VBD', 'NNP', 'NNP', 'NNP', 'HYPH', 'NNP', ',', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'NNP', ',', 'IN', 'DT', 'JJ', 'NN', '.']]\n"
     ]
    }
   ],
   "source": [
    "train_iter = iter(UDPOS(split=\"train\"))\n",
    "print(next(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7HJWFvb3AgTI",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T12:38:35.081357500Z",
     "start_time": "2023-11-27T12:38:33.641961900Z"
    },
    "id": "7HJWFvb3AgTI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 12543\n",
      "Number of validation examples: 2002\n",
      "Number of testing examples: 2077\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(list(UDPOS(split='train')))}\")\n",
    "print(f\"Number of validation examples: {len(list(UDPOS(split='valid')))}\")\n",
    "print(f\"Number of testing examples: {len(list(UDPOS(split='test')))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RaAW09-n_5DC",
   "metadata": {
    "id": "RaAW09-n_5DC"
   },
   "source": [
    "Note that the data is already tokenized!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "rcG-FcDu5DcA",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T12:38:36.531860100Z",
     "start_time": "2023-11-27T12:38:35.086284Z"
    },
    "id": "rcG-FcDu5DcA"
   },
   "outputs": [],
   "source": [
    "# Create vocabularies\n",
    "def yield_tokens(data_iter, field):\n",
    "    for item in data_iter:\n",
    "        if field == 'text':\n",
    "            yield item[0]\n",
    "        elif field == 'ud_tags':\n",
    "            yield item[1]\n",
    "\n",
    "# Reset the iterator for text\n",
    "train_iter = UDPOS(split=\"train\")\n",
    "\n",
    "# Create vocabulary for text\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter, 'text'), specials=[\"<pad>\", \"<unk>\"], min_freq=2, special_first=True)\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "# Reset the iterator for UD tags\n",
    "train_iter = UDPOS(split=\"train\")\n",
    "\n",
    "# Create vocabulary for UD tags, ensuring <pad> is first in the list of specials\n",
    "ud_vocab = build_vocab_from_iterator(yield_tokens(train_iter, 'ud_tags'), specials=[\"<pad>\"], special_first=True)\n",
    "ud_vocab.set_default_index(ud_vocab['<pad>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nzalgc92j2mG",
   "metadata": {
    "id": "nzalgc92j2mG"
   },
   "source": [
    "Prepare a text processing pipeline that takes raw input and labels and converts them to ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "BDqKYQnRj2D8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T12:38:36.546996800Z",
     "start_time": "2023-11-27T12:38:36.536321300Z"
    },
    "id": "BDqKYQnRj2D8"
   },
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(x)\n",
    "label_pipeline = lambda x: ud_vocab(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "wg36B2Z_Hdi0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T12:38:36.548494800Z",
     "start_time": "2023-11-27T12:38:36.541327200Z"
    },
    "id": "wg36B2Z_Hdi0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 9271, 35, 9097, 0, 1]\n",
      "[6, 8, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "print(text_pipeline(['the', 'preacher', 'at', 'mosque', \"<pad>\",\"pppp\"])) #should output [3, 9271, 35, 9097, 0, 1]\n",
    "print(label_pipeline(['DET', 'ADJ', 'NOUN', 'PUNCT'])) # should output [6, 8, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cjn1qH2dHoaa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T12:38:36.557011800Z",
     "start_time": "2023-11-27T12:38:36.547997500Z"
    },
    "id": "cjn1qH2dHoaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in input vocabulary: 9875\n",
      "Unique tokens in UD vocabulary: 18\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in input vocabulary: {len(vocab)}\")\n",
    "print(f\"Unique tokens in UD vocabulary: {len(ud_vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FUnDiAOW-N5S",
   "metadata": {
    "id": "FUnDiAOW-N5S"
   },
   "source": [
    "Write a custom function for the dataloader that applies the text and label pipeline and pads the sequences to have equal lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0aN6s1UhoRhK",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T12:38:36.627893Z",
     "start_time": "2023-11-27T12:38:36.552014500Z"
    },
    "id": "0aN6s1UhoRhK"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def collate_batch(batch, text_pipeline, label_pipeline, pad_token_ix, pad_token_ix_ud):\n",
    "    text_list, label_list = [], []\n",
    "    for item in batch:\n",
    "        _text, _label, _ = item  # ignore PTB label\n",
    "        processed_text = text_pipeline(_text)\n",
    "        processed_label = label_pipeline(_label)\n",
    "        text_list.append(torch.tensor(processed_text, dtype=torch.int64))\n",
    "        label_list.append(torch.tensor(processed_label, dtype=torch.int64))\n",
    "\n",
    "    x_padded = pad_sequence(text_list, batch_first=True, padding_value=pad_token_ix)\n",
    "    y_padded = pad_sequence(label_list, batch_first=True, padding_value=pad_token_ix_ud)\n",
    "\n",
    "    return x_padded, y_padded\n",
    "\n",
    "\n",
    "collate_fn = partial(collate_batch, text_pipeline=text_pipeline, label_pipeline=label_pipeline, pad_token_ix=vocab['<pad>'], pad_token_ix_ud=ud_vocab['<pad>'])\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    UDPOS(split=\"train\"), batch_size=128, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    UDPOS(split=\"valid\"), batch_size=128, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    UDPOS(split=\"test\"), batch_size=128, shuffle=True, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2X8ZZ0J4oek_",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T12:38:37.257627500Z",
     "start_time": "2023-11-27T12:38:36.581124Z"
    },
    "id": "2X8ZZ0J4oek_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 57])\n",
      "torch.Size([128, 57])\n",
      "tensor([1832,   13,   17,  488, 2837,    3, 3074,    1, 5532,  512,   13,   23,\n",
      "          56,  143,    5,  170,  154,  273,   39,  157,  971,   44, 2212,  648,\n",
      "           2,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0])\n",
      "tensor([14,  4,  9, 12,  3,  6,  1,  1, 10, 17,  4,  3, 10, 13,  5, 13,  1, 10,\n",
      "        14,  9,  3,  5,  7,  7,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0])\n",
      "### Valid_dataloader ###\n",
      "torch.Size([128, 41])\n",
      "torch.Size([128, 41])\n",
      "tensor([   1, 3633,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0])\n",
      "tensor([7, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "for idx, (label, text) in enumerate(train_dataloader):\n",
    "      print(label.shape)\n",
    "      print(text.shape)\n",
    "      print(label[0])\n",
    "      print(text[0])\n",
    "      break\n",
    "print('### Valid_dataloader ###')\n",
    "for idx, (label, text) in enumerate(valid_dataloader):\n",
    "      print(label.shape)\n",
    "      print(text.shape)\n",
    "      print(label[0])\n",
    "      print(text[0])\n",
    "      break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jDy_wvjR_1Sg",
   "metadata": {
    "id": "jDy_wvjR_1Sg"
   },
   "source": [
    "Let's take a closer look at the data and the distribution of tags.\n",
    "Implement `tag_percentage`:\n",
    "\n",
    "*   Use the `collection.counter` to count the unique instances of each tag.\n",
    "\n",
    "*    Compute the percentage of each tag in the entire set, by using the counted frequencies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "273JqNZn_rRT",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T12:38:37.268052Z",
     "start_time": "2023-11-27T12:38:37.259534600Z"
    },
    "id": "273JqNZn_rRT"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def tag_percentage(training_iterator):\n",
    "    counter = Counter()\n",
    "    #### code to count the number of tags\n",
    "    for item in training_iterator:\n",
    "        text, ud_tags, ptb_tags = item  # 分别获取文本、UD标签和PTB标签\n",
    "        counter.update(ud_tags)  # 只计算UD标签\n",
    "\n",
    "    total_tags = sum(counter.values())\n",
    "\n",
    "    ### compute the tag percentages based on the counter object\n",
    "\n",
    "    tag_p = [(tag, count, count / total_tags) for tag, count in counter.items()]\n",
    "    ###\n",
    "    return tag_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fWzoEd7h_qRw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T12:38:38.007021700Z",
     "start_time": "2023-11-27T12:38:37.264047600Z"
    },
    "id": "fWzoEd7h_qRw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag\t\tCount\t\tPercentage\n",
      "PROPN\t\t12946\t\t 6.3%\n",
      "PUNCT\t\t23679\t\t11.6%\n",
      "ADJ\t\t12477\t\t 6.1%\n",
      "NOUN\t\t34781\t\t17.0%\n",
      "VERB\t\t23081\t\t11.3%\n",
      "DET\t\t16285\t\t 8.0%\n",
      "ADP\t\t17638\t\t 8.6%\n",
      "AUX\t\t12343\t\t 6.0%\n",
      "PRON\t\t18577\t\t 9.1%\n",
      "PART\t\t5567\t\t 2.7%\n",
      "SCONJ\t\t3843\t\t 1.9%\n",
      "NUM\t\t3999\t\t 2.0%\n",
      "ADV\t\t10548\t\t 5.2%\n",
      "CCONJ\t\t6707\t\t 3.3%\n",
      "X\t\t847\t\t 0.4%\n",
      "INTJ\t\t688\t\t 0.3%\n",
      "SYM\t\t599\t\t 0.3%\n"
     ]
    }
   ],
   "source": [
    "print(\"Tag\\t\\tCount\\t\\tPercentage\\n\")\n",
    "\n",
    "for tag, count, percent in tag_percentage(UDPOS(split='train')):\n",
    "    print(f\"{tag}\\t\\t{count}\\t\\t{percent*100:4.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GKTQL0EsxSUU",
   "metadata": {
    "id": "GKTQL0EsxSUU"
   },
   "source": [
    "####${\\color{red}{Comments\\ 1.1}}$\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
    "\n",
    "\n",
    "```\n",
    "✅ Point distribution so far ✅\n",
    "\n",
    "- 0.25 / 0.25 points if the example outputs align with solution, if the ids do not match because the id of `<pad>`,`<unk>` are different, assign the points.\n",
    "\n",
    "The solution is  a little bit different in how th methods are constructed (2 methods instead of 1 for yielding tags and tokens) but it is practically the same with the correct output\n",
    "\n",
    "- 0.25 / 0.25 points if len(input_vocab)=9875 and len(ud_vocab)=18, showing that the tokens were added correctly.\n",
    "\n",
    "The output is correct\n",
    "\n",
    "- 0.25 / 0.25 points if the special tokens are set correctly.\n",
    "\n",
    "The implementation so far is identical, the special tokens are set at the beginning. \n",
    "\n",
    "- 0.25 / 0.25 points if the batch size of the output example is 128.\n",
    "\n",
    "Batch size is identical\n",
    "\n",
    "- 0.5 / 0.5 points if the output is correctly converted to indices and tensors and the function is correct.\n",
    "\n",
    "The functions differes slightly implementationwise ( classic for loop instead of enumeration and usage of helper lists ), but it is identical with the correct output. The sample solution is a bit more elgeant because it uses less code. Only real difference is the print for the validation set as well, that was performed here, which has no effect on the grading\n",
    "\n",
    "\n",
    "- 0.5 / 0.5 points if the largest portion of data is NOUN with 34781 counts and 17.0%, and at the bottom is SYM with 599 counts and 0.3% and the logic is correct.\n",
    "\n",
    "Correctly counted everything, the logic is also identical.\n",
    "\n",
    "\n",
    "Overall points for subtask 1: \n",
    " 2 / 2 Points\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ end⚠️}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gYHy3vSELfaN",
   "metadata": {
    "id": "gYHy3vSELfaN"
   },
   "source": [
    "### Subtask 2: The Model\n",
    "\n",
    "We start by creating a simple model and then make it more complex in later subtasks. The class `BiLSTMTagger` must subclass the `nn.Module` class of `PyTorch`. Fill the blank in the class by following the notes described below.\n",
    "1.   The input is a sequence of tokens, $X = \\{x_1, x_2,...,x_T\\}$.\n",
    "2.   Each token passes through  an embeddings layer, $e(X) = \\{e(x_1), e(x_2), ..., e(x_T)\\}$. Use `nn.Embedding` for the embedding layer and make sure to pass in the index of the pad token.\n",
    "3. Embedding is processed by forward and backward LSTMs from left to right and right to left.  The first input to the forward LSTM is $x_1$ and the first input to the backward LSTM is $x_T$. The hidden state of LSTMs is dependent on\n",
    "the hidden, $h$, and cell, $c$, states from the previous time-steps:\n",
    "$$h^{\\rightarrow}_t = \\text{LSTM}^{\\rightarrow}(e(x^{\\rightarrow}_t), h^{\\rightarrow}_{t-1}, c^{\\rightarrow}_{t-1})$$\n",
    "$$h^{\\leftarrow}_t=\\text{LSTM}^{\\leftarrow}(e(x^{\\leftarrow}_t), h^{\\leftarrow}_{t-1}, c^{\\leftarrow}_{t-1})$$\n",
    "4. The hidden, $h$, and cell, $c$ of each layer is passed to the next layer, where the $h_0$ and $c_0$, for each direction and layer, are initialized to a tensor full of zeros. Use `nn.LSTM` for LSTM cells. How can you make it bidirectional?\n",
    "5. Final hidden state is the concatenation of forward and backward hidden states from the final layer of the LSTM, $H = \\{h_1, h_2, ... h_T\\}$, where $h_1 = [h^{\\rightarrow}_1;h^{\\leftarrow}_T]$, $h_2 = [h^{\\rightarrow}_2;h^{\\leftarrow}_{T-1}]$. Use `nn.Linear` here.\n",
    "6. The last layer is linear layer $f$, which is used to make the prediction of which tag applies to this token, $\\hat{y}_t = f(h_t)$.\n",
    "7. Define a `nn.Dropout` layer to apply to the embeddings and the outputs of the final layer of the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "qE4GQE6NJ1k8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T12:38:38.021731500Z",
     "start_time": "2023-11-27T12:38:38.014023200Z"
    },
    "id": "qE4GQE6NJ1k8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BiLSTMTagger(nn.Module):\n",
    "    def __init__(self, hyperparameters):\n",
    "        '''\n",
    "        hyperparameters: is a dictionary containing:\n",
    "            input_dim: dimension of the input\n",
    "            embedding_dim: dimension of the embedding layer\n",
    "            lstm_hidden_dim: dimension of the hidden state of lstms\n",
    "            output_dim: dimension of the output hidden layer\n",
    "            n_layers: number of layers to stack\n",
    "            bidirectional: is the lstm bi-directional\n",
    "            dropout: probability for the drop out layer\n",
    "            pad_idx: id of the pad token\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=hyperparameters['input_dim'],\n",
    "            embedding_dim=hyperparameters['embedding_dim'],\n",
    "            padding_idx=hyperparameters['pad_idx']\n",
    "        )\n",
    "\n",
    "        # Bi-LSTM, apply dropout if the number of layers is more than 1\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hyperparameters['embedding_dim'],\n",
    "            hidden_size=hyperparameters['lstm_hidden_dim'],\n",
    "            num_layers=hyperparameters['n_layers'],\n",
    "            bidirectional=hyperparameters['bidirectional'],\n",
    "            dropout=hyperparameters['dropout'] if hyperparameters['n_layers'] > 1 else 0\n",
    "        )\n",
    "\n",
    "        # Linear layer for prediction\n",
    "        self.fc = nn.Linear(\n",
    "            in_features=hyperparameters['lstm_hidden_dim'] * 2 if hyperparameters['bidirectional'] else hyperparameters['lstm_hidden_dim'],\n",
    "            out_features=hyperparameters['output_dim']\n",
    "        )\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(hyperparameters['dropout'])\n",
    "\n",
    "    def forward(self, text, debug=False):  # (B,S)\n",
    "        '''\n",
    "        S: sentence len\n",
    "        B: batch size\n",
    "        E: embedding size\n",
    "        H: hidden size\n",
    "        O: output size\n",
    "        L: number of layers\n",
    "        '''\n",
    "\n",
    "        # Pass text through the embedding layer and a dropout layer\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "\n",
    "        # Pass embeddings into Bi-LSTM\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "\n",
    "        # Concatenate the final hidden states from both directions if bidirectional\n",
    "        if self.lstm.bidirectional:\n",
    "            hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
    "        else:\n",
    "            hidden = hidden[-1, :, :]\n",
    "\n",
    "        # Compute prediction\n",
    "        predictions = self.fc(hidden)\n",
    "\n",
    "        if debug:\n",
    "            print(\"Input shape:\", text.shape)\n",
    "            print(\"Embedding shape:\", embedded.shape)\n",
    "            print(\"LSTM output shape:\", outputs.shape)\n",
    "            print(\"LSTM hidden shape:\", hidden.shape)\n",
    "            print(\"LSTM cell shape:\", cell.shape)\n",
    "            print(\"Output shape:\", predictions.shape)\n",
    "\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4OcQbD9LUkD5",
   "metadata": {
    "id": "4OcQbD9LUkD5"
   },
   "source": [
    "Response in plain text:\n",
    "1. Based on the notation defined in the forward function. What is the dimension of `outputs`, `hidden`, and `cell`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ml4KUd1BU3op",
   "metadata": {
    "id": "ml4KUd1BU3op"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ivB8Iv4yScVW",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T12:38:38.076135300Z",
     "start_time": "2023-11-27T12:38:38.019724300Z"
    },
    "id": "ivB8Iv4yScVW"
   },
   "outputs": [],
   "source": [
    "hyper_parameters={\n",
    "  'input_dim':  len(vocab),\n",
    "  'embedding_dim': 100,\n",
    "  'lstm_hidden_dim': 128,\n",
    "  'output_dim':len(ud_vocab),\n",
    "  'n_layers': 2 ,\n",
    "  'bidirectional':True,\n",
    "  'dropout': 0.25,\n",
    "  'pad_idx': vocab['<pad>']\n",
    "}\n",
    "model = BiLSTMTagger(hyper_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1XvNFfJBV7ya",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T12:38:38.134564600Z",
     "start_time": "2023-11-27T12:38:38.051126100Z"
    },
    "id": "1XvNFfJBV7ya"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 6])\n",
      "Embedding shape: torch.Size([2, 6, 100])\n",
      "LSTM output shape: torch.Size([2, 6, 256])\n",
      "LSTM hidden shape: torch.Size([6, 256])\n",
      "LSTM cell shape: torch.Size([4, 6, 128])\n",
      "Output shape: torch.Size([6, 18])\n",
      "pad_idx: 0\n"
     ]
    }
   ],
   "source": [
    "input=torch.tensor([[3, 9271, 35, 9097, 0, 1],\n",
    "                    [3, 9271, 35, 9097, 0, 1]])\n",
    "model(input,debug=True).shape\n",
    "\n",
    "print(\"pad_idx:\", hyper_parameters['pad_idx'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2XiJkG-NfrDV",
   "metadata": {
    "id": "2XiJkG-NfrDV"
   },
   "source": [
    "Weights of the network are initialized randomly, so let's make a more systematic initialization to help us with the optimization. For example,  Xavier Initialization creates weights such that the variance of the activations is the same across every layer. This constant variance helps prevent the gradient from exploding or vanishing. However, it does not apply to bias terms.\n",
    "\n",
    "Create a function that uses Xavier Initialization to initialize the weights of the network, for biases use a normal distribution with a mean of 0 and a standard deviation of 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "PB78pCHpa758",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T12:38:38.158565300Z",
     "start_time": "2023-11-27T12:38:38.133564500Z"
    },
    "id": "PB78pCHpa758"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTMTagger(\n",
       "  (embedding): Embedding(9875, 100, padding_idx=0)\n",
       "  (lstm): LSTM(100, 128, num_layers=2, dropout=0.25, bidirectional=True)\n",
       "  (fc): Linear(in_features=256, out_features=18, bias=True)\n",
       "  (dropout): Dropout(p=0.25, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.init as init\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias, mean=0, std=0.1)\n",
    "    elif type(m) == nn.Embedding:\n",
    "        init.xavier_uniform_(m.weight)\n",
    "\n",
    "# Applying the initialization to the model\n",
    "model.apply(init_weights)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sdPx1mSOgy0V",
   "metadata": {
    "id": "sdPx1mSOgy0V"
   },
   "source": [
    "Let's count the number of trainable parameters in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "P5738nU2ew_O",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T12:38:38.177574200Z",
     "start_time": "2023-11-27T12:38:38.155876500Z"
    },
    "id": "P5738nU2ew_O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of trainable parameters: 1622910\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"number of trainable parameters:\",count_parameters(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91uNA-vnyoUW",
   "metadata": {
    "id": "91uNA-vnyoUW"
   },
   "source": [
    "####${\\color{red}{Comments\\ 1.2}}$\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
    "\n",
    "\n",
    "```\n",
    "✅ Point distribution so far ✅\n",
    "- 0 / 0.25 points if the output shape is correct\n",
    "- 0 / 0.25 points if the hidden and cell shapes are correct from the question.\n",
    "\n",
    "Sadly, I think this has been overlooked by the team, because the markdown is empty, hence 0/ 0.5 points. But as feedback: It would be nice of the ones creating the assignments to highlight such cells better, because it is quite easy to overlook that you need to do something there. \n",
    "\n",
    "\n",
    "✅ Point distribution ✅\n",
    "- 0.666 / 1 point if the output shapes are correct:\n",
    "\n",
    "Input shape: torch.Size([2, 6])\n",
    "Embedding shape: torch.Size([2, 6, 100])\n",
    "LSTM output shape: torch.Size([2, 6, 256])\n",
    "LSTM hidden shape: torch.Size([4, 6, 128])\n",
    "LSTM cell shape: torch.Size([4, 6, 128])\n",
    "Output shape: torch.Size([2, 6, 18])\n",
    "\n",
    "this one is duplicated with the last shape above, hence follow up error which is disregarded by me --> torch.Size([2, 6, 18])\n",
    "\n",
    "\n",
    "Not all elements are correct. The LSTM hidden shape and the output shapes are incorrect. The bidirectionality was not implemented correctly, hence the error. Since there are 6 values (as explained above instead of 7) for 1 point and 4/6 are correct. this makes up for 0.666 Points\n",
    "\n",
    "- 0.5 / 1 point check the layers, if the bi-directionality and dropout are correct and all the elements are in place.\n",
    "\n",
    "As explained above bidirectionality was not implemented correctly. I don't understand what all elements are in place means, but I cannot ay that I see a misplaced element, hence 0.5 / 1 Point\n",
    "\n",
    "\n",
    "- 0.25 / 0.5 points if the forward function is implemented correctly\n",
    "\n",
    "The forward function is implemented correctly, but not regarding the bidirectionality. The concatenation approach for the bidirectionality is not needed and shouldn't be implemented. This is also the root cause for the errors described above. Overall a solid forward function though --> 0.25 / 0.5\n",
    "\n",
    "\n",
    "\n",
    "Overall points for Subtask 2:\n",
    "\n",
    "1.416 / 3 Points \n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ end⚠️}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-lVg08iTd0XN",
   "metadata": {
    "id": "-lVg08iTd0XN"
   },
   "source": [
    "### Subtask 3: Training\n",
    "\n",
    "We start by defining a loss function and an optimizer.\n",
    "\n",
    "\n",
    "*   **optimizer:** We use Adam with the learning rate=0.0001.\n",
    "*   **loss:** We use cross-entropy loss.\n",
    "\n",
    "Even though we have no `<unk>` tokens within our tag vocab, we still have `<pad>` tokens to create batches of the same size. However, we do not want to calculate loss on those tokens, so make sure you define your loss function in such a way that ignores the `<pad>` tokens.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8LEyNeKhmBvK",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T12:38:38.707956700Z",
     "start_time": "2023-11-27T12:38:38.161568500Z"
    },
    "id": "8LEyNeKhmBvK"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "# Loss function (ignores <pad> tokens)\n",
    "pad_idx = hyper_parameters['pad_idx']\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ Point distribution ✅\n",
    "- 0.25/0.25 points if the optimizer is defined correctly\n",
    "- 0.25/0.25 points if the loss is defined correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95Cf7ZcVgLHK",
   "metadata": {
    "id": "95Cf7ZcVgLHK"
   },
   "source": [
    "Watching the loss go down as you train a model is a good indication of the correct training procedure, but does not tell us how well we are doing on a given task.\n",
    "To this end, we also implement a categorical accuracy measure to keep track of how well our model is doing on a given task.\n",
    "Same as before: we don't want to calculate accuracy over the `<pad>` tokens as we aren't interested in predicting them.\n",
    "Implement the function `categorical_acc` to compare the prediction of non-pad tokens with labels count the correct ones and calculate the accuracy over a single batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "J5OpDKuBitk1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T12:38:38.720699400Z",
     "start_time": "2023-11-27T12:38:38.711188500Z"
    },
    "id": "J5OpDKuBitk1"
   },
   "outputs": [],
   "source": [
    "def categorical_acc(preds, gt, pad_idx):\n",
    "    \"\"\"\n",
    "    Returns categorical accuracy per batch\n",
    "    \"\"\"\n",
    "    # Get the index of the max probability\n",
    "    max_preds = preds.argmax(dim=1)\n",
    "\n",
    "    # Mask positions where the ground truth is not <pad>\n",
    "    non_pad_positions = gt != pad_idx\n",
    "\n",
    "    # Count the correct predictions where the ground truth is not <pad>\n",
    "    correct = (max_preds[non_pad_positions] == gt[non_pad_positions]).sum().item()\n",
    "\n",
    "    # Count the total number of non-pad tokens\n",
    "    total_non_pad = non_pad_positions.sum().item()\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = correct / total_non_pad if total_non_pad > 0 else 0\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "WANOhA56kjY_",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T12:38:38.781664400Z",
     "start_time": "2023-11-27T12:38:38.717701500Z"
    },
    "id": "WANOhA56kjY_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummpy_input=torch.tensor([\n",
    "    [0.9,0,0,0],\n",
    "    [0.1,0.9,0,0],\n",
    "    [0.1,0,0,0.9],\n",
    "    [0.9,0.1,0,0],\n",
    "    [0.1,0.8,0,0]\n",
    "\n",
    "])\n",
    "categorical_acc(dummpy_input, torch.tensor([0,2,3,0,1]), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ Point distribution ✅\n",
    "- 0.5/0.5 points if the function is correct, the output should be `0.6667`, if not check the function code to find error and give partial grade."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y_wngIs7jFrT",
   "metadata": {
    "id": "y_wngIs7jFrT"
   },
   "source": [
    "Define the `train` model that performs one epoch of training. You can refer to the Tutorial 2 of the course to get a sample workflow. The only difference to the tutorial is that we keep track of the batch-wise accuracy as well as the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "kyMs0FJNfGCp",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T12:38:38.794592Z",
     "start_time": "2023-11-27T12:38:38.737891900Z"
    },
    "id": "kyMs0FJNfGCp"
   },
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, loss_function, pad_idx):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.train()\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    for length_dataloader,(text,tags) in enumerate(dataloader):\n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(text)\n",
    "\n",
    "        # compute the loss, ignoring <pad> token\n",
    "        loss = loss_function(predictions.view(-1, predictions.shape[-1]), tags.view(-1))\n",
    "\n",
    "        # Backward pass and optimization step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute categorical accuracy\n",
    "        acc = categorical_acc(predictions, tags, pad_idx)\n",
    "\n",
    "        # Update epoch loss and accuracy\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc\n",
    "        \n",
    "    end_time= time.time() - epoch_start_time\n",
    "    return epoch_loss / length_dataloader, epoch_acc / length_dataloader,end_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ Point distribution ✅\n",
    "- 1/1 \n",
    "- 0.25/0.25 points if the loss function is called correctly\n",
    "- 0.25/0.25 points if the loss and accuracy are accumulated properly\n",
    "It is ok to divide by length_dataloader or length_dataloader+1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "W0ytUn7vkdcR",
   "metadata": {
    "id": "W0ytUn7vkdcR"
   },
   "source": [
    "It is not enough to only look at the training loss and accuracy, since with more training, we can always do better on the training set, but lose the generalizability to unseen data, a phenomenon known as **overfitting**. Therefore, it is important to check the loss and accuracy on the validation set after each epoch and stop before  overfitting occurs. Moreover, we can use the validation metric as an indication of which checkpoint of our model is the best.\n",
    "\n",
    "Define an `evaluate` function that runs once through the validation set and computes loss and accuracy. **Note:** You should not be updating gradients here and your model should be in evaluation mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0CKzEqyAfktd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T12:42:48.699300800Z",
     "start_time": "2023-11-27T12:42:48.681329500Z"
    },
    "id": "0CKzEqyAfktd"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, loss_function, pad_idx):\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    length_dataloader = len(dataloader)\n",
    "\n",
    "    with torch.no_grad():  # No need to track gradients during evaluation\n",
    "        for batch_num, (text, tags) in enumerate(dataloader):\n",
    "            # Forward pass\n",
    "            predictions = model(text)\n",
    "\n",
    "            # Flatten predictions and targets, excluding padded elements\n",
    "            predictions_flat = predictions.view(-1, predictions.shape[-1])\n",
    "            tags_flat = tags.view(-1)\n",
    "\n",
    "            # Mask out padded elements\n",
    "            non_pad_elements = (tags_flat != pad_idx).nonzero().squeeze()\n",
    "\n",
    "            # Compute the loss, ignoring <pad> tokens\n",
    "            loss = loss_function(predictions_flat[non_pad_elements], tags_flat[non_pad_elements])\n",
    "\n",
    "            # Compute categorical accuracy\n",
    "            acc = categorical_acc(predictions_flat[non_pad_elements], tags_flat[non_pad_elements], pad_idx)\n",
    "\n",
    "            # Update validation loss and accuracy\n",
    "            val_loss += loss.item()\n",
    "            val_acc += acc\n",
    "\n",
    "    return val_loss / length_dataloader, val_acc / length_dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ Point distribution ✅\n",
    "- 0.25/0.25 points if the `model.eval()` is used and the block is in `torch.no_grad()`\n",
    "- 0.5/0.5 points if the rest of the function is correct (should be similar to the one above)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3AZJidLFnOUW",
   "metadata": {
    "id": "3AZJidLFnOUW"
   },
   "source": [
    "Let's use the functions defined so far and train our model for `30` epochs. We suggest using GPU for this task, as it is quite slow on the CPU. Run the training loop for the given number of epochs and calculate the validation metric at the end of each epoch. Based on the validation loss, save the best checkpoint of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "NblN-U0inVYc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T12:42:51.718242500Z",
     "start_time": "2023-11-27T12:42:50.789982300Z"
    },
    "id": "NblN-U0inVYc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad_idx: 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (54) to match target batch_size (6912).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 9\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpad_idx:\u001b[39m\u001b[38;5;124m\"\u001b[39m, hyper_parameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpad_idx\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     train_loss, train_acc, epoch_time \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyper_parameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpad_idx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Validation\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     valid_loss, valid_acc \u001b[38;5;241m=\u001b[39m evaluate(model, valid_dataloader, loss_function, hyper_parameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpad_idx\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[20], line 17\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, dataloader, optimizer, loss_function, pad_idx)\u001b[0m\n\u001b[0;32m     14\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model(text)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# compute the loss, ignoring <pad> token\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization step\u001b[39;00m\n\u001b[0;32m     20\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp\\Lib\\site-packages\\torch\\nn\\functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3052\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (54) to match target batch_size (6912)."
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "print(\"pad_idx:\", hyper_parameters['pad_idx'])\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    train_loss, train_acc, epoch_time = train(model, train_dataloader, optimizer, loss_function, hyper_parameters['pad_idx'])\n",
    "\n",
    "    # Validation\n",
    "    valid_loss, valid_acc = evaluate(model, valid_dataloader, loss_function, hyper_parameters['pad_idx'])\n",
    "\n",
    "    # Save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best_model_checkpoint.pth')\n",
    "    elapsed_mins = int(epoch_time / 60)\n",
    "    elapsed_secs = int(epoch_time - (elapsed_mins * 60))\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {elapsed_mins}m {elapsed_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Validation Loss: {valid_loss:.3f} |  Validation Acc: {valid_acc*100:.2f}%')\n",
    "\n",
    "\n",
    "print(\"pad_idx:\", hyper_parameters['pad_idx'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ Point distribution ✅\n",
    "- 0/0.25 points if the training and validation loop are correctly called\n",
    "- 0/0.25 points if the best checkpoint is one that is saved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dXTYFEbHzNL",
   "metadata": {
    "id": "7dXTYFEbHzNL"
   },
   "source": [
    "Question:\n",
    "\n",
    "1. Does overfitting occur? If so, after which epochs?\n",
    "\n",
    "2. How do you detect overfitting?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9qSbr8CrH9hD",
   "metadata": {
    "id": "9qSbr8CrH9hD"
   },
   "source": [
    "**Answer:**\n",
    "```\n",
    "In our case there was a value mismatch error resulting in us not able to analyze the code properly\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ Point distribution ✅\n",
    "- 0/0.25 points for 1\n",
    "- 0/0.25 points for 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GmqNesqiMISb",
   "metadata": {
    "id": "GmqNesqiMISb"
   },
   "source": [
    "Let's see how well our model is doing on the test set. Load the best checkpoint and calculate the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "SgCFqby0m8sf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T11:54:16.279542200Z",
     "start_time": "2023-11-27T11:54:16.261493200Z"
    },
    "id": "SgCFqby0m8sf"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#### you code ####\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#### you code ####\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mtest_acc\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_acc' is not defined"
     ]
    }
   ],
   "source": [
    "#### you code ####\n",
    "\n",
    "#### you code ####\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tMIDBeSFyqJr",
   "metadata": {
    "id": "tMIDBeSFyqJr"
   },
   "source": [
    "####${\\color{red}{Comments\\ 1.3}}$\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
    "\n",
    "\n",
    "```\n",
    "Overall points: 3.25/4.25\n",
    "\n",
    "Sadly they overlooked, that they could have still described what overfitting is without actual results of the model. \n",
    "The training and model steps are all very close to the sample-solution. They just didn't work with the sizes of input batch and output batch correctly. \n",
    "Their error \"ValueError: Expected input batch_size (54) to match target batch_size (6912).\" shows, that they didn't flatten the inputs correctly for the loss function. \n",
    "6912/128 = 54. This shows, that they tried to evaluate on the entire batch (128) and not just each 54 token long sequence. \n",
    "```\n",
    "\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ end⚠️}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dPfzmVGMeSO",
   "metadata": {
    "id": "7dPfzmVGMeSO"
   },
   "source": [
    "### Subtask 4: Inference\n",
    "\n",
    "Let's use the model we trained to tag some actual sentences. We have the preprocessing pipeline ready from Subtask 1, now we need to map the predictions back to label texts for each token.\n",
    "\n",
    "Implement the `tag_sequence` function that takes a model and a sentence as input and generates POS tags. Keep in mind that you need to divide the sentence into tokens first. For this purpose, we just split each sentence on whitespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "Xe7_ROCyEslD",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T11:54:17.056206700Z",
     "start_time": "2023-11-27T11:54:17.034346600Z"
    },
    "id": "Xe7_ROCyEslD"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (358943157.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[24], line 5\u001b[1;36m\u001b[0m\n\u001b[1;33m    predictions = # make predictions on th esentence\u001b[0m\n\u001b[1;37m                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def tag_sentence(model, sentence):\n",
    "    ### your code ###\n",
    "\n",
    "\n",
    "    predictions = # make predictions on th esentence\n",
    "\n",
    "\n",
    "    predicted_tags = # get the tags\n",
    "    ### your code ###\n",
    "    return predicted_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "JtiR75PFRR61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T11:54:17.482884300Z",
     "start_time": "2023-11-27T11:54:17.284290600Z"
    },
    "id": "JtiR75PFRR61"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tag_sentence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mlist\u001b[39m(UDPOS(split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m))[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      2\u001b[0m label\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(UDPOS(split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m))[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m----> 3\u001b[0m predicted_tag\u001b[38;5;241m=\u001b[39m\u001b[43mtag_sentence\u001b[49m(model,text)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText: \u001b[39m\u001b[38;5;124m\"\u001b[39m,text)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted Tags: \u001b[39m\u001b[38;5;124m\"\u001b[39m,predicted_tag)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tag_sentence' is not defined"
     ]
    }
   ],
   "source": [
    "text=\" \".join(list(UDPOS(split='test'))[0][0])\n",
    "label= list(UDPOS(split='test'))[0][1]\n",
    "predicted_tag=tag_sentence(model,text)\n",
    "print(\"Text: \",text)\n",
    "print(\"Predicted Tags: \",predicted_tag)\n",
    "print(\"True Tags: \",label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ Point distribution ✅\n",
    "- 0/0.5 no inference possible because model was not working"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bPriv78yuyQ",
   "metadata": {
    "id": "7bPriv78yuyQ"
   },
   "source": [
    "####${\\color{red}{Comments\\ 1.4}}$\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
    "\n",
    "\n",
    "```\n",
    "Overall points: 0/0.5\n",
    "\n",
    "Subtask 4 not done. \n",
    "```\n",
    "\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ end⚠️}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "X4n7Xi4TT_Ww",
   "metadata": {
    "id": "X4n7Xi4TT_Ww"
   },
   "source": [
    "## **Task 2: Theoretical Questions** (0.5+1.5+1+3=6 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QbQJ3baXUHkQ",
   "metadata": {
    "id": "QbQJ3baXUHkQ"
   },
   "source": [
    "### Subtask 1:\n",
    "In beam search, if you increase the beam width, what will happen to a) the runtime and memory and b) the quality of results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ei5A-D_djZup",
   "metadata": {
    "id": "Ei5A-D_djZup"
   },
   "source": [
    "**Answer:**\n",
    "\n",
    "\n",
    "a) Running time and memory usage: Increasing the beam width means that during the search process, the algorithm needs to track and calculate more candidate solutions simultaneously. This naturally increases the computational burden of the algorithm, resulting in longer running times. At the same time, memory usage will increase accordingly as more candidate solutions need to be stored. In short, increasing beam width results in increased runtime and memory usage.\n",
    "\n",
    "b) Quality of results: Increasing the beam width usually improves the quality of the solutions found. This is because the algorithm has more room to explore different possibilities, reducing the risk of missing good solutions due to early pruning. However, this is not always proportional, as in some cases increasing beam width may only bring marginal improvement. Furthermore, in some specific cases, a beam that is too wide may even lead to a decrease in search efficiency, as the algorithm may spend unnecessary time exploring low-quality solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RuzX2q7TywyW",
   "metadata": {
    "id": "RuzX2q7TywyW"
   },
   "source": [
    "####${\\color{red}{Comments\\ 2.1}}$\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
    "\n",
    "\n",
    "```\n",
    "Very extensive and intuitive answers. Message is the same as in sample solution.\n",
    "\n",
    "0.25/0.25 for a\n",
    "0.25/0.25 for b\n",
    "```\n",
    "\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ end⚠️}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "N7KlRPnhknbl",
   "metadata": {
    "id": "N7KlRPnhknbl"
   },
   "source": [
    "\n",
    "### Subtask 2:\n",
    "Except for beam search, there are other ways to create a more coherent output for generation tasks, one of which is adding a temperature to the softmax over the vocabulary. Temperature is a hyperparameter that is applied to the input of a softmax to affect the final probabilities. All values in the input are divided by the temperature before going through the softmax. What do you think will happen in these cases:\n",
    "\n",
    "1. A low temperature - below 1\n",
    "2. A high temperature - above 1\n",
    "3. Really small temperature - temperature $→$ 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uxuh5LOFnF3D",
   "metadata": {
    "id": "uxuh5LOFnF3D"
   },
   "source": [
    "**Answer:**\n",
    "\n",
    " **Low temperature (below 1)**: When the temperature value is lower than 1, it amplifies the difference in the input vector. This means that even small differences become more noticeable. In the softmax output, the items with the highest probability will become more prominent, while the probabilities of other items will be relatively reduced. Therefore, low temperatures will bias the model's output toward selecting the words with the highest probability, resulting in more deterministic, less stylized text generation. This helps improve the consistency and predictability of the output, but may reduce text variety.\n",
    "\n",
    "**High temperature (above 1)**: When the temperature value is higher than 1, it reduces the difference in the input vector. This will make the probability distribution of the softmax output more uniform, and even items with lower probabilities will have a higher probability of being selected. Therefore, high temperatures will cause the generated text to become more diverse and unpredictable, but at the same time may sacrifice some consistency and accuracy.\n",
    "\n",
    "**Really small temperatures (temperature → 0):** As the temperature approaches 0, the behavior of the model becomes extreme. In practice, this causes the softmax output to almost always select the word with the highest score, since the probability of all other options will approach zero. In this case, the generated text will be very deterministic and repetitive, almost always selecting the same sequence of words, severely limiting the diversity and creativity of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GmW4NNb4yyZF",
   "metadata": {
    "id": "GmW4NNb4yyZF"
   },
   "source": [
    "####${\\color{red}{Comments\\ 2.2}}$\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
    "\n",
    "\n",
    "```\n",
    "cross-feedback comment section\n",
    "Behaviour and text characteristics are described correctly. For subtask 3 the similarity to greedy decoding is not mentioned as in the smaple solution. Nevertheless, the answers absolutely sufficient.\n",
    "\n",
    "- 0.5/0.5 for 1\n",
    "- 0.5/0.5 for 2\n",
    "- 0.5/0.5 for 3\n",
    "```\n",
    "\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ end⚠️}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tM2DmNOwod4r",
   "metadata": {
    "id": "tM2DmNOwod4r"
   },
   "source": [
    "### Subtask 3:\n",
    "Explain what the “bottleneck” of an encoder-decoder RNN is and how attention provides a way to get around this bottleneck."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DNJ3f_MtdzrM",
   "metadata": {
    "id": "DNJ3f_MtdzrM"
   },
   "source": [
    "**Answer:**\n",
    "\n",
    "In the encoder-decoder (Encoder-Decoder) recurrent neural network (RNN) structure, the \"bottleneck\" refers to a restriction point in the flow of information. This structure is often used for sequence-to-sequence tasks such as machine translation, where the encoder converts the input sequence (such as a sentence) into a fixed-size internal representation (or called a context vector), and then the decoder uses this internal representation to generate the output sequence.\n",
    "\n",
    "**Bottleneck problem**:\n",
    "\n",
    "- **Fixed size internal representation**: The encoder compresses all input information into a fixed size vector. For longer input sequences, this fixed-size vector may not capture all important information efficiently, especially when the sequence is very long.\n",
    "- **Information Loss**: As sequence length increases, the encoder may lose information about the beginning of the input sequence when creating an internal representation, which can negatively impact the decoder's ability to produce accurate and relevant output.\n",
    "\n",
    "**Attention Mechanism (Attention)**:\n",
    "\n",
    "Attention mechanisms provide a way to solve the encoder-decoder bottleneck. It allows the decoder to \"focus\" (or weight) on different parts of the encoder's output at each step in generating the output. The core idea of this mechanism is to selectively focus on certain parts of the input sequence at each decoding step, rather than relying only on a fixed context vector.\n",
    "\n",
    "- **Dynamic Context**: The attention mechanism allows the decoder to take into account the entire range of the input sequence when generating each word, rather than relying only on a single, static context vector. In this way, the decoder can extract more relevant information from the input sequence.\n",
    "- **Improved long-distance dependencies**: The attention mechanism is particularly useful when dealing with long sequences, as it allows the model to directly access distant elements in the input sequence, thus handling long-distance dependencies more effectively.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JFUHVF40y0j-",
   "metadata": {
    "id": "JFUHVF40y0j-"
   },
   "source": [
    "####${\\color{red}{Comments\\ 2.3}}$\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
    "\n",
    "\n",
    "```\n",
    "Absolutely correct! Even sebarated and structured explanations for partial problems (dynamic context and long-distance dependencies) which are summarised in one sentence of the sample solution.\n",
    "1/1 points\n",
    "```\n",
    "\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ end⚠️}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HaHhdLdJehL3",
   "metadata": {
    "id": "HaHhdLdJehL3"
   },
   "source": [
    "### Subtask 4:\n",
    "As mentioned, there are various way to remedy the repetitiveness and incoherence of generation outputs. One of the widely used methods is Nucleus sampling described the paper \"[ The Curious Case of Neural Text DeGeneration](https://arxiv.org/pdf/1904.09751.pdf)\". Read the model section and introduction of the paper and use it as reference to answer the following questions:\n",
    "\n",
    "1. Describe top-k sampling in your own words, no need for mathematical notation.\n",
    "2. Describe Nucleus sampling in your own words, there is not need for mathematical notation.\n",
    "3.  Why is beam search not a good strategy for human-like text generation and why don't these methods suffer from the problem of the beam search?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9Ojtbn1kgaYE",
   "metadata": {
    "id": "9Ojtbn1kgaYE"
   },
   "source": [
    "**Answer:**\n",
    "\n",
    "\n",
    "**Beam Search Problem**:\n",
    "- Beam search optimizes for high-probability output, which sounds reasonable because we expect good models to assign higher probabilities to human-like, grammatically correct text. But in practice, this approach often produces generic, repetitive, and clunky results when generating long texts. Beam search produces text that differs significantly in probability assignments from naturally occurring text, which results in poor output quality.\n",
    "\n",
    "**Top-k sampling**:\n",
    "- In top-k sampling, each time a word is generated, the model randomly selects one from the k words with the highest probability. Rather than simply selecting the most likely words, this approach adds randomness to words with higher probability, thereby increasing text diversity and creativity.\n",
    "\n",
    "**Nucleus Sampling**:\n",
    "- Nucleus sampling is a more refined sampling method that takes into account the cumulative probability distribution of vocabulary. In this approach, instead of selecting from a fixed number of words, the model selects a \"core\" set that contains words whose sum of probabilities reaches a certain threshold. This threshold is usually a percentage, meaning that the set of words selected changes dynamically based on the context. Doing so balances diversity and accuracy because it takes into account the model's \"confidence\" in the current situation\n",
    "\n",
    "Compared with beam search, these sampling methods are not restricted by selecting the most likely words, so they better capture the diversity and creativity of the text and avoid problems of repetition and unnaturalness. These methods get closer to the characteristics of human text generation by introducing randomness and diversity while maintaining language fluency.```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0wU67Iqpy2rg",
   "metadata": {
    "id": "0wU67Iqpy2rg"
   },
   "source": [
    "####${\\color{red}{Comments\\ 2.4}}$\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
    "\n",
    "\n",
    "```\n",
    "- Top-K sampling word correctly described. In contrast to the sample solution, which refers to the tail, the solution explains the subset that is sampled from.\n",
    "- Very intuitive explanation. Sample solution uses p to explain the algorithm.\n",
    "- Intuitive explanation. Nevertheless it is not described, why the two previous approaches do not suffer from the problem.\n",
    "\n",
    "\n",
    "- 1/1 point for 1\n",
    "- 1/1 point for 2\n",
    "- 1/1 point for 3\n",
    "```\n",
    "\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ end⚠️}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IZl0-MMY6MWs",
   "metadata": {
    "id": "IZl0-MMY6MWs"
   },
   "source": [
    "## **Task 3: Scaled Dot-Product Attention** (4+1=5 points)\n",
    "In class, you learned about attention and Transformers as described in the 2017 paper\n",
    "[Attention Is All You Need](https://arxiv.org/abs/1706.03762).\n",
    "The base of the attention module is a scaled dot product with Queries, Keys, and Values.\n",
    "In this task, you will implement a simplified version of scaled dot-product attention and inspired by the translation task, aim to replicate word alignment between English and French.\n",
    "You will not be training the embedding from scratch, we provide you with pre-trained embedding for both languages.\n",
    "However, you need to know the details of scaled dot product attention, which mainly consists of two matrix multiplications and a softmax scaling.\n",
    "Refer to Figure 2 of the [Attention Is All You Need](https://arxiv.org/abs/1706.03762) paper.\n",
    "\n",
    "The inputs of the attention module are Queries, Keys, and Values. Mathematically, attention is defined as follows:\n",
    "\n",
    "$$\n",
    "\\large \\mathrm{Attention}\\left(Q, K, V\\right) = \\mathrm{softmax}\\left(\\frac{QK^{\\top}}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "\n",
    "*   $Q$, $K$, and $V$ are the Queries, Keys, and Values matrices.\n",
    "* $d_k$ is the dimension of the Keys (in practice dimensions of all matrices are the same).\n",
    "*   $QK^{\\top}$ is a measure of the similarity between the Queries and the Keys\n",
    "* softmax transforms the similarity into weights.\n",
    "* Weights multiplied by the Values are the output of the attention, defining how much importance should be given to each token of the input.\n",
    "\n",
    "In the case of self-attention, both Queries and Keys come from the encoder, however, for cross attention between encoder and decoder, decoder states are used as the queries while encoder states are the Keys and Values.\n",
    "In our case, we need the cross attention between one language to another to find the correct alignment.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7c3fa7f4-8c1b-4a4b-9754-9d158ab4e398",
   "metadata": {
    "id": "gOP3Fa8IfkMK"
   },
   "source": [
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "tB6RD3wssHe9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T11:55:59.551710600Z",
     "start_time": "2023-11-27T11:55:58.849442300Z"
    },
    "id": "tB6RD3wssHe9"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#load the dictionaries (dictionary of words to ids)\n",
    "with open(\"./word2int_en.pkl\", \"rb\") as f:\n",
    "    en_dict = pickle.load(f)\n",
    "\n",
    "with open(\"./word2int_fr.pkl\", \"rb\") as f:\n",
    "    fr_dict = pickle.load(f)\n",
    "\n",
    "# load word embeddings (dictionary of token ids to embeddings)\n",
    "en_embeddings = np.load(\"./embeddings_en.npz\")[\"embeddings\"]\n",
    "fr_embeddings = np.load(\"./embeddings_fr.npz\")[\"embeddings\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7VYjN1xHsbYj",
   "metadata": {
    "id": "7VYjN1xHsbYj"
   },
   "source": [
    "### Subtask 1: Attention Weights\n",
    "Fill the blanks in `tokenize` to tokenize a sentence and convert it to ids and `embed` function to create an embedding of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cwS_M10nwOBH",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T11:56:00.191989600Z",
     "start_time": "2023-11-27T11:56:00.187459300Z"
    },
    "id": "cwS_M10nwOBH"
   },
   "outputs": [],
   "source": [
    "def tokenize(sentence, token_mapping):\n",
    "   # we stick to simple blank space tokenization\n",
    "   tokenized = []\n",
    "   for word in sentence.lower().split(\" \"):\n",
    "      ### your code ###\n",
    "      # Check if the word is in the token mapping, else use a special token for unknown words, e.g., \"<unk>\"\n",
    "      token_id = token_mapping.get(word, token_mapping.get(\"<UNK>\", None))\n",
    "      if token_id is None:\n",
    "            # Handle the case where \"<UNK>\" is also not in the dictionary\n",
    "          continue  # Skip this word or you can choose to append a specific ID for unknown words\n",
    "      tokenized.append(token_id)\n",
    "      ### your code ###\n",
    "   return tokenized\n",
    "\n",
    "def embed(tokens, embeddings):\n",
    "    \"\"\" get the embedding for the tokens in a sentence stacked in a simple matrix (sequence length, embedding size)\n",
    "        tokens: tokenized sentence\n",
    "        embeddings: dictionary of token to embeddings.\n",
    "    \"\"\"\n",
    "    embed_size = embeddings.shape[1]\n",
    "    output = np.zeros((len(tokens), embed_size))\n",
    "\n",
    "    for i, token_id in enumerate(tokens):\n",
    "        if token_id is not None and 0 <= token_id < len(embeddings):\n",
    "            output[i] = embeddings[token_id]\n",
    "        else:\n",
    "            output[i] = np.zeros(embed_size)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dkAulXarueT_",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T11:56:00.615185900Z",
     "start_time": "2023-11-27T11:56:00.599323600Z"
    },
    "id": "dkAulXarueT_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized english: [59, 40, 9355, 6, 158]\n",
      "embedding english: (5, 300)\n",
      "Tokenized french: [21, 73, 192, 16, 8652, 558]\n",
      "embedding french: (6, 300)\n"
     ]
    }
   ],
   "source": [
    "sentence_en = \"there were clouds in my coffeeeeee.\"\n",
    "tokenized_en = tokenize(sentence_en, en_dict)\n",
    "embedded_en = embed(tokenized_en, en_embeddings)\n",
    "print(\"Tokenized english:\",tokenized_en)\n",
    "print(\"embedding english:\",embedded_en.shape)\n",
    "\n",
    "sentence_fr = \"il y avait des nuages ​​dans mon ccafé.\"\n",
    "tokenized_fr = tokenize(sentence_fr, fr_dict)\n",
    "embedded_fr = embed(tokenized_fr, fr_embeddings)\n",
    "print(\"Tokenized french:\",tokenized_fr)\n",
    "print(\"embedding french:\",embedded_fr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WJ58FCMpsHJP",
   "metadata": {
    "id": "WJ58FCMpsHJP"
   },
   "source": [
    "\n",
    "Implement the `softmax` function with `Numpy`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caOjXfbxon9h",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T11:56:01.997338800Z",
     "start_time": "2023-11-27T11:56:01.992332500Z"
    },
    "id": "caOjXfbxon9h"
   },
   "outputs": [],
   "source": [
    "def softmax(x, axis=0):\n",
    "    \"\"\"\n",
    "    x: input matrix\n",
    "    axis: defines which axis to compute the softmax over 0 for rows and 1 for columns\n",
    "        axis=0 calculates softmax across rows which means each column sums to 1\n",
    "        axis=1 calculates softmax across columns which means each row sums to 1\n",
    "    \"\"\"\n",
    "\n",
    "    # Shift the input x by subtracting its max value for numerical stability\n",
    "    x_shifted = x - np.max(x, axis=axis, keepdims=True)\n",
    "\n",
    "    # Exponentiate the shifted values\n",
    "    exp_x = np.exp(x_shifted)\n",
    "\n",
    "    # Normalize by the sum along the specified axis\n",
    "    softmax_x = exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "\n",
    "    return softmax_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "kCbjmG49p-v0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T11:56:02.699401900Z",
     "start_time": "2023-11-27T11:56:02.681459900Z"
    },
    "id": "kCbjmG49p-v0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w=softmax(np.array([[1,3,4,1], [24,3,2,3]]),axis=0)\n",
    "w.sum(axis=0)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oNqtNPB9orLr",
   "metadata": {
    "id": "oNqtNPB9orLr"
   },
   "source": [
    "Use the `softmax` function to calculate the weights.\n",
    "$$ \\mathrm{softmax}\\left(\\frac{QK^{\\top}}{\\sqrt{d_k}}\\right)$$\n",
    "Assume the queries and keys are 2D matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ysRdVCnP7Qir",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T11:56:04.261147100Z",
     "start_time": "2023-11-27T11:56:04.249138700Z"
    },
    "id": "ysRdVCnP7Qir"
   },
   "outputs": [],
   "source": [
    "def calc_weights(queries, keys):\n",
    "    \"\"\"\n",
    "    queries: queries matrix\n",
    "    keys: keys matrix\n",
    "    \"\"\"\n",
    "\n",
    "    dk = keys.shape[1]  # Dimension of the keys\n",
    "    # Calculate the dot product of queries and the transpose of keys, then scale\n",
    "    scaled_dot_product = np.dot(queries, keys.T) / np.sqrt(dk)\n",
    "\n",
    "    # Apply softmax to obtain the weights\n",
    "    weights = softmax(scaled_dot_product, axis=1)\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "LVYGelEsqsoo",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T11:56:05.962411Z",
     "start_time": "2023-11-27T11:56:05.925406600Z"
    },
    "id": "LVYGelEsqsoo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.19557032 0.80442968]\n",
      " [0.19557032 0.80442968]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights=calc_weights(np.array([[1,3],[1,3]]),np.array([[0,3],[2,3]]))\n",
    "print(weights)\n",
    "weights.sum(axis=1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16V5yFAew1Hx",
   "metadata": {
    "id": "16V5yFAew1Hx"
   },
   "source": [
    "Use the `calcu_weights` to compute the attention matrix between two sentences from English and French and visualize the weights to check for alignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "miW865arw9Fv",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T11:56:07.261141100Z",
     "start_time": "2023-11-27T11:56:07.056311800Z"
    },
    "id": "miW865arw9Fv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French word 'il' is most aligned with English words: ['unicorns', 'in']\n",
      "French word 'y' is most aligned with English words: ['there', 'unicorns']\n",
      "French word 'avait' is most aligned with English words: ['were', 'in']\n",
      "French word 'des' is most aligned with English words: ['in', 'and']\n",
      "French word 'nuages' is most aligned with English words: ['clouds', 'and']\n",
      "French word '​​dans' is most aligned with English words: ['my', 'in']\n",
      "French word 'mon' is most aligned with English words: ['coffee', 'unicorns']\n",
      "French word 'café' is most aligned with English words: ['and', 'unicorns']\n",
      "French word 'et' is most aligned with English words: ['in', 'and']\n",
      "French word 'des' is most aligned with English words: ['unicorns', 'in']\n",
      "French word 'licornes' is most aligned with English words: ['in', 'and']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahul\\AppData\\Local\\Temp\\ipykernel_18600\\872748905.py:18: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_yticklabels(sentence_fr.split(\" \"), size=12);\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence_en = \"there were clouds in my coffee and unicorns in the sky.\"\n",
    "tokenized_en = tokenize(sentence_en, en_dict)\n",
    "embedded_en = embed(tokenized_en, en_embeddings)\n",
    "\n",
    "sentence_fr = \"il y avait des nuages ​​dans mon café et des licornes dans le ciel.\"\n",
    "tokenized_fr = tokenize(sentence_fr, fr_dict)\n",
    "embedded_fr = embed(tokenized_fr, fr_embeddings)\n",
    "\n",
    "alignment = calc_weights(embedded_fr, embedded_en)\n",
    "# visualize weights\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "ax.imshow(alignment,cmap='gray')\n",
    "ax.xaxis.tick_top()\n",
    "# ax.set_xticks(np.arange(alignment.shape[1]))\n",
    "ax.set_xticks(np.arange(len(sentence_en.split(' '))))\n",
    "ax.set_xticklabels(sentence_en.split(\" \"), rotation=90, size=12);\n",
    "ax.set_xticks(np.arange(len(sentence_fr.split(' '))));\n",
    "ax.set_yticklabels(sentence_fr.split(\" \"), size=12);\n",
    "\n",
    "# Compute the top-2 alignments for each French word\n",
    "top_2_alignments = []\n",
    "# top_2_alignments = np.argsort(alignment, axis=0)[-2:]\n",
    "\n",
    "for row in alignment:\n",
    "    # argsort returns indices of elements sorted in ascending order, so take the last two for the largest values\n",
    "    top_indices = row.argsort()[-2:][::-1]  # Reverse to get largest first\n",
    "    top_2_alignments.append(top_indices)\n",
    "\n",
    "# Display the top-2 alignments\n",
    "for i, indices in enumerate(top_2_alignments):\n",
    "    french_word = sentence_fr.split(\" \")[i]\n",
    "    # english_words = [sentence_en.split(\" \")[index] for index in indices]\n",
    "    # print(f\"French word '{french_word}' aligns with English words: {english_words}\")\n",
    "\n",
    "    aligned_english_words = [sentence_en.split(\" \")[j] for j in indices]\n",
    "    print(f\"French word '{french_word}' is most aligned with English words: {aligned_english_words}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0BxnOJJ2xhIZ",
   "metadata": {
    "id": "0BxnOJJ2xhIZ"
   },
   "source": [
    "1. What are the top 2 alignments shown in the figure?\n",
    "2. On the rows you see flat lines for `ciel` and `dans` and `licornes`, why do you think that is?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QUwdKXhZxqUP",
   "metadata": {
    "id": "QUwdKXhZxqUP"
   },
   "source": [
    "**Answer:**\n",
    "\n",
    "```\n",
    "Answer 1. \n",
    "The top 2 alignments for each French word are determined based on the highest values in the alignment matrix. \n",
    "like there are strong associations for the words \"nuages\" and \"il\".\n",
    "Answer 2. \n",
    "The flat lines for 'ciel', 'dans', and 'licornes' indicate that these French words are not strongly aligned with any specific English word in the given context. This could be due to several reasons:\n",
    "\n",
    "- Lack of direct translation equivalents\n",
    "- Polysemy or multiple meanings\n",
    "In summary, the flat lines suggest that the alignment model has difficulty establishing strong associations between certain French words and their corresponding English counterparts, possibly due to translation complexities or lack of direct correspondence.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "C2TnQmsxy70i",
   "metadata": {
    "id": "C2TnQmsxy70i"
   },
   "source": [
    "####${\\color{red}{Comments\\ 3.1}}$\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
    "\n",
    "\n",
    "```\n",
    "cross-feedback comment section|\n",
    "```\n",
    "\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ end⚠️}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eqXwL4MpzW--",
   "metadata": {
    "id": "eqXwL4MpzW--"
   },
   "source": [
    "### Subtask 2: Scaled Dot-product\n",
    "Implement the scaled dot-product attention using the functions from above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8iQTr4KCzqZk",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T12:03:42.905587800Z",
     "start_time": "2023-11-27T12:03:42.884606400Z"
    },
    "id": "8iQTr4KCzqZk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11, 300])\n",
      "tensor([[-0.0095, -0.0042, -0.0182,  0.0725, -0.0279, -0.0102,  0.0010, -0.0457,\n",
      "          0.0331,  0.0656],\n",
      "        [-0.0095, -0.0041, -0.0183,  0.0725, -0.0279, -0.0102,  0.0010, -0.0457,\n",
      "          0.0331,  0.0657]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def attention(queries, keys, values):\n",
    "    \"\"\"\n",
    "    Scaled dot-product attention\n",
    "    \n",
    "    Parameters:\n",
    "        queries: Query matrix\n",
    "        keys: Key matrix\n",
    "        values: Value matrix\n",
    "    \n",
    "    Returns:\n",
    "        attention: Attention matrix\n",
    "    \"\"\"\n",
    "    # Convert NumPy arrays to PyTorch tensors\n",
    "    queries = torch.tensor(queries)\n",
    "    keys = torch.tensor(keys)\n",
    "    values = torch.tensor(values)\n",
    "    \n",
    "    # Calculate the dot product of queries and keys\n",
    "    dot_product = torch.matmul(queries, keys.transpose(-2, -1))\n",
    "\n",
    "    # Scale the dot product\n",
    "    scaled_dot_product = dot_product / torch.sqrt(torch.tensor(queries.size(-1)).float())\n",
    "\n",
    "    # Apply softmax to obtain attention weights\n",
    "    attention_weights = F.softmax(scaled_dot_product, dim=-1)\n",
    "\n",
    "    # Calculate the weighted sum using the attention weights\n",
    "    attention_w = torch.matmul(attention_weights, values)\n",
    "\n",
    "    return attention_w\n",
    "\n",
    "\n",
    "attention_result = attention(embedded_fr, embedded_en, embedded_en)\n",
    "print(attention_result.shape)\n",
    "print(attention_result[0:2,:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vlwGjAWMy_OX",
   "metadata": {
    "id": "vlwGjAWMy_OX"
   },
   "source": [
    "####${\\color{red}{Comments\\ 3.2}}$\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
    "\n",
    "\n",
    "```\n",
    "cross-feedback comment section\n",
    "```\n",
    "\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ end⚠️}}$"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
