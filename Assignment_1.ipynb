{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Davo00/INLTP-Assignments/blob/main/Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "5c3754947e9f4287b58fabb0365b87f8",
        "deepnote_cell_type": "markdown",
        "id": "be9f7653"
      },
      "source": [
        "**Heidelberg University**\n",
        "\n",
        "**Data Science  Group**\n",
        "    \n",
        "Prof. Dr. Michael Gertz  \n",
        "\n",
        "Ashish Chouhan, Satya Almasian, John Ziegler, Jayson Salazar, Nicolas Reuter\n",
        "    \n",
        "October 30, 2023\n",
        "    \n",
        "Natural Language Processing with Transformers\n",
        "\n",
        "Winter Semster 2023/2024     \n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00cc26cb17d74c5d9cf7fc80ccd89b3c",
        "deepnote_cell_type": "markdown",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 35,
        "execution_start": 1698673360795,
        "id": "F3RqZ4rQz21E",
        "source_hash": "50c5f835"
      },
      "source": [
        "## Group Names\n",
        "- Simon Pavicic\n",
        "- Davit Melkonyan\n",
        "- Leon Remke\n",
        "- Max Ludwig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "da699d0a94ef4adda6cbb18fc90e329d",
        "deepnote_cell_type": "markdown",
        "id": "258e9648"
      },
      "source": [
        "# **Assignment 1: “Word Embeddings and Probabilistic Language Models”**\n",
        "**Due**: Monday, November 13, 2pm, via [Moodle](https://moodle.uni-heidelberg.de/course/view.php?id=19251)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "e8e7fa61a1ce43ebb2067cea05c50124",
        "deepnote_cell_type": "markdown",
        "id": "fc27ad9e"
      },
      "source": [
        "### **Submission Guidelines**\n",
        "\n",
        "- Solutions need to be uploaded as a **single** Jupyter notebook. You will find several pre-filled code segments in the notebook, your task is to fill in the missing cells.\n",
        "- For the written solution, use LaTeX in markdown inside the same notebook. Do **not** hand in a separate file for it.\n",
        "- Download the .zip file containing the dataset but do **not** upload it with your solution.\n",
        "- It is sufficient if one person per group uploads the solution to Moodle, but make sure that the complete names of all team members are given in the notebook.\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "5eab8af814334d949ad496a3549c9f21",
        "deepnote_cell_type": "markdown",
        "id": "e322e8b0"
      },
      "source": [
        "## **Task 1: F.R.I.E.N.D.S and  Word2Vec (Grade (2 + 2 + 4) = 8)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "7049deddd1c64861b24c4cf213d38c0d",
        "deepnote_cell_type": "markdown",
        "id": "b4ca26ac"
      },
      "source": [
        "[Friends](https://en.wikipedia.org/wiki/Friends) is an American television sitcom, created by David Crane and Marta Kauffman. In this assignment we will use the transcripts from the show to train a Word2Vec model using the [Gensim](https://radimrehurek.com/gensim/) library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "53646b0ed8264cb58c00f0bb8f80ff1e",
        "deepnote_cell_type": "markdown",
        "id": "fc29cb37"
      },
      "source": [
        "### Subtask 1: Pre-processing\n",
        "We start by loading and cleaning the data. Download the dataset for this assignment and load the `friends_quotes.csv` using pandas. The dataset is from Kaggle (https://www.kaggle.com/ryanstonebraker/friends-transcript) and is created for building a classifier that  determines which friend from the Friend's TV Show would be most likely to say a quote. The column `quote` contains a line from the movie and the `author` is the one who said it. Since these are the only two columns we need, we remove the rest and only keep these two columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "59c8f15767044eaaad6390b4b928d87f",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 3539,
        "execution_start": 1698684199184,
        "id": "787e2059",
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import spacy\n",
        "import logging  # Setting up the loggings to monitor gensim\n",
        "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "8326dd8c5d934f4d8df5de2d832c894c",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 418,
        "execution_start": 1698685766269,
        "id": "7e7e8c7d",
        "outputId": "058322cb-03d6-4774-a249-0bb2e1a52086",
        "scrolled": true,
        "source_hash": null
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of data before filtering: (60291, 6)\n",
            "Shape of data after filtering: (60291, 6)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>episode_number</th>\n",
              "      <th>episode_title</th>\n",
              "      <th>quote</th>\n",
              "      <th>quote_order</th>\n",
              "      <th>season</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Monica</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Monica Gets A Roommate</td>\n",
              "      <td>There's nothing to tell! He's just some guy I ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Joey</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Monica Gets A Roommate</td>\n",
              "      <td>C'mon, you're going out with the guy! There's ...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Chandler</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Monica Gets A Roommate</td>\n",
              "      <td>All right Joey, be nice. So does he have a hum...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Phoebe</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Monica Gets A Roommate</td>\n",
              "      <td>Wait, does he eat chalk?</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Phoebe</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Monica Gets A Roommate</td>\n",
              "      <td>Just, 'cause, I don't want her to go through w...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Monica</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Monica Gets A Roommate</td>\n",
              "      <td>Okay, everybody relax. This is not even a date...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Chandler</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Monica Gets A Roommate</td>\n",
              "      <td>Sounds like a date to me.</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Chandler</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Monica Gets A Roommate</td>\n",
              "      <td>Alright, so I'm back in high school, I'm stand...</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>All</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Monica Gets A Roommate</td>\n",
              "      <td>Oh, yeah. Had that dream.</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Chandler</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Monica Gets A Roommate</td>\n",
              "      <td>Then I look down, and I realize there's a phon...</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     author  episode_number           episode_title  \\\n",
              "0    Monica             1.0  Monica Gets A Roommate   \n",
              "1      Joey             1.0  Monica Gets A Roommate   \n",
              "2  Chandler             1.0  Monica Gets A Roommate   \n",
              "3    Phoebe             1.0  Monica Gets A Roommate   \n",
              "4    Phoebe             1.0  Monica Gets A Roommate   \n",
              "5    Monica             1.0  Monica Gets A Roommate   \n",
              "6  Chandler             1.0  Monica Gets A Roommate   \n",
              "7  Chandler             1.0  Monica Gets A Roommate   \n",
              "8       All             1.0  Monica Gets A Roommate   \n",
              "9  Chandler             1.0  Monica Gets A Roommate   \n",
              "\n",
              "                                               quote  quote_order  season  \n",
              "0  There's nothing to tell! He's just some guy I ...          0.0     1.0  \n",
              "1  C'mon, you're going out with the guy! There's ...          1.0     1.0  \n",
              "2  All right Joey, be nice. So does he have a hum...          2.0     1.0  \n",
              "3                           Wait, does he eat chalk?          3.0     1.0  \n",
              "4  Just, 'cause, I don't want her to go through w...          4.0     1.0  \n",
              "5  Okay, everybody relax. This is not even a date...          5.0     1.0  \n",
              "6                          Sounds like a date to me.          6.0     1.0  \n",
              "7  Alright, so I'm back in high school, I'm stand...          7.0     1.0  \n",
              "8                          Oh, yeah. Had that dream.          8.0     1.0  \n",
              "9  Then I look down, and I realize there's a phon...          9.0     1.0  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "FRIENDS_PATH = \"friends_quotes.csv\"\n",
        "\n",
        "df = pd.read_csv(FRIENDS_PATH)\n",
        "print(f\"Shape of data before filtering: {df.shape}\")\n",
        "\n",
        "# Drop rows with any null values\n",
        "df = df.dropna()\n",
        "\n",
        "# Replace empty strings with NaN and then drop those rows\n",
        "df = df.replace('', float('nan')).dropna()\n",
        "\n",
        "# Reset the index after dropping rows\n",
        "df = df.reset_index(drop=True)\n",
        "print(f\"Shape of data after filtering: {df.shape}\")\n",
        "\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "4c31a0e5ddf44a808e5a67460ca91597",
        "deepnote_cell_type": "markdown",
        "id": "be5fc72f"
      },
      "source": [
        "Fortunately, there is no missing data, so we do not need to worry about that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "57b2178d94a84b4dad45bf0c40662dc3",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 62,
        "execution_start": 1698685769630,
        "id": "dc3aea10",
        "outputId": "be9d3f27-587c-4cdb-e06f-f87e7798f80b",
        "source_hash": null
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "author            0\n",
              "episode_number    0\n",
              "episode_title     0\n",
              "quote             0\n",
              "quote_order       0\n",
              "season            0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.isnull().sum() # check for missing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "417b32125a1b47dead08c7acbdda4b14",
        "deepnote_cell_type": "markdown",
        "id": "26a249b1"
      },
      "source": [
        "Use SpaCy to preprocess the text. For this, perform the following steps:\n",
        "- lowercase the words\n",
        "- remove the stopwords and single characters\n",
        "- use regex to remove non-alphabetic characters (anything that is not a number or alphabet including punctuations), in other words only keep \"a\" to \"z\" and digits.\n",
        "- remove lines that have less than 4 words, since they cannot contribute much to the training process.\n",
        "\n",
        "Please do not add any additional steps on your own or additional cleaning as we want to achieve comparable results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "7fb5ee15b93843b197e0c960810f03f6",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 41742,
        "execution_start": 1698685779359,
        "id": "9pIcWDYMz21I",
        "outputId": "9312494e-fc21-4c48-994b-586b5f8657c9",
        "source_hash": null
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stopwords: {'ever', 'up', 'anyway', 'my', \"n't\", 'most', 'please', 'becoming', 'five', 'although', 'latter', 'amount', '‘ve', 'off', 'under', 'around', 'already', 'alone', 'however', 'there', 'below', 'anywhere', 'nine', 'thereupon', '‘ll', 'enough', 'therefore', 'whence', 'eight', 'anyone', 'themselves', 'hereafter', 'everywhere', 'thus', '’d', 'go', 'meanwhile', 'quite', 'does', 'whom', 'still', 'among', 'give', 'hers', 'its', 'which', 'bottom', 'she', 'mine', 'n‘t', 'back', 'sixty', 'he', 'across', 'get', 'put', 'formerly', 'hereupon', 'whatever', 'before', 'thru', 'since', 'without', 'whereafter', 'thence', 'with', 'ourselves', 'another', 'how', 'we', 'except', 'seeming', 'three', 'seems', 'because', 'several', 'whoever', 'upon', 'both', 'sometimes', 'this', 'some', 'beyond', '‘d', 'along', \"'m\", 'had', 'throughout', 'almost', 'any', 'somehow', 'itself', 'per', 'as', 'our', 'here', 'indeed', 'mostly', 'anyhow', 'the', \"'s\", 'herself', 'hundred', 'never', 'besides', 'whole', 'whereby', 'have', 'whither', \"'d\", 'via', 'be', 'cannot', 'may', 'whereupon', 'name', 'top', 'in', 'less', 'else', 'on', 'keep', 'full', \"'re\", 'least', 'above', '’m', 'herein', 're', '’ve', 'one', 'would', 'neither', 'ten', 'part', 'everyone', 'you', '’s', 'ours', 'behind', 'did', 'who', 'should', 'first', 'various', 'yourselves', 'beside', 'really', 'due', 'show', 'out', 'more', 'nowhere', 'thereafter', 'six', 'otherwise', 'last', 'wherein', 'then', 'if', \"'ve\", 'using', 'what', 'why', 'rather', 'toward', 'been', 'your', 'next', 'serious', 'take', 'either', 'a', 'yourself', 'too', 'himself', 'down', 'fifty', 'am', 'therein', 'myself', 'each', 'by', 'not', 'yet', 'many', 'nor', \"'ll\", 'an', 'might', 'and', 'being', 'namely', 'hence', 'but', 'when', 'somewhere', 'see', 'to', '‘s', 'become', 'only', 'no', 'just', 'for', 'anything', 'became', 'move', 'are', 'twenty', 'side', 'latterly', 'it', 'so', 'beforehand', 'few', 'will', 'third', 'sometime', 'used', 'noone', 'that', 'former', 'against', 'into', 'unless', 'at', 'us', 'four', 'was', '’re', 'between', 'everything', 'must', 'of', 'someone', 'over', 'again', 'very', 'me', 'moreover', 'do', 'even', 'has', 'empty', 'much', 'from', 'often', 'such', 'seemed', 'through', 'something', 'after', 'towards', 'those', 'call', 'onto', 'eleven', 'nobody', 'than', 'within', 'regarding', 'him', 'made', 'other', '‘m', 'afterwards', 'always', 'forty', 'front', 'they', 'every', 'until', 'i', 'can', 'is', 'or', 'seem', 'together', 'her', 'becomes', 'say', 'fifteen', 'none', 'same', 'all', 'whether', 'them', 'further', 'whose', 'two', 'once', 'his', 'done', 'about', 'where', 'make', 'while', 'yours', '‘re', 'others', 'could', 'well', 'whenever', 'elsewhere', 'now', 'though', 'twelve', 'ca', 'were', 'these', '’ll', 'wherever', 'during', 'also', 'their', 'perhaps', 'doing', 'thereby', 'n’t', 'nothing', 'amongst', 'hereby', 'own', 'whereas', 'nevertheless'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/simon/opt/anaconda3/envs/INLPT/lib/python3.9/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n"
          ]
        }
      ],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\",\"ner\"])\n",
        "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
        "print(f\"Stopwords: {stopwords}\")\n",
        "\n",
        "def preprocess(doc):\n",
        "    # Remove stopwords and single characters\n",
        "    tokens = [token.text for token in doc if token.text not in stopwords and len(token.text) > 1]\n",
        "\n",
        "    # Use regex to remove non-alphabetic characters\n",
        "    tokens = [re.sub(r'[^a-zA-Z0-9]', '', token) for token in tokens]\n",
        "\n",
        "    if len(tokens) > 3:\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "# lower qoutes\n",
        "lowercased_qoutes = df[\"quote\"].str.lower().tolist()\n",
        "\n",
        "quotes = [preprocess(doc) for doc in nlp.pipe(lowercased_qoutes)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "646cb7f59f14452f96423ca016ca0331",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 79,
        "execution_start": 1698685821063,
        "id": "hJgU-797z21I",
        "outputId": "01b9531d-5619-40ec-9148-91c75f76ec96",
        "source_hash": null
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "remember went central park rented boats  fun\n",
            "know romantic things planned like having picnic central park ya know coffee central perk oh got kiss\n",
            "thought picnic phoebe gasps central park\n",
            "sucks trip came park gon na high stupid central park right house right\n",
            "gon na home bask triumph central park idea gets leave\n",
            "oh joey thought fun central park hit rocks bigger rocks starts leave stops entering rachel hey rach tennis racquet\n"
          ]
        }
      ],
      "source": [
        "# to remove None values in list\n",
        "quotes = [i for i in quotes if i is not None]\n",
        "\n",
        "for quote in quotes:\n",
        "    if \"central park\" in quote:\n",
        "        print(quote)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jpNdsDsz21I",
        "outputId": "75d73ed4-d965-47cc-b704-d5d6effdfaa5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(30940, 1)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_clean = pd.DataFrame({'quotes': quotes})\n",
        "df_clean = df_clean.dropna().drop_duplicates()\n",
        "df_clean.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r00f4jApz21I",
        "outputId": "14534884-0f32-4b66-b24e-cc2180a0339d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>quotes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cm going guy got ta wrong</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>right joey nice hump hump hairpiece</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>cause want went carl oh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>okay everybody relax date people going dinner ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>alright high school standing middle cafeteria ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              quotes\n",
              "0                          cm going guy got ta wrong\n",
              "1                right joey nice hump hump hairpiece\n",
              "2                            cause want went carl oh\n",
              "3  okay everybody relax date people going dinner ...\n",
              "4  alright high school standing middle cafeteria ..."
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_clean.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "a29a00530f96483aa614a78bc7bd8d44",
        "deepnote_cell_type": "markdown",
        "id": "6aaa2190"
      },
      "source": [
        "The next step is to build the vocabulary of the words and word combinations we want to learn representations from. We choose a subset of the most frequent words and bigrams to represent our corpus.\n",
        "- Use the Gensim Phrases package to automatically detect common phrases (bigrams) from a list of lines from the previous step (`min_count=10`). Now words like New_York will be considered as one entity and character names like joey_tribbiani will be recognized.\n",
        "- Create a list of words/bigrams with their frequencies and choose the top 15.000 words for the vocabulary, in order to keep the computation time-limited and to choose the most important words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJ7OJPBkz21J",
        "outputId": "8c461c25-7015-4bd1-f11e-67303e7bc599"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO - 18:06:33: collecting all words and their counts\n",
            "INFO - 18:06:33: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
            "INFO - 18:06:33: PROGRESS: at sentence #10000, processed 85287 words and 63841 word types\n",
            "INFO - 18:06:33: PROGRESS: at sentence #20000, processed 172176 words and 112765 word types\n",
            "INFO - 18:06:33: PROGRESS: at sentence #30000, processed 258784 words and 157583 word types\n",
            "INFO - 18:06:33: collected 161456 token types (unigram + bigrams) from a corpus of 266601 words and 30940 sentences\n",
            "INFO - 18:06:33: merged Phrases<161456 vocab, min_count=10, threshold=10.0, max_vocab_size=40000000>\n",
            "INFO - 18:06:33: Phrases lifecycle event {'msg': 'built Phrases<161456 vocab, min_count=10, threshold=10.0, max_vocab_size=40000000> in 0.25s', 'datetime': '2023-11-06T18:06:33.249894', 'gensim': '4.3.2', 'python': '3.9.18 (main, Sep 11 2023, 08:38:23) \\n[Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n",
            "INFO - 18:06:33: exporting phrases from Phrases<161456 vocab, min_count=10, threshold=10.0, max_vocab_size=40000000>\n",
            "INFO - 18:06:33: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<208 phrases, min_count=10, threshold=10.0> from Phrases<161456 vocab, min_count=10, threshold=10.0, max_vocab_size=40000000> in 0.25s', 'datetime': '2023-11-06T18:06:33.497886', 'gensim': '4.3.2', 'python': '3.9.18 (main, Sep 11 2023, 08:38:23) \\n[Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n"
          ]
        }
      ],
      "source": [
        "from gensim.models.phrases import Phrases, Phraser\n",
        "\n",
        "sent = [line.split(\" \") for line in df_clean[\"quotes\"]]\n",
        "\n",
        "# bigram model\n",
        "phrases = Phrases(sent, min_count=10)\n",
        "bigram = Phraser(phrases)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDoSkH4lz21J",
        "outputId": "e75cd9ed-f1a5-44e0-e9c7-880b1ce6e497"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "16602"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentences = bigram[sent]\n",
        "\n",
        "word_freq = defaultdict(int)\n",
        "for sent in sentences:\n",
        "    for i in sent:\n",
        "        word_freq[i] += 1\n",
        "len(word_freq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GS1dDO9Vz21J",
        "outputId": "d9a3b7bd-f0ae-4f93-d55d-176490b0dd86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sorted list of word frequencies (first 10): [('oh', 4924), ('know', 3752), ('okay', 3611), ('yeah', 3102), ('right', 2745), ('like', 2683), ('gon_na', 2505), ('hey', 2230), ('im', 2197), ('ross', 2125)]\n",
            "Top words: ['oh', 'know', 'okay', 'yeah', 'right', 'like', 'gon_na', 'hey', 'im', 'ross']\n",
            "['oh', 'know', 'okay', 'yeah', 'right', 'like', 'gon_na', 'hey', 'im', 'ross']\n"
          ]
        }
      ],
      "source": [
        "# sort by frequency in descending order\n",
        "sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
        "print(f\"Sorted list of word frequencies (first 10): {sorted_words[:10]}\")\n",
        "\n",
        "# Select the top 15,000 words for the vocabulary\n",
        "top_words = [word for word, freq in sorted_words[:15000]]\n",
        "print(f\"Top words: {top_words[:10]}\")\n",
        "\n",
        "# Display the top words\n",
        "print(top_words[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "1ec5ded163b54ffb92415149a4018376",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 196,
        "execution_start": 1698686046586,
        "id": "9d08031d",
        "outputId": "60a602f8-7ac6-4a8a-844b-e0270c30b5ca",
        "source_hash": null
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "36"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_freq['central_perk']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "671bba1de38646c0a88fda5aaaf545c1",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 21,
        "execution_start": 1698686050404,
        "id": "0882dc8c",
        "outputId": "c544c6ab-5967-4dc0-e90e-422a44288bef",
        "source_hash": null
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1851"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_freq['joey']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "01482298eba6452ab71f4c6e5e11667d",
        "deepnote_cell_type": "markdown",
        "id": "15dfce56"
      },
      "source": [
        "### Subtask 2: Training the Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "4ae7eb1a2fc54d28bb502c445cd8c95c",
        "deepnote_cell_type": "markdown",
        "id": "b59c94e0"
      },
      "source": [
        "Use the Gensim implementation of Word2Vec to train a model on the scripts. The training can be divided into 3 stages:\n",
        "\n",
        "\n",
        "1) Set up your model with parameters; define your parameters in such a way that the following conditions are satisfied:\n",
        " - ignore all words that have a total frequency of less than 2.\n",
        " - dimensions of the embeddings: 100\n",
        " - initial learning rate (step size) of 0.03\n",
        " - 20 negative samples\n",
        " - window size 3\n",
        " - the learning rate in the training will decrease as you apply more and more updates. Most of the time when starting with gradient descent the initial steps can be larger, and as we get close to the local minima it is best to use smaller steps to avoid jumping over the local minima. This adjustment is done internally using a learning rate scheduler. Make sure that the smallest learning rate does not go below 0.0001.\n",
        " - set the threshold for configuring which higher-frequency words are randomly down-sampled to 6e-5. This parameter forces the sampling to choose the very frequent words less often in the sampling.\n",
        " - set the hashfunction of the word2vec to the given function.\n",
        " - train on a single worker to make sure you get the same result as ours."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "9b4af7e9f7bb4476a669a23ddff5d39b",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 11,
        "execution_start": 1698686110604,
        "id": "8SDUpMMhz21K",
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "def hash(astring):\n",
        "    return ord(astring[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "7b8d8990e50642bbadb19e39fbc7a4f0",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 1682,
        "execution_start": 1698686388704,
        "id": "7c12a7a6",
        "outputId": "9fe256ab-873c-455c-e5da-52fb97c3b935",
        "source_hash": null
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO - 18:14:10: Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=100, alpha=0.03>', 'datetime': '2023-11-06T18:14:10.910199', 'gensim': '4.3.2', 'python': '3.9.18 (main, Sep 11 2023, 08:38:23) \\n[Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n"
          ]
        }
      ],
      "source": [
        "w2v = Word2Vec(\n",
        "    min_count=2,\n",
        "    window=3,\n",
        "    vector_size=100,\n",
        "    sample=6e-5,\n",
        "    alpha=0.03,\n",
        "    min_alpha=0.0001,\n",
        "    negative=20,\n",
        "    hashfxn=hash,\n",
        "    workers=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "1dfd2753f5744603919cd923eddef1ed",
        "deepnote_cell_type": "markdown",
        "id": "03eba781"
      },
      "source": [
        "2) Before training, Word2Vec requires us to build the vocabulary table by filtering out the unique words and doing some basic counts on them.\n",
        "Use the `build_vocab` function to process the data. If you look at the logs you can see the effect of `min_count` and `sample` on the word corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "4db8ca7a59d947fbb6fd571cac2cc061",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 2908,
        "execution_start": 1698686473190,
        "id": "d9795441",
        "outputId": "cb4358cd-bd13-4b66-9b27-59b92d0ae28c",
        "scrolled": true,
        "source_hash": null
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO - 18:14:12: collecting all words and their counts\n",
            "INFO - 18:14:12: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "INFO - 18:14:12: PROGRESS: at sentence #10000, processed 80066 words, keeping 9471 word types\n",
            "INFO - 18:14:12: PROGRESS: at sentence #20000, processed 162991 words, keeping 13113 word types\n",
            "INFO - 18:14:12: PROGRESS: at sentence #30000, processed 244025 words, keeping 16345 word types\n",
            "INFO - 18:14:12: collected 16602 word types from a corpus of 251230 raw words and 30940 sentences\n",
            "INFO - 18:14:12: Creating a fresh vocabulary\n",
            "INFO - 18:14:12: Word2Vec lifecycle event {'msg': 'effective_min_count=2 retains 9164 unique words (55.20% of original 16602, drops 7438)', 'datetime': '2023-11-06T18:14:12.650150', 'gensim': '4.3.2', 'python': '3.9.18 (main, Sep 11 2023, 08:38:23) \\n[Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
            "INFO - 18:14:12: Word2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 243792 word corpus (97.04% of original 251230, drops 7438)', 'datetime': '2023-11-06T18:14:12.650903', 'gensim': '4.3.2', 'python': '3.9.18 (main, Sep 11 2023, 08:38:23) \\n[Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
            "INFO - 18:14:12: deleting the raw counts dictionary of 16602 items\n",
            "INFO - 18:14:12: sample=6e-05 downsamples 972 most-common words\n",
            "INFO - 18:14:12: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 114991.68790476587 word corpus (47.2%% of prior 243792)', 'datetime': '2023-11-06T18:14:12.700598', 'gensim': '4.3.2', 'python': '3.9.18 (main, Sep 11 2023, 08:38:23) \\n[Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
            "INFO - 18:14:12: estimated required memory for 9164 words and 100 dimensions: 11913200 bytes\n",
            "INFO - 18:14:12: resetting layer weights\n",
            "INFO - 18:14:12: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-11-06T18:14:12.782656', 'gensim': '4.3.2', 'python': '3.9.18 (main, Sep 11 2023, 08:38:23) \\n[Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'build_vocab'}\n"
          ]
        }
      ],
      "source": [
        "# I had to use lines, otherwise the words joey and central_perk were not in the top 15000 words\n",
        "w2v.build_vocab(sentences, progress_per=10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "2d979180168543c9ac657de95eb52c57",
        "deepnote_cell_type": "markdown",
        "id": "aefdc06f"
      },
      "source": [
        "3) Finally, we  train the model. Train the model for 100 epochs. This will take a while. As we do not plan to train the model any further, we call `init_sims()`, which will make the model much more memory-efficient by precomputing L2-norms of word weight vectors for further analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "9bc14f8821da46e09a477743403c648a",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 19,
        "execution_start": 1698686653132,
        "id": "0febaedc",
        "outputId": "6ab128be-dd27-49c7-c748-0cfde8047ff5",
        "source_hash": null
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO - 18:14:14: Word2Vec lifecycle event {'msg': 'training model with 1 workers on 9164 vocabulary and 100 features, using sg=0 hs=0 sample=6e-05 negative=20 window=3 shrink_windows=True', 'datetime': '2023-11-06T18:14:14.344847', 'gensim': '4.3.2', 'python': '3.9.18 (main, Sep 11 2023, 08:38:23) \\n[Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n",
            "INFO - 18:14:14: EPOCH 0: training on 100833 raw words (69987 effective words) took 0.2s, 343002 effective words/s\n",
            "WARNING - 18:14:14: EPOCH 0: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:14: EPOCH 1: training on 100833 raw words (70059 effective words) took 0.1s, 471495 effective words/s\n",
            "WARNING - 18:14:14: EPOCH 1: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:14: EPOCH 2: training on 100833 raw words (70011 effective words) took 0.1s, 517385 effective words/s\n",
            "WARNING - 18:14:14: EPOCH 2: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:14: EPOCH 3: training on 100833 raw words (70024 effective words) took 0.1s, 527307 effective words/s\n",
            "WARNING - 18:14:14: EPOCH 3: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:15: EPOCH 4: training on 100833 raw words (70041 effective words) took 0.1s, 515234 effective words/s\n",
            "WARNING - 18:14:15: EPOCH 4: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:15: EPOCH 5: training on 100833 raw words (70007 effective words) took 0.2s, 453660 effective words/s\n",
            "WARNING - 18:14:15: EPOCH 5: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:15: EPOCH 6: training on 100833 raw words (70021 effective words) took 0.1s, 491172 effective words/s\n",
            "WARNING - 18:14:15: EPOCH 6: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:15: EPOCH 7: training on 100833 raw words (70010 effective words) took 0.1s, 499940 effective words/s\n",
            "WARNING - 18:14:15: EPOCH 7: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:15: EPOCH 8: training on 100833 raw words (70011 effective words) took 0.1s, 478605 effective words/s\n",
            "WARNING - 18:14:15: EPOCH 8: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:15: EPOCH 9: training on 100833 raw words (69983 effective words) took 0.1s, 525066 effective words/s\n",
            "WARNING - 18:14:15: EPOCH 9: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:16: EPOCH 10: training on 100833 raw words (70021 effective words) took 0.1s, 496295 effective words/s\n",
            "WARNING - 18:14:16: EPOCH 10: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:16: EPOCH 11: training on 100833 raw words (70043 effective words) took 0.1s, 521786 effective words/s\n",
            "WARNING - 18:14:16: EPOCH 11: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:16: EPOCH 12: training on 100833 raw words (70026 effective words) took 0.1s, 517670 effective words/s\n",
            "WARNING - 18:14:16: EPOCH 12: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:16: EPOCH 13: training on 100833 raw words (70029 effective words) took 0.1s, 483161 effective words/s\n",
            "WARNING - 18:14:16: EPOCH 13: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:16: EPOCH 14: training on 100833 raw words (70016 effective words) took 0.2s, 397851 effective words/s\n",
            "WARNING - 18:14:16: EPOCH 14: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:16: EPOCH 15: training on 100833 raw words (70020 effective words) took 0.2s, 381433 effective words/s\n",
            "WARNING - 18:14:16: EPOCH 15: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:16: EPOCH 16: training on 100833 raw words (70027 effective words) took 0.2s, 422277 effective words/s\n",
            "WARNING - 18:14:16: EPOCH 16: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:17: EPOCH 17: training on 100833 raw words (70041 effective words) took 0.1s, 469460 effective words/s\n",
            "WARNING - 18:14:17: EPOCH 17: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:17: EPOCH 18: training on 100833 raw words (70023 effective words) took 0.2s, 441233 effective words/s\n",
            "WARNING - 18:14:17: EPOCH 18: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:17: EPOCH 19: training on 100833 raw words (70033 effective words) took 0.2s, 435211 effective words/s\n",
            "WARNING - 18:14:17: EPOCH 19: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:17: EPOCH 20: training on 100833 raw words (70021 effective words) took 0.2s, 399123 effective words/s\n",
            "WARNING - 18:14:17: EPOCH 20: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:17: EPOCH 21: training on 100833 raw words (70031 effective words) took 0.2s, 387677 effective words/s\n",
            "WARNING - 18:14:17: EPOCH 21: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:18: EPOCH 22: training on 100833 raw words (70048 effective words) took 0.2s, 381563 effective words/s\n",
            "WARNING - 18:14:18: EPOCH 22: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:18: EPOCH 23: training on 100833 raw words (70015 effective words) took 0.2s, 428609 effective words/s\n",
            "WARNING - 18:14:18: EPOCH 23: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:18: EPOCH 24: training on 100833 raw words (70049 effective words) took 0.1s, 476478 effective words/s\n",
            "WARNING - 18:14:18: EPOCH 24: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:18: EPOCH 25: training on 100833 raw words (70051 effective words) took 0.1s, 477908 effective words/s\n",
            "WARNING - 18:14:18: EPOCH 25: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:18: EPOCH 26: training on 100833 raw words (70042 effective words) took 0.1s, 490920 effective words/s\n",
            "WARNING - 18:14:18: EPOCH 26: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:18: EPOCH 27: training on 100833 raw words (70019 effective words) took 0.1s, 484837 effective words/s\n",
            "WARNING - 18:14:18: EPOCH 27: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:18: EPOCH 28: training on 100833 raw words (70037 effective words) took 0.1s, 495587 effective words/s\n",
            "WARNING - 18:14:18: EPOCH 28: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:19: EPOCH 29: training on 100833 raw words (70006 effective words) took 0.1s, 493846 effective words/s\n",
            "WARNING - 18:14:19: EPOCH 29: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:19: EPOCH 30: training on 100833 raw words (69995 effective words) took 0.1s, 490242 effective words/s\n",
            "WARNING - 18:14:19: EPOCH 30: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:19: EPOCH 31: training on 100833 raw words (70022 effective words) took 0.2s, 466500 effective words/s\n",
            "WARNING - 18:14:19: EPOCH 31: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:19: EPOCH 32: training on 100833 raw words (70008 effective words) took 0.1s, 484149 effective words/s\n",
            "WARNING - 18:14:19: EPOCH 32: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:19: EPOCH 33: training on 100833 raw words (70033 effective words) took 0.2s, 385432 effective words/s\n",
            "WARNING - 18:14:19: EPOCH 33: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:19: EPOCH 34: training on 100833 raw words (70011 effective words) took 0.2s, 348132 effective words/s\n",
            "WARNING - 18:14:19: EPOCH 34: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:20: EPOCH 35: training on 100833 raw words (70020 effective words) took 0.2s, 396250 effective words/s\n",
            "WARNING - 18:14:20: EPOCH 35: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:20: EPOCH 36: training on 100833 raw words (70006 effective words) took 0.1s, 474887 effective words/s\n",
            "WARNING - 18:14:20: EPOCH 36: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:20: EPOCH 37: training on 100833 raw words (70027 effective words) took 0.1s, 473328 effective words/s\n",
            "WARNING - 18:14:20: EPOCH 37: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:20: EPOCH 38: training on 100833 raw words (70050 effective words) took 0.2s, 464622 effective words/s\n",
            "WARNING - 18:14:20: EPOCH 38: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:20: EPOCH 39: training on 100833 raw words (70033 effective words) took 0.2s, 402684 effective words/s\n",
            "WARNING - 18:14:20: EPOCH 39: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:20: EPOCH 40: training on 100833 raw words (70032 effective words) took 0.1s, 468119 effective words/s\n",
            "WARNING - 18:14:20: EPOCH 40: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:21: EPOCH 41: training on 100833 raw words (70027 effective words) took 0.2s, 412798 effective words/s\n",
            "WARNING - 18:14:21: EPOCH 41: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:21: EPOCH 42: training on 100833 raw words (70022 effective words) took 0.2s, 437292 effective words/s\n",
            "WARNING - 18:14:21: EPOCH 42: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:21: EPOCH 43: training on 100833 raw words (70046 effective words) took 0.2s, 431866 effective words/s\n",
            "WARNING - 18:14:21: EPOCH 43: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:21: EPOCH 44: training on 100833 raw words (70021 effective words) took 0.2s, 457965 effective words/s\n",
            "WARNING - 18:14:21: EPOCH 44: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:21: EPOCH 45: training on 100833 raw words (70017 effective words) took 0.2s, 461968 effective words/s\n",
            "WARNING - 18:14:21: EPOCH 45: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:21: EPOCH 46: training on 100833 raw words (70003 effective words) took 0.1s, 486213 effective words/s\n",
            "WARNING - 18:14:21: EPOCH 46: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:22: EPOCH 47: training on 100833 raw words (70014 effective words) took 0.1s, 473013 effective words/s\n",
            "WARNING - 18:14:22: EPOCH 47: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:22: EPOCH 48: training on 100833 raw words (70008 effective words) took 0.1s, 488425 effective words/s\n",
            "WARNING - 18:14:22: EPOCH 48: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:22: EPOCH 49: training on 100833 raw words (70022 effective words) took 0.2s, 437873 effective words/s\n",
            "WARNING - 18:14:22: EPOCH 49: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:22: EPOCH 50: training on 100833 raw words (70006 effective words) took 0.2s, 442051 effective words/s\n",
            "WARNING - 18:14:22: EPOCH 50: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:22: EPOCH 51: training on 100833 raw words (70010 effective words) took 0.1s, 511140 effective words/s\n",
            "WARNING - 18:14:22: EPOCH 51: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:22: EPOCH 52: training on 100833 raw words (70010 effective words) took 0.1s, 502304 effective words/s\n",
            "WARNING - 18:14:22: EPOCH 52: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:22: EPOCH 53: training on 100833 raw words (70004 effective words) took 0.1s, 514268 effective words/s\n",
            "WARNING - 18:14:22: EPOCH 53: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:23: EPOCH 54: training on 100833 raw words (70034 effective words) took 0.1s, 490596 effective words/s\n",
            "WARNING - 18:14:23: EPOCH 54: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:23: EPOCH 55: training on 100833 raw words (70020 effective words) took 0.1s, 516324 effective words/s\n",
            "WARNING - 18:14:23: EPOCH 55: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:23: EPOCH 56: training on 100833 raw words (70017 effective words) took 0.1s, 511246 effective words/s\n",
            "WARNING - 18:14:23: EPOCH 56: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:23: EPOCH 57: training on 100833 raw words (70016 effective words) took 0.1s, 496661 effective words/s\n",
            "WARNING - 18:14:23: EPOCH 57: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:23: EPOCH 58: training on 100833 raw words (70030 effective words) took 0.1s, 510015 effective words/s\n",
            "WARNING - 18:14:23: EPOCH 58: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:23: EPOCH 59: training on 100833 raw words (70026 effective words) took 0.1s, 504199 effective words/s\n",
            "WARNING - 18:14:23: EPOCH 59: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:23: EPOCH 60: training on 100833 raw words (70047 effective words) took 0.1s, 523400 effective words/s\n",
            "WARNING - 18:14:23: EPOCH 60: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:24: EPOCH 61: training on 100833 raw words (70023 effective words) took 0.1s, 511942 effective words/s\n",
            "WARNING - 18:14:24: EPOCH 61: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:24: EPOCH 62: training on 100833 raw words (70042 effective words) took 0.1s, 519557 effective words/s\n",
            "WARNING - 18:14:24: EPOCH 62: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:24: EPOCH 63: training on 100833 raw words (70028 effective words) took 0.1s, 510645 effective words/s\n",
            "WARNING - 18:14:24: EPOCH 63: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:24: EPOCH 64: training on 100833 raw words (69998 effective words) took 0.1s, 499041 effective words/s\n",
            "WARNING - 18:14:24: EPOCH 64: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:24: EPOCH 65: training on 100833 raw words (70009 effective words) took 0.1s, 506750 effective words/s\n",
            "WARNING - 18:14:24: EPOCH 65: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:24: EPOCH 66: training on 100833 raw words (70028 effective words) took 0.1s, 512712 effective words/s\n",
            "WARNING - 18:14:24: EPOCH 66: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:24: EPOCH 67: training on 100833 raw words (70011 effective words) took 0.1s, 510451 effective words/s\n",
            "WARNING - 18:14:24: EPOCH 67: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:25: EPOCH 68: training on 100833 raw words (70029 effective words) took 0.1s, 515260 effective words/s\n",
            "WARNING - 18:14:25: EPOCH 68: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:25: EPOCH 69: training on 100833 raw words (70018 effective words) took 0.1s, 505111 effective words/s\n",
            "WARNING - 18:14:25: EPOCH 69: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:25: EPOCH 70: training on 100833 raw words (70037 effective words) took 0.1s, 511680 effective words/s\n",
            "WARNING - 18:14:25: EPOCH 70: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:25: EPOCH 71: training on 100833 raw words (70026 effective words) took 0.1s, 507525 effective words/s\n",
            "WARNING - 18:14:25: EPOCH 71: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:25: EPOCH 72: training on 100833 raw words (70037 effective words) took 0.1s, 510768 effective words/s\n",
            "WARNING - 18:14:25: EPOCH 72: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:25: EPOCH 73: training on 100833 raw words (70022 effective words) took 0.1s, 519201 effective words/s\n",
            "WARNING - 18:14:25: EPOCH 73: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:25: EPOCH 74: training on 100833 raw words (70047 effective words) took 0.1s, 522709 effective words/s\n",
            "WARNING - 18:14:25: EPOCH 74: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:26: EPOCH 75: training on 100833 raw words (70036 effective words) took 0.1s, 502881 effective words/s\n",
            "WARNING - 18:14:26: EPOCH 75: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:26: EPOCH 76: training on 100833 raw words (70028 effective words) took 0.2s, 407881 effective words/s\n",
            "WARNING - 18:14:26: EPOCH 76: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:26: EPOCH 77: training on 100833 raw words (70023 effective words) took 0.2s, 466811 effective words/s\n",
            "WARNING - 18:14:26: EPOCH 77: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:26: EPOCH 78: training on 100833 raw words (70015 effective words) took 0.2s, 461737 effective words/s\n",
            "WARNING - 18:14:26: EPOCH 78: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:26: EPOCH 79: training on 100833 raw words (70057 effective words) took 0.2s, 434872 effective words/s\n",
            "WARNING - 18:14:26: EPOCH 79: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:26: EPOCH 80: training on 100833 raw words (70032 effective words) took 0.2s, 399419 effective words/s\n",
            "WARNING - 18:14:26: EPOCH 80: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:27: EPOCH 81: training on 100833 raw words (70027 effective words) took 0.2s, 315605 effective words/s\n",
            "WARNING - 18:14:27: EPOCH 81: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:27: EPOCH 82: training on 100833 raw words (70040 effective words) took 0.2s, 412592 effective words/s\n",
            "WARNING - 18:14:27: EPOCH 82: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:27: EPOCH 83: training on 100833 raw words (70028 effective words) took 0.2s, 354645 effective words/s\n",
            "WARNING - 18:14:27: EPOCH 83: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:27: EPOCH 84: training on 100833 raw words (70007 effective words) took 0.2s, 326825 effective words/s\n",
            "WARNING - 18:14:27: EPOCH 84: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:27: EPOCH 85: training on 100833 raw words (70051 effective words) took 0.2s, 347941 effective words/s\n",
            "WARNING - 18:14:27: EPOCH 85: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:28: EPOCH 86: training on 100833 raw words (70042 effective words) took 0.2s, 378700 effective words/s\n",
            "WARNING - 18:14:28: EPOCH 86: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:28: EPOCH 87: training on 100833 raw words (70036 effective words) took 0.2s, 368652 effective words/s\n",
            "WARNING - 18:14:28: EPOCH 87: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:28: EPOCH 88: training on 100833 raw words (70045 effective words) took 0.2s, 409820 effective words/s\n",
            "WARNING - 18:14:28: EPOCH 88: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:28: EPOCH 89: training on 100833 raw words (70052 effective words) took 0.2s, 412604 effective words/s\n",
            "WARNING - 18:14:28: EPOCH 89: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:28: EPOCH 90: training on 100833 raw words (70025 effective words) took 0.2s, 407061 effective words/s\n",
            "WARNING - 18:14:28: EPOCH 90: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:29: EPOCH 91: training on 100833 raw words (70037 effective words) took 0.2s, 310354 effective words/s\n",
            "WARNING - 18:14:29: EPOCH 91: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:29: EPOCH 92: training on 100833 raw words (69995 effective words) took 0.2s, 291230 effective words/s\n",
            "WARNING - 18:14:29: EPOCH 92: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:29: EPOCH 93: training on 100833 raw words (70043 effective words) took 0.2s, 396735 effective words/s\n",
            "WARNING - 18:14:29: EPOCH 93: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:29: EPOCH 94: training on 100833 raw words (70040 effective words) took 0.2s, 296090 effective words/s\n",
            "WARNING - 18:14:29: EPOCH 94: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:30: EPOCH 95: training on 100833 raw words (70029 effective words) took 0.2s, 332121 effective words/s\n",
            "WARNING - 18:14:30: EPOCH 95: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:30: EPOCH 96: training on 100833 raw words (70018 effective words) took 0.1s, 495371 effective words/s\n",
            "WARNING - 18:14:30: EPOCH 96: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:30: EPOCH 97: training on 100833 raw words (70042 effective words) took 0.2s, 395543 effective words/s\n",
            "WARNING - 18:14:30: EPOCH 97: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:30: EPOCH 98: training on 100833 raw words (70055 effective words) took 0.2s, 285655 effective words/s\n",
            "WARNING - 18:14:30: EPOCH 98: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:30: EPOCH 99: training on 100833 raw words (70023 effective words) took 0.2s, 424554 effective words/s\n",
            "WARNING - 18:14:30: EPOCH 99: supplied example count (15000) did not equal expected count (30940)\n",
            "INFO - 18:14:30: Word2Vec lifecycle event {'msg': 'training on 10083300 raw words (7002570 effective words) took 16.5s, 425667 effective words/s', 'datetime': '2023-11-06T18:14:30.797276', 'gensim': '4.3.2', 'python': '3.9.18 (main, Sep 11 2023, 08:38:23) \\n[Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(7002570, 10083300)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "w2v.train(top_words, total_examples=w2v.corpus_count, epochs=100, report_delay=1)\n",
        "# w2v.init_sims(replace=True) No longer required for efficiency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "8d7bcc59fa9a4db58253ce90a910afc8",
        "deepnote_cell_type": "markdown",
        "id": "a0c86f82"
      },
      "source": [
        "### Subtask 3: Exploring the Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "b5c5ad8c1f8748b7bca40daf0622a87b",
        "deepnote_cell_type": "markdown",
        "id": "377e1f70"
      },
      "source": [
        "As mentioned in the lecture, word embeddings are suited for similarity and analogy tasks. Let's explore some of that with our dataset:\n",
        "\n",
        "We look for the most similar words to the name of the famous coffee shop where most of the episodes took place, namely `central_perk` and also one of the characters `joey`. If you have followed the exercise correctly until now, you should see that words like `laying` are similar to `central_perk` and the other main characters are also considered similar to `joey`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "f3d08ec040cd4381a22787e1a66a20da",
        "deepnote_cell_type": "code",
        "id": "b7074ffa",
        "outputId": "257d8aeb-87ff-4353-9bcb-a22ad06b7904",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('feel_like', 0.39562320709228516),\n",
              " ('kane', 0.35994401574134827),\n",
              " ('triangle', 0.3595897853374481),\n",
              " ('neurologist', 0.3557359576225281),\n",
              " ('critical', 0.3415874242782593),\n",
              " ('require', 0.32100245356559753),\n",
              " ('smack', 0.3076178729534149),\n",
              " ('delivery', 0.30015745759010315),\n",
              " ('muscle', 0.29952970147132874),\n",
              " ('egypt', 0.29396921396255493)]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "w2v.wv.most_similar(positive=[\"central_perk\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "d38ea41d5e2c4652b220cad504e69dbe",
        "deepnote_cell_type": "code",
        "id": "2e66e435",
        "outputId": "3be7e73b-bf5b-4bb4-ab86-ba19a6540326",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('cox', 0.4008691906929016),\n",
              " ('singers', 0.3855212330818176),\n",
              " ('successful', 0.3553626835346222),\n",
              " ('waitresses', 0.3535996079444885),\n",
              " ('robin', 0.3457915484905243),\n",
              " ('fluid', 0.3353068232536316),\n",
              " ('midnight', 0.33333948254585266),\n",
              " ('obviously', 0.31480520963668823),\n",
              " ('estelle', 0.309605211019516),\n",
              " ('earlier', 0.30446717143058777)]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "w2v.wv.most_similar(positive=[\"joey\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "8b3ebb52da6d4c7abd32422223ad91de",
        "deepnote_cell_type": "markdown",
        "id": "e3591274"
      },
      "source": [
        "Look at the similarity of `green` to `rachel` (her lastname) and `ross`  and `spaceship` (urelated). The first one should have a high and the second a low score. Finally, look at the similarity of `smelly_cat` ( a song from pheobe) and `song` the similarity should be high."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "c9292e5f8e8b4bbab08c1c0a7327036a",
        "deepnote_cell_type": "code",
        "id": "c5de6309",
        "outputId": "db4cd28d-807f-4bd1-c470-45a550ed596b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-0.06260073"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "w2v.wv.similarity(\"green\", 'rachel')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "2792fcb2bd2941b29483e040ea801111",
        "deepnote_cell_type": "code",
        "id": "322d0605",
        "outputId": "88144ea4-5431-4fe6-e88e-8bee32b1eb32"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.15397792"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "w2v.wv.similarity(\"ross\", 'spaceship')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "857a05fc9c1642979acf147eb55ee495",
        "deepnote_cell_type": "code",
        "id": "f7a0a1f5",
        "outputId": "a9b3b6a0-6c78-4c6a-b82b-5f1a0482a7d4",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-0.08080724"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "w2v.wv.similarity(\"smelly_cat\", 'song')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "92a2bdf7949c432e913ff1f58f9518df",
        "deepnote_cell_type": "markdown",
        "id": "7343365d"
      },
      "source": [
        "We can also ask our model to give us the word that does not belong to a list of words. Let's see from the list of all 5 characters which one is the most dissimilar?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "4a5f55cfe2444eb5adcdc32dc8fd810f",
        "deepnote_cell_type": "code",
        "id": "37bff805"
      },
      "outputs": [],
      "source": [
        "character_names= ['joey', 'rachel', 'phoebe','monica','chandler']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "c461f1e0b8a147ef8ab5d0aa2fc398f4",
        "deepnote_cell_type": "code",
        "id": "9020cd09",
        "outputId": "aeaa97f3-d77b-4596-ef69-576f32d79f1f",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'joey'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "w2v.wv.doesnt_match(character_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "7278cc4a122a439cb7bb3be20a1ce94a",
        "deepnote_cell_type": "markdown",
        "id": "28abdaec"
      },
      "source": [
        "Based on the analogies, which word is to `monica` as `man` is to `women`? (print the top 3 words); you should get `chandler`among the answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "30e6af02c91f4e81b37433f0fa70782f",
        "deepnote_cell_type": "code",
        "id": "4a270c9b",
        "outputId": "dd21a5b2-6e51-4653-ac00-89072950c15f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('supposably', 0.39309394359588623),\n",
              " ('nipple', 0.3489933907985687),\n",
              " ('slumber', 0.3425472378730774)]"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "w2v.wv.most_similar(positive=[\"monica\", \"man\"], negative=[\"woman\"], topn=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "5ad13bd66cd04aea8612260b0d8674df",
        "deepnote_cell_type": "markdown",
        "id": "1c97ae83"
      },
      "source": [
        "Finally, lets use [t-SNE](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf) to look at the distribution of our embeddings in the vector space for the character `joey`. Follow the instructions and fill in the blank in the `tsneplot` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "ee0c0d3840564901a6727dcb24609e41",
        "deepnote_cell_type": "code",
        "id": "d0b8930e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import sys\n",
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "aa191d846a5a4fe58a05370e983046fc",
        "deepnote_cell_type": "code",
        "id": "faa51d9e"
      },
      "outputs": [],
      "source": [
        "def tsneplot(model, word):\n",
        "    \"\"\" Plot in seaborn the results from the t-SNE dimensionality reduction for the top 10 most similar and dissimilar words\n",
        "    \"\"\"\n",
        "    embs = np.empty((0, 100), dtype='f')# to save all the embeddings\n",
        "    word_labels = [word]\n",
        "    color_list  = ['green']\n",
        "\n",
        "    # adds the vector of the query word\n",
        "    embs = np.append(embs, model.wv.__getitem__([word]), axis=0)\n",
        "\n",
        "    # gets list of most similar words\n",
        "    close_words = model.wv.most_similar([word])\n",
        "    all_sims = model.wv.most_similar([word], topn=sys.maxsize)\n",
        "    far_words = list(reversed(all_sims[-10:]))\n",
        "\n",
        "    # adds the vector for each of the closest words to the array\n",
        "    for wrd_score in close_words:\n",
        "        wrd_vector = model.wv.__getitem__([wrd_score[0]])\n",
        "        word_labels.append(wrd_score[0])\n",
        "        color_list.append('blue')\n",
        "        embs = np.append(embs, wrd_vector, axis=0)\n",
        "\n",
        "    # adds the vector for each of the furthest words to the array\n",
        "    for wrd_score in far_words:\n",
        "        wrd_vector = model.wv.__getitem__([wrd_score[0]])\n",
        "        word_labels.append(wrd_score[0])\n",
        "        color_list.append('red')\n",
        "        embs = np.append(embs, wrd_vector, axis=0)\n",
        "\n",
        "    np.set_printoptions(suppress=True)\n",
        "    Y = TSNE(n_components=2, random_state=110, perplexity=15).fit_transform(embs)\n",
        "\n",
        "    # sets everything up to plot\n",
        "    df = pd.DataFrame({'x': [x for x in Y[:, 0]],\n",
        "                       'y': [y for y in Y[:, 1]],\n",
        "                       'words': word_labels,\n",
        "                       'color': color_list})\n",
        "\n",
        "    fig, _ = plt.subplots()\n",
        "    fig.set_size_inches(10, 10)\n",
        "\n",
        "    # basic plot\n",
        "    p1 = sns.regplot(data=df,\n",
        "                     x=\"x\",\n",
        "                     y=\"y\",\n",
        "                     fit_reg=False,\n",
        "                     marker=\"o\",\n",
        "                     scatter_kws={'s': 40,\n",
        "                                  'facecolors': df['color']\n",
        "                                 }\n",
        "                    )\n",
        "\n",
        "    # adds annotations one by one with a loop\n",
        "    for line in range(0, df.shape[0]):\n",
        "         p1.text(df[\"x\"][line],\n",
        "                 df['y'][line],\n",
        "                 '  ' + df[\"words\"][line].title(),\n",
        "                 horizontalalignment='left',\n",
        "                 verticalalignment='bottom', size='medium',\n",
        "                 color=df['color'][line],\n",
        "                 weight='normal'\n",
        "                ).set_size(15)\n",
        "\n",
        "\n",
        "    plt.xlim(Y[:, 0].min()-1, Y[:, 0].max()+1)\n",
        "    plt.ylim(Y[:, 1].min()-1, Y[:, 1].max()+1)\n",
        "\n",
        "    plt.title('t-SNE visualization for {}'.format(word.title()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "725afc6c92ba4f908ec460419775eb18",
        "deepnote_cell_type": "code",
        "id": "3880bad8",
        "outputId": "5b75d388-b804-45f7-d959-03b35796eb35",
        "scrolled": false
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x1000 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tsneplot(w2v, 'joey')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "14d7e861f1dc4204a8e39b27547dd9c4",
        "deepnote_cell_type": "markdown",
        "id": "1ce2c270"
      },
      "source": [
        "## **Task 2: Questions on the Conceptual Level (non-programming) (Grade (2 + 1 + 1 + 4) = 8)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "ace660a64baf473d9724dbc2ff371725",
        "deepnote_cell_type": "markdown",
        "id": "c3fa409f"
      },
      "source": [
        "Please answer the following questions in the notebook cells using markdown. Be precise and short."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "24fb0c7ae4074588bdb05a3d9a3dfcf6",
        "deepnote_cell_type": "markdown",
        "id": "1ec614b0"
      },
      "source": [
        "### Subtask 1: For gradient descent, what advantage has a decaying learning rate?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "e75be01ffafa494a9ba4d6504c083822",
        "deepnote_cell_type": "markdown",
        "id": "bec059fb"
      },
      "source": [
        "<<< your answer >>>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "ce28e2169a524b7daf96af474b569360",
        "deepnote_cell_type": "markdown",
        "id": "0af2b619"
      },
      "source": [
        "### Subtask 2: Why is it easier to maximize the log likelihood instead of the \"normal\" likelihood?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "a4da45940d9b456ba937eefbda717f62",
        "deepnote_cell_type": "markdown",
        "id": "deb75406"
      },
      "source": [
        "<<< your answer >>>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "55401062c4414e22b07999c9516ddd47",
        "deepnote_cell_type": "markdown",
        "id": "a9828a2f"
      },
      "source": [
        "### Subtask 3: Name one advantage that fastText has over Word2Vec?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "37206cb5835d458ca95650ef71670421",
        "deepnote_cell_type": "markdown",
        "id": "6c7aeb3b"
      },
      "source": [
        "<<< your answer >>>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "1296e557d1ea498e9c6a52fe8fbcb8b1",
        "deepnote_cell_type": "markdown",
        "id": "07634ed8"
      },
      "source": [
        "### Subtask 4: Compute the partial derivate of softmax loss for word2vec with\n",
        "\n",
        "---\n",
        "\n",
        "respect to the center word vector.\n",
        "$$ \\frac{\\partial J}{\\partial v_c} =\\frac{\\partial}{\\partial v_c}\\left[ -log \\left( \\frac{exp(u^T_o v_c)}{\\Sigma_{w \\in Vocab} exp(u^T_o v_c)} \\right)\\right] $$\n",
        "use $U$ to denote the matrix of all embeddings and $y$ for a one-hot vector with a 1 for the true outside word $o$, and $\\hat{y}$ for the predicted distribution $P(w|c)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "bf7f894433d94dde90e01b3a129333a6",
        "deepnote_cell_type": "markdown",
        "id": "baba8a44"
      },
      "source": [
        "<<< your answer >>>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "1db960c2184f4d86a99007290ddb7d53",
        "deepnote_cell_type": "markdown",
        "id": "8364e05b"
      },
      "source": [
        "## **Task 3: Auto-Complete Feature (Grade (2 + 6 + 4) = 12)**\n",
        "\n",
        "Let's get even more practical! In this problem set, you will build your own auto-completion system that you see every day while using search engines.\n",
        "\n",
        "[google]: https://www.thedad.com/wp-content/uploads/2018/05/screen-shot-2018-05-12-at-2-01-56-pm.png \"google auto complete\"\n",
        "\n",
        "![google]\n",
        "By the end of this assignment, you will develop a simple prototype of such a system using n-gram language model. At the heart of the system is a language model that assigns the probability to a sequence of words. We take advantage of this probability calculation to predict the next word.\n",
        "\n",
        "The problem set contains 3 main parts:\n",
        "\n",
        "1. Load and preprocess data (tokenize and split into train and test)\n",
        "2. Develop n-gram based language model by estimating the conditional probability of the next word.\n",
        "3. Evaluate the model by computing the perplexity score.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "8f2e3c8d05744ca2ba9d6954068ccc4f",
        "deepnote_cell_type": "markdown",
        "id": "c9d5f84e"
      },
      "source": [
        "### Subtask 1: Load and Preprocess Data\n",
        "We use a subset of English tweets to train our model. Run the cell below to load the data and observe a few lines of it. Notice that tweets are saved in a text file, where the individual tweets are separated by `\\n`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cell_id": "58f762e9ed1d424fbe3ff9709f85a931",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "deepnote_cell_type": "code",
        "id": "48fc8e51",
        "outputId": "fdca5c58-4cde-4f2a-c405-8acb99e2017c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 500 characters of the data:\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"How are you? Btw thanks for the RT. You gonna be in DC anytime soon? Love to see you. Been way, way too long.\\nWhen you meet someone special... you'll know. Your heart will beat more rapidly and you'll smile for no reason.\\nthey've decided its more fun if I don't.\\nSo Tired D; Played Lazer Tag & Ran A LOT D; Ughh Going To Sleep Like In 5 Minutes ;)\\nWords from a complete stranger! Made my birthday even better :)\\nFirst Cubs game ever! Wrigley field is gorgeous. This is perfect. Go Cubs Go!\\ni no! i ge\""
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\",\"ner\"])\n",
        "\n",
        "\n",
        "with open(\"./twitter.txt\", \"r\") as f:\n",
        "    data = f.read()\n",
        "print(\"First 500 characters of the data:\")\n",
        "display(data[0:500])\n",
        "print(\"-------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "80b3757b35d046eab04bb777b6a416b6",
        "deepnote_cell_type": "markdown",
        "id": "aa007e1d"
      },
      "source": [
        "Now we need to separate the tweets and split them into train and test set. Apply the following pre-processing steps:\n",
        "\n",
        "1. Split data into sentences using \"\\n\" as the delimiter and remove the leading and trailing spaces (drop empty sentences)\n",
        "2. Tokenize the sentences into words using SpaCy and lowercase them. (notice that we do not remove stop words or punctuations.)\n",
        "3. Divide the sentences into 80 percent training and 20 percent test set. No validation set is required. Although in a real-world application it is best to set aside part of the data for hyperparameter tuning.\n",
        "4. To limit the vocabulary and remove potential spelling mistakes, make a vocabulary of the words that appear at least 2 times. The rest of the words will be replaced by the `<unk>` symbol. This is a crucial step since if your model encounters a word that it never saw during training, it won't have an input word that helps determining the next word for suggestion. We use the `<unk>` word for **out of Vocabulary (OOV)** words. Keep in mind that we built the vocabulary on the training data only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cell_id": "aacd6837aaf646eea5c57d147ae86251",
        "deepnote_cell_type": "code",
        "id": "16d5b640"
      },
      "outputs": [],
      "source": [
        "sentences = data.split(\"\\n\")\n",
        "sentences = list(map(lambda s: s.strip(), sentences))\n",
        "# remove empty string as well\n",
        "sentences.remove(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cell_id": "14d26cc808a64f57972f5fc17543c798",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deepnote_cell_type": "code",
        "id": "2faabb7a",
        "outputId": "7dd3cb1f-8b76-4a90-e5b1-95ce1c411f8d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n"
          ]
        }
      ],
      "source": [
        "tokenized_corpus = []# list of list of the tokens in a sentence\n",
        "for s in sentences:\n",
        "    doc = nlp(s)\n",
        "    tokens = [token.text.lower() for token in doc]\n",
        "    tokenized_corpus.append(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cell_id": "99e828609a5b46839a99a2be980c8769",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deepnote_cell_type": "code",
        "id": "235d1b53",
        "outputId": "e99e8597-0d81-4d18-e3f4-f123633baa20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "overall length: 47961, train: 38369, test: 9592\n"
          ]
        }
      ],
      "source": [
        "from random import Random\n",
        "Random(4).shuffle(tokenized_corpus)\n",
        "split = round(len(tokenized_corpus)*0.8)\n",
        "\n",
        "train = tokenized_corpus[:split]\n",
        "test = tokenized_corpus[split:]\n",
        "\n",
        "print(f\"overall length: {len(tokenized_corpus)}, train: {len(train)}, test: {len(test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cell_id": "d18ffa61ccda41e1aed028f98aef4013",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deepnote_cell_type": "code",
        "id": "2526e4b1",
        "outputId": "d79ff069-f7c6-46cb-c917-5295270f7c30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14861\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "flatten_corpus = [item for sublist in train for item in sublist]\n",
        "word_counts = Counter(flatten_corpus)\n",
        "vocab = [k for k,v in word_counts.items() if v >= 2]\n",
        "\n",
        "### keep only the ones with frequency bigger than 2 ###\n",
        "print(len(vocab)) ### should be 14861 ###\n",
        "train_replaced = []\n",
        "test_replaced = []\n",
        "for sentence in train:\n",
        "    train_replaced.append([word if word in vocab else \"<unk>\" for word in sentence])\n",
        "for sentence in test:\n",
        "    test_replaced.append([word if word in vocab else \"<unk>\" for word in sentence])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "1cf527629aa04e3b934ece7dc59724c9",
        "deepnote_cell_type": "markdown",
        "id": "f447294a"
      },
      "source": [
        "### Subtask 2: N-gram Based Language Model:\n",
        "In this section, you will develop an n-grams language model [**1. Large Language Models (LLMs), slide 1-24**]. We assume that the probability of the next word depends only on the previous n-gram or previous n words. We compute this probability by counting the occurrences in the corpus.\n",
        "The conditional probability for the word at position 't' in the sentence, given that the words preceding it are $w_{t-1}, w_{t-2} \\cdots w_{t-n}$ can be estimated as follows:\n",
        "\n",
        "$$ \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_t)}{C(w_{t-1}\\dots w_{t-n})}  $$\n",
        "\n",
        "The numerator is the number of times word '$w_t$' appears after the n-gram, and the denominator is the number of times the n-gram occurs in the corpus, where $C(\\cdots)$ is a count function. Later, we add k-smoothing to avoid errors when any counts are zero."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "b8907d5a38c144708623c6b664f12085",
        "deepnote_cell_type": "markdown",
        "id": "7a590b34"
      },
      "source": [
        "To tackle the problem of probability estimation we divide the problem into 3 parts. In the following you will:\n",
        "1. Implement a function that computes the counts of n-grams for an arbitrary number n.\n",
        "2. Estimate the probability of a word given the prior n-words using the n-gram counts.\n",
        "3. Calculate probabilities for all possible words.\n",
        "The steps are detailed below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "491396e5d91545b492d55fd8830bac8f",
        "deepnote_cell_type": "markdown",
        "id": "8da09885"
      },
      "source": [
        "Let's start by implementing a function that computes the counts of n-grams for an arbitrary number n.\n",
        "- Prepend necessary starting markers `<s>` to indicate the beginning of the sentence. In the case of a bi-gram model, you need to prepend two start tokens `<s><s>` to be able to predict the first word. \"hello world\"-> \"`<s><s>`hello world\".\n",
        "- Append an end token `<e>` so that the model can predict when to finish a sentence.\n",
        "- Create a dictionary to store all the n_gram counts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cell_id": "1831e1e0042c44928f0804fb52e4b056",
        "deepnote_cell_type": "code",
        "id": "e8a98cee"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "def n_grams_counts(corpus, n):\n",
        "    \"\"\"\n",
        "    Count all n-grams in the corpus given the parameter n\n",
        "\n",
        "    data: List of lists of words (your tokenized corpus)\n",
        "    n: n in the n-gram\n",
        "\n",
        "    Returns: A dictionary that maps a tuple of n words to its frequency\n",
        "    \"\"\"\n",
        "    start_token='<s>'\n",
        "    end_token = '<e>'\n",
        "    n_grams = defaultdict(int)\n",
        "    for sentence in corpus:\n",
        "        sentence = [start_token] * n + sentence + [end_token] * n\n",
        "        # convert list to tuple so it can be used as the key in the dictionary\n",
        "        sentence = tuple(sentence)\n",
        "\n",
        "        ###iterate over the n-grams in the sentence, you can use the range() function, and increament the counts in the\n",
        "        ## n_grams dictionary, where the key is the n_gram and the value is count\n",
        "\n",
        "        for idx in range(0, len(sentence)-(n-1)):\n",
        "            n_gram = sentence[idx:idx+n]\n",
        "            n_grams[n_gram] +=1\n",
        "    return n_grams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "1b61c4f79d0b4398a3381a6b49ec9bb9",
        "deepnote_cell_type": "markdown",
        "id": "ba75f530"
      },
      "source": [
        "The next step is to estimate the probability of a word given the prior n words using the n-gram counts, based on the formula given at the beginning of this task. To deal with the problem of zero division we add k-smoothing. K-smoothing adds a positive constant $k$ to each numerator and $k \\times |vocabulary size|$ in the denominator. Below we will define a function that takes in a dictionary `n_gram_cnt`, where the key is the n-gram, and the value is the count of that n-gram, plus a dictionary for `plus_current_gram_cnt`, which you'll use to find the count for the previous n-gram plus the current word. Notice that these dictionaries are computed using the previous function `n_grams_counts`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cell_id": "f7953b233226442db6b6bfa34e210faf",
        "deepnote_cell_type": "code",
        "id": "a2c6898d"
      },
      "outputs": [],
      "source": [
        "def probability(word, prev_n_gram, n_gram_cnts, plus_current_gram_cnts, vocab_size):\n",
        "    \"\"\"\n",
        "    Estimate the probabilities of the next word using the n-gram counts with k-smoothing\n",
        "    word: next word\n",
        "    prev_n_gram: previous n gram\n",
        "    n_gram_cnts: dictionary of counts of n-grams\n",
        "    plus_current_gram_cnts: dictionary of counts of (n+1)-grams -> current word+ n-gram = (n+1)-gram\n",
        "    vocab_size: number of words in the vocabulary\n",
        "\n",
        "    Returns: A probability\n",
        "    \"\"\"\n",
        "    k=1.0\n",
        "    prev_n_gram = tuple(prev_n_gram)\n",
        "\n",
        "    prev_n_gram_cnt =  n_gram_cnts[prev_n_gram]\n",
        "    denominator = (k * vocab_size) + prev_n_gram_cnt\n",
        "    plus_current_gram =  prev_n_gram + (word,)\n",
        "    plus_current_gram_cnt = plus_current_gram_cnts[plus_current_gram]\n",
        "    numerator = k + plus_current_gram_cnt\n",
        "    prob = numerator / denominator\n",
        "\n",
        "    return prob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "0ace7921a06e4a0fbcb65e145a7b1887",
        "deepnote_cell_type": "markdown",
        "id": "7803eb7c"
      },
      "source": [
        "Let's use the functions we have defined to calculate probabilities for all possible words.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "cell_id": "dcaccf27e5f34299b5b9cb23de52581a",
        "deepnote_cell_type": "code",
        "id": "7157c4cb"
      },
      "outputs": [],
      "source": [
        "def probabilities(prev_n_gram, n_gram_cnts, plus_current_gram_cnts, vocab):\n",
        "    \"\"\"\n",
        "    Estimate the probabilities for all the words in the vocabulary given the previous n-gram\n",
        "    prev_n_gram: previous n-gram\n",
        "    n_gram_cnts: dictionary of counts of n-grams\n",
        "    plus_current_gram_cntsplus_current_gram_cnt: dictionary of counts of (n+1)-grams -> current word+ n-gram = (n+1)-gram\n",
        "    vocab: List of words\n",
        "\n",
        "    Returns: A dictionary mapping from next words to the probability.\n",
        "    \"\"\"\n",
        "    prev_n_gram = tuple(prev_n_gram)\n",
        "\n",
        "    vocab =  vocab + [\"<e>\", \"<unk>\"]\n",
        "    vocabulary_size = len(vocab)\n",
        "\n",
        "    probabilities = {}\n",
        "    for word in vocab:\n",
        "        prob = probability(word, prev_n_gram, n_gram_cnts, plus_current_gram_cnts, vocabulary_size)\n",
        "        probabilities[word] = prob\n",
        "    return probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cell_id": "69b27da7a72540d7b98314cc06ee03d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deepnote_cell_type": "code",
        "id": "88fea049",
        "outputId": "aea4ec32-01db-4a6d-ba10-cb68a09c644a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The word 'moon' should have the highest probability, if it is not the case, re-visit your previous functions.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'tonight': 0.05263157894736842,\n",
              " 'shinnig': 0.05263157894736842,\n",
              " 'mars': 0.05263157894736842,\n",
              " 'moon': 0.21052631578947367,\n",
              " 'stars': 0.05263157894736842,\n",
              " 'shining': 0.05263157894736842,\n",
              " 'a': 0.05263157894736842,\n",
              " 'and': 0.05263157894736842,\n",
              " 'is': 0.05263157894736842,\n",
              " 'plants': 0.05263157894736842,\n",
              " 'plant': 0.05263157894736842,\n",
              " 'are': 0.05263157894736842,\n",
              " 'bright': 0.05263157894736842,\n",
              " 'the': 0.05263157894736842,\n",
              " '<e>': 0.05263157894736842,\n",
              " '<unk>': 0.05263157894736842}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Predict the probability of all possible words after the unigram \"the\"\n",
        "sentences = [['the', 'moon', 'and', 'stars', 'are','shining','bright'],\n",
        "             ['the', 'moon', 'is', 'shinnig','tonight'],\n",
        "             ['mars','and' ,'moon', 'are', 'plants'],\n",
        "             ['the' ,'moon', 'is','a', 'plant']]\n",
        "unique_words = list(set(sentences[0] + sentences[1] + sentences[2]+ sentences[3]))\n",
        "unigram_counts = n_grams_counts(sentences, 1)\n",
        "bigram_counts = n_grams_counts(sentences, 2)\n",
        "print(\"The word 'moon' should have the highest probability, if it is not the case, re-visit your previous functions.\")\n",
        "probabilities([\"the\"], unigram_counts, bigram_counts, unique_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "9ccfa9c6597d462793202b50b314aa8d",
        "deepnote_cell_type": "markdown",
        "id": "014a385f"
      },
      "source": [
        "### Subtask 3: Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "176162b78f544ee19a82a30f807ec9ee",
        "deepnote_cell_type": "markdown",
        "id": "90a4345f"
      },
      "source": [
        "In this part, we use the perplexity score to evaluate your model on the test set. The perplexity score of the test set on an n-gram model is defined as follows:\n",
        "\n",
        "$$ PP(W) =\\sqrt[N]{ \\prod_{t=n}^{N-1} \\frac{1}{P(w_t | w_{t-n} \\cdots w_{t-1})} } $$\n",
        "- where $N$ is the length of the sentence. ($N-1$ is used because in the code we start from the index 0).\n",
        "- $n$ is the number of words in the n-gram.\n",
        "- $W$ is the n-gram\n",
        "\n",
        "Notice that we have already computed this probability.\n",
        "\n",
        "The higher the probabilities are, the lower the perplexity will be."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "cell_id": "2f8f6acf9f11492f8f1305fdf99c31de",
        "deepnote_cell_type": "code",
        "id": "9644c33f"
      },
      "outputs": [],
      "source": [
        "def perplexity(sentence, n_gram_cnts, plus_current_gram_cnts, vocab_size, k=1.0):\n",
        "    \"\"\"\n",
        "    Calculate perplexity for a list of sentences\n",
        "    sentence: List of strings\n",
        "    n_gram_cnts: dictionary of counts of n-grams\n",
        "    plus_current_gram_cnts: dictionary of counts of (n+1)-grams -> current word+ n-gram = (n+1)-gram\n",
        "    vocab_size: number of unique words in the vocabulary\n",
        "    k: positive smoothing constant\n",
        "\n",
        "    Returns: Perplexity score for a single sentence\n",
        "    \"\"\"\n",
        "\n",
        "    n =  len(list(n_gram_cnts.keys())[0])\n",
        "    original_sentence = sentence\n",
        "    sentence =  [\"<s>\"] * n + sentence + [\"<e>\"] * n\n",
        "    sentence = tuple(sentence)\n",
        "    N = len(sentence)\n",
        "\n",
        "\n",
        "    product_pi = 1.0\n",
        "\n",
        "    ### Compute the product of probabilities ###\n",
        "\n",
        "    for t in range(n, N):\n",
        "        n_gram = sentence[t-n:t]# get the n-gram before the predicted word (n-gram before t )\n",
        "        word =  sentence[t]\n",
        "        prob = probability(word, n_gram, n_gram_cnts,  plus_current_gram_cnts, vocab_size)\n",
        "        product_pi *= 1/prob\n",
        "\n",
        "    perplexity = product_pi**(1/float(N))\n",
        "    return perplexity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "7f83ee8ea3f540b0bec43660bbbb4182",
        "deepnote_cell_type": "markdown",
        "id": "8745e1bc"
      },
      "source": [
        "Use `perplexity` function to find the perplexity of a bi-gram model on the first training sample and on the first test sample (first element of the set)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "cell_id": "cb594a2f6b6541ca9c12ef418bf2f571",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deepnote_cell_type": "code",
        "id": "d857043a",
        "outputId": "6e7963d0-c1f0-4d62-be8b-cec014c89751"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perplexity for first train sample: 85.7743\n",
            "Perplexity for test sample: 1032.9659\n"
          ]
        }
      ],
      "source": [
        "bigram_counts = n_grams_counts(train_replaced, 2)\n",
        "trigram_counts = n_grams_counts(train_replaced, 3)\n",
        "\n",
        "perplexity_train = perplexity(train_replaced[0],bigram_counts, trigram_counts,len(vocab))\n",
        "print(f\"Perplexity for first train sample: {perplexity_train:.4f}\")\n",
        "\n",
        "perplexity_test = perplexity(test_replaced[0],bigram_counts, trigram_counts,len(vocab))\n",
        "print(f\"Perplexity for test sample: {perplexity_test:.4f}\")# the perplexity for the train sample should be much lower"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "583838d297e949249a58d4a1dc31b9fb",
        "deepnote_cell_type": "markdown",
        "id": "7c697faa"
      },
      "source": [
        "Finally, let's use the model we created to generate an auto-complete system that makes suggestions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "cell_id": "49b35d180c85431f9c2ef7af0f6663d4",
        "deepnote_cell_type": "code",
        "id": "b6d6163b"
      },
      "outputs": [],
      "source": [
        "def suggest_a_word(up_to_here, n_gram_cnts, plus_current_gram_cnts, vocab , start_with=None):\n",
        "    \"\"\"\n",
        "    Get suggestion for the next word\n",
        "    up_to_here: the sentence so far, must have length > n\n",
        "    n_gram_cnts: dictionary of counts of n-grams\n",
        "    plus_current_gram_cnts: dictionary of counts of (n+1)-grams -> current word+ n-gram = (n+1)-gram\n",
        "    vocab: List of words\n",
        "    start_with: If not None, specifies the first few letters of the next word\n",
        "\n",
        "    Returns: (most likely next word,  probability)\n",
        "    \"\"\"\n",
        "    n = len(list(n_gram_cnts.keys())[0]) # get the number 'n' in  n-gram  from n_gram_cnts\n",
        "    previous_n_gram = previous_tokens[-n:] # get the last 'n' words as the previous n-gram from the input sentence\n",
        "\n",
        "\n",
        "    probabs = probabilities(previous_n_gram, n_gram_cnts, plus_current_gram_cnts, vocab)\n",
        "\n",
        "    ### sort the probability for higher to lower and return the highest probability word,probability tuple\n",
        "    #if start_with is specified then return the highest probability word that starts with that specific character\n",
        "\n",
        "    sorted_dict_as_list = sorted(probabs.items(), key=lambda x:x[1],reverse=True)\n",
        "    if start_with == None:\n",
        "        # do not return the <unk> symbol if it is the most common one, as it is not helpful at all\n",
        "        return sorted_dict_as_list[0] if sorted_dict_as_list[0][0] != \"<unk>\" else sorted_dict_as_list[1]\n",
        "    else:\n",
        "        for item in sorted_dict_as_list:\n",
        "            if item[0].startswith(start_with):\n",
        "                return item"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "7d12dbd16a224d3bb0b676614c3263ae",
        "deepnote_cell_type": "markdown",
        "id": "f88f9abc"
      },
      "source": [
        "Test your model based on the bi-gram model created on the training corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "cell_id": "b34a4567484945d0af203f460c78ac29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deepnote_cell_type": "code",
        "id": "460dff09",
        "outputId": "2ce27b7d-bd56-4e95-dfd1-492b689ce8a1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('better', 0.00013324450366422385)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "previous_tokens=['i','like']\n",
        "start_with='b'\n",
        "suggestion = suggest_a_word(previous_tokens, bigram_counts,trigram_counts, list(vocab),start_with=start_with)\n",
        "suggestion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "cell_id": "a88d32a32d2449e9a675b69ded886d28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deepnote_cell_type": "code",
        "id": "e68f730c",
        "outputId": "b0d808a8-9c09-4a4c-9698-06c1c0218f3d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('see', 0.0006017249448418801)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "previous_tokens=['i','like','to']\n",
        "start_with=None\n",
        "suggestion = suggest_a_word(previous_tokens, bigram_counts,trigram_counts, list(vocab),start_with=start_with)\n",
        "suggestion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "cell_id": "52be0e5887c74aa0b7da9fa207b7e9bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deepnote_cell_type": "code",
        "id": "001a7ff4",
        "outputId": "f360df27-fe66-4587-f283-e7d85e92423a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('riley', 0.00020153164046755341)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "previous_tokens=[\"hello\", \"my\", \"name\", \"is\"]\n",
        "start_with=None\n",
        "suggestion = suggest_a_word(previous_tokens, bigram_counts,trigram_counts, list(vocab),start_with=start_with)\n",
        "suggestion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "cell_id": "068963f618b544f7a786a781626bf295",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deepnote_cell_type": "code",
        "id": "c556e16f",
        "outputId": "61e1d15c-5a39-46eb-cb62-fa1c6daaecd3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('allison', 0.00013435442697836894)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "previous_tokens=[\"hello\", \"my\", \"name\", \"is\"]\n",
        "start_with='a'\n",
        "suggestion = suggest_a_word(previous_tokens, bigram_counts,trigram_counts, list(vocab),start_with=start_with)\n",
        "suggestion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "50f4e5d8be004436bd0398c0d2836135",
        "deepnote_cell_type": "markdown",
        "id": "b4a15ac8"
      },
      "source": [
        "## **Task 4: Understanding GloVe (Grade (2 + 4.25 + 4 + 1.75) = 12)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "28bf5028a4d14fa28da11a21ec934de7",
        "deepnote_cell_type": "markdown",
        "id": "15e586ca"
      },
      "source": [
        "In this part, you will implement the [GloVe](https://nlp.stanford.edu/projects/glove/) model and train your own word vectors with gradient\n",
        "descent and numpy. GloVe stands for Global Vectors for word representation, which was developed by researchers at Stanford University to generate word embeddings from corpus statistics.\n",
        "The statistics of the corpus are represented by a co-occurrence matrix, indicating how often a particular word pair occurs together.\n",
        "GloVe is based on ratios of probabilities from this co-occurrence matrix, combining the intuitions of count-based models while also being similar to neural models like word2vec.\n",
        "From this matrix, one can compute the co-occurrence probabilities. We motivate this with an example:\n",
        "\n",
        "$P_{ik} = P(i,j)$ -> co-occurrence probability or joint probability of words $i$ and $k$\n",
        "\n",
        "$P_{jk}$ ->co-occurrence probability of words $j$ and $k$\n",
        "\n",
        "$\\frac{P_{ik}}{P_{jk}}$-> corelation between $i$ and $j$ with the prob word $k$\n",
        "\n",
        "This ratio gives us some insight into the co-relation of the probe word $k$ with the words $i$ and $j$.\n",
        "An example is shown below for different prob words $k$, where $i$ and $j$ are `ice` and `steam`, respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "1bbb4dae5beb4beabae651222c316a93",
        "deepnote_cell_type": "markdown",
        "id": "076234c5"
      },
      "source": [
        "\n",
        "![glove](https://nlp.stanford.edu/projects/glove/images/table.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "a24cac6213ce4cabbbcff9053b74e2ac",
        "deepnote_cell_type": "markdown",
        "id": "3b933587"
      },
      "source": [
        "Image taken from Stanford NLP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "4af664b4603d4aa0a52783311c0d3834",
        "deepnote_cell_type": "markdown",
        "id": "76e8f6f0"
      },
      "source": [
        "Compared to the raw probabilities, the ratio of probabilities is better able to distinguish relevant words from irrelevant words. Consider the raw probabilities (the first two rows), the values are close to one another and not indicative of the relationships. However, the ratios have more distinct values.\n",
        "\n",
        "This ratio can be small, large, or equal to 1 depending on the prob word and their co-relation. In the example above, the ratio between ice and steam for `k=solid` is large and for `k=gas` is small, indicating that `solid` is related to ice but `gas` is irrelevant. On the other hand, `water` is not a discriminating element between them, and therefore the ratio is close to one. The same applies to an irrelevant word like `fashion`.\n",
        "\n",
        "The GloVe model is built on the idea that the \"ratio of conditional probabilities represents the word meanings\" and a neural model is trained to estimate this conditional probability.\n",
        "\n",
        "$F(w_i,w_j,\\tilde{w_k})=\\frac{P_ik}{P_jk}$ -> the right-hand side is computed from the corpus statistics, $w$ is the word vector and $\\tilde{w_k}$ is the context vector\n",
        "\n",
        "The GloVe model embeds the words in a vector space and claims that the difference between them (distinguishing factor) is hidden in the ratio of probabilities. In vector space, the best way to encode this is by vector differences.\n",
        "\n",
        "$F((w_i-w_j),\\tilde{w_k})=\\frac{P_ik}{P_jk}$\n",
        "\n",
        "At this point, the left-hand side is a vector and the right-hand side is a scalar showing the similarity of i and j with the context word k.\n",
        "For both sides to match and to encode the similarity in the vector space, the left-hand side becomes a dot product.\n",
        "\n",
        "$F((w_i-w_j)^{T}.\\tilde{w_k})=\\frac{P_ik}{P_jk}=\\frac{F(w_i.\\tilde{w_k})}{F(w_j.\\tilde{w_k})} $\n",
        "\n",
        "$F(w_i.\\tilde{w_k})=\\frac{X_{ik}}{X_{i}}$ -> where $X$ is drived from the co-occurrences matrix\n",
        "\n",
        "To satisfy a symmetrical relationship (a.k.a. relation(a, b) = relation(b, a)), $F$ is chosen to be an exponential function, $F(x)=exp(x)$. As a result:\n",
        "\n",
        "$w_i.\\tilde{w_k} = log(P_{ik} ) = log(X_{ik} ) − log(X_{i})$\n",
        "\n",
        "$log(X_{i})$ is independent of $k$ and can be absored into a bias term.\n",
        "\n",
        "$w_i.\\tilde{w_k} = log(P_{ik} ) + b_i +\\tilde{b_k} = log(X_{ik} )$\n",
        "\n",
        "After some weighting and alterations the final cost function, based on the weighted least squares regression model is as follows:\n",
        "\n",
        "$J= \\Sigma^{V}_{i,j=1} f(X_{ij})(w_i.w_j+ b_i +b_j)-log(X_{ik})$\n",
        "\n",
        "For a detailed overview refer to the original paper: https://nlp.stanford.edu/pubs/glove.pdf\n",
        "You need to read and understand the GloVe model to solve this exercise.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "d18592954c2f43589f1b553dee3eb66e",
        "deepnote_cell_type": "markdown",
        "id": "0d71fbc8"
      },
      "source": [
        "### Subtask 1: Cost function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "193122ad29574a278b45cb0441edf95d",
        "deepnote_cell_type": "markdown",
        "id": "174acc1f"
      },
      "source": [
        "Read the paper and describe the following, in your own words:\n",
        "\n",
        " 1. The intuition behind the weighting schema in the cost function.\n",
        "\n",
        " 2. How does the objective function of GloVe relate to the objective function of the (word2vec) skip-gram model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**\n",
        "1. The weighting schema can be intuitively described as reducing the influence of extremely frequent words while maintaining the influence of rare words in a given context. Therefore the weighting scheme is based on the context words co-occurences. This is relevant, since extremely frequent words are not neccessarily important to the sentences meaning. The weighting function is a generic alternative to word filtering, which can improve performance for instance word2vec.\n",
        "2. Both objective functions aim to predict co-occurence probabilities, whereas the GloVe is based on the global co-occurences and word2vec fixed to a local and limited context window."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "a507b0ae2dca40289bf21e6a236ec901",
        "deepnote_cell_type": "markdown",
        "id": "988ae19d"
      },
      "source": [
        "### Subtask 2: Build Co-occurence matrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "60138a8f9b4d49a1ace89f5af783f7f0",
        "deepnote_cell_type": "markdown",
        "id": "edca833e"
      },
      "source": [
        "We use the same dataset as the first task and use the `quotes` as the corpus to build the co-occurrences matrix. Similar to the first task we use the `phrases` to transform our input and extract the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cell_id": "21adcd9b357d4502ab675284c266f7f2",
        "deepnote_cell_type": "code",
        "id": "bca0b788"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy import sparse\n",
        "from math import log\n",
        "from itertools import chain\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import spacy\n",
        "import re\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "fa664f53d96f4ce4825e3c8f915968df",
        "deepnote_cell_type": "markdown",
        "id": "dacc60e8"
      },
      "source": [
        "Complete the function to create dictionaries used for mapping ids to words and words to ids, and a dictionary that counts the number of occurrences of each word. The first two dictionaries are used to map indices in vector space to words and back, and the third dictionary contains the counts of the corpus statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of data before filtering: (60291, 6)\n",
            "Shape of data after filtering: (60291, 6)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>episode_number</th>\n",
              "      <th>episode_title</th>\n",
              "      <th>quote</th>\n",
              "      <th>quote_order</th>\n",
              "      <th>season</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Monica</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Monica Gets A Roommate</td>\n",
              "      <td>There's nothing to tell! He's just some guy I ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Joey</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Monica Gets A Roommate</td>\n",
              "      <td>C'mon, you're going out with the guy! There's ...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Chandler</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Monica Gets A Roommate</td>\n",
              "      <td>All right Joey, be nice. So does he have a hum...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Phoebe</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Monica Gets A Roommate</td>\n",
              "      <td>Wait, does he eat chalk?</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Phoebe</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Monica Gets A Roommate</td>\n",
              "      <td>Just, 'cause, I don't want her to go through w...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Monica</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Monica Gets A Roommate</td>\n",
              "      <td>Okay, everybody relax. This is not even a date...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Chandler</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Monica Gets A Roommate</td>\n",
              "      <td>Sounds like a date to me.</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Chandler</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Monica Gets A Roommate</td>\n",
              "      <td>Alright, so I'm back in high school, I'm stand...</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>All</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Monica Gets A Roommate</td>\n",
              "      <td>Oh, yeah. Had that dream.</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Chandler</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Monica Gets A Roommate</td>\n",
              "      <td>Then I look down, and I realize there's a phon...</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     author  episode_number           episode_title  \\\n",
              "0    Monica             1.0  Monica Gets A Roommate   \n",
              "1      Joey             1.0  Monica Gets A Roommate   \n",
              "2  Chandler             1.0  Monica Gets A Roommate   \n",
              "3    Phoebe             1.0  Monica Gets A Roommate   \n",
              "4    Phoebe             1.0  Monica Gets A Roommate   \n",
              "5    Monica             1.0  Monica Gets A Roommate   \n",
              "6  Chandler             1.0  Monica Gets A Roommate   \n",
              "7  Chandler             1.0  Monica Gets A Roommate   \n",
              "8       All             1.0  Monica Gets A Roommate   \n",
              "9  Chandler             1.0  Monica Gets A Roommate   \n",
              "\n",
              "                                               quote  quote_order  season  \n",
              "0  There's nothing to tell! He's just some guy I ...          0.0     1.0  \n",
              "1  C'mon, you're going out with the guy! There's ...          1.0     1.0  \n",
              "2  All right Joey, be nice. So does he have a hum...          2.0     1.0  \n",
              "3                           Wait, does he eat chalk?          3.0     1.0  \n",
              "4  Just, 'cause, I don't want her to go through w...          4.0     1.0  \n",
              "5  Okay, everybody relax. This is not even a date...          5.0     1.0  \n",
              "6                          Sounds like a date to me.          6.0     1.0  \n",
              "7  Alright, so I'm back in high school, I'm stand...          7.0     1.0  \n",
              "8                          Oh, yeah. Had that dream.          8.0     1.0  \n",
              "9  Then I look down, and I realize there's a phon...          9.0     1.0  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "FRIENDS_PATH = \"friends_quotes.csv\"\n",
        "\n",
        "df = pd.read_csv(FRIENDS_PATH)\n",
        "print(f\"Shape of data before filtering: {df.shape}\")\n",
        "\n",
        "# Drop rows with any null values\n",
        "df = df.dropna()\n",
        "\n",
        "# Replace empty strings with NaN and then drop those rows\n",
        "df = df.replace('', float('nan')).dropna()\n",
        "\n",
        "# Reset the index after dropping rows\n",
        "df = df.reset_index(drop=True)\n",
        "print(f\"Shape of data after filtering: {df.shape}\")\n",
        "\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/leonremke/miniconda3/envs/bomberman/lib/python3.11/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of quotes after filtering None: 57196\n"
          ]
        }
      ],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\",\"ner\"])\n",
        "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
        "\n",
        "def preprocess(doc):\n",
        "    # Remove stopwords and single characters\n",
        "    tokens = [token.text for token in doc if token.text not in stopwords and len(token.text) > 1]\n",
        "\n",
        "    # Use regex to remove non-alphabetic characters\n",
        "    tokens = [re.sub(r'[^a-zA-Z0-9]', '', token) for token in tokens]\n",
        "\n",
        "    # Filter None Types \n",
        "    tokens = [token for token in tokens if token]\n",
        "\n",
        "    # if len(tokens) > 3:\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# lower qoutes \n",
        "lowercased_qoutes = df[\"quote\"].str.lower().tolist()\n",
        "\n",
        "quotes = [preprocess(doc) for doc in nlp.pipe(lowercased_qoutes)]\n",
        "\n",
        "# none_count = 0\n",
        "# for quote in quotes:\n",
        "#     if quote is None:\n",
        "#         none_count += 1\n",
        "# print(f\"Number of quotes that are None due to less than three tokens: {none_count}\")\n",
        "\n",
        "# Filter None Types\n",
        "quotes = [quote for quote in quotes if quote]\n",
        "print(f\"Number of quotes after filtering None: {len(quotes)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of quotes that contain 'central perk': 36\n"
          ]
        }
      ],
      "source": [
        "#Check if central or perk is included in preprocessed quotes:\n",
        "central_perk = [line for line in quotes if 'central' in line and 'perk' in line]\n",
        "print(f\"Number of quotes that contain 'central perk': {len(central_perk)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 10 quotes:'\n",
            "\n",
            "['tell guy work', 'cm going guy got ta wrong', 'right joey nice hump hump hairpiece', 'wait eat chalk', 'cause want went carl oh', 'okay everybody relax date people going dinner and having sex', 'sounds like date', 'alright high school standing middle cafeteria realize totally naked', 'oh yeah dream', 'look realize phone']\n"
          ]
        }
      ],
      "source": [
        "print(\"First 10 quotes:'\\n\")\n",
        "print(quotes[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cell_id": "cc93449a51b043fd9e5217ab27face75",
        "deepnote_cell_type": "code",
        "id": "09dc3179"
      },
      "outputs": [],
      "source": [
        "from gensim.models.phrases import Phrases\n",
        "\n",
        "def tokenize(sentence):\n",
        "    # Basic whitespace tokenizer\n",
        "    return sentence.split(\" \")\n",
        "\n",
        "def create_vocab(corpus):\n",
        "    \"\"\"\n",
        "    Build a vocabulary containing the frequencies\n",
        "    corpus: the list of tokenized lines form the corpus\n",
        "\n",
        "    Returns dictionaries `word` -> (index or unique identified), frequency)`\n",
        "    and `word` -> (index or unique identified)\n",
        "    and index or unique identified -> `word`\n",
        "    and length of the vocab\n",
        "    \"\"\"\n",
        "  \n",
        "    word_count_dict = {}  # word id to the number of time it appears\n",
        "    id_to_word = {}  # mapping ids to words\n",
        "    word_to_id = {}  # mapping words to ids\n",
        "    \n",
        "    # Tokenize the input corpus into list of words\n",
        "\n",
        "    # Tokenize the input corpus into list of words\n",
        "    tokenized_corpus = [tokenize(line) for line in corpus]\n",
        "    \n",
        "    # bigram model\n",
        "    # Prepare quotes for Phrases\n",
        "    phrases = Phrases(tokenized_corpus, min_count=10)\n",
        "    # Check if bigrams worked\n",
        "    print(f\"Number of phrases: {len(phrases.vocab)}\")\n",
        "    # Get a list of all the lines\n",
        "    lines = list(phrases[tokenized_corpus])\n",
        "\n",
        "    # flatten the list of lists into a single list of words\n",
        "    words = [word for sentence in lines for word in sentence]\n",
        "\n",
        "    # frequency of each word\n",
        "    word_freq = {}\n",
        "    for word in words:\n",
        "        if word in word_freq:\n",
        "            word_freq[word] += 1\n",
        "        else:\n",
        "            word_freq[word] = 1\n",
        "\n",
        "    # Iterate over word_count dict and add words to vocab\n",
        "    for idx, (word, freq) in enumerate(word_freq.items()):\n",
        "        # Add word to vocab dictionary\n",
        "        word_to_id[word] = idx\n",
        "        # Add word to id_to_word dictionary\n",
        "        id_to_word[idx] = word\n",
        "        # Add frequency\n",
        "        word_count_dict[idx] = freq\n",
        "    return word_count_dict, id_to_word, word_to_id, len(word_count_dict), lines\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cell_id": "3688bc20f4c840fc850daaa13d9dde90",
        "deepnote_cell_type": "code",
        "id": "9fb4ca54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of phrases: 171526\n"
          ]
        }
      ],
      "source": [
        "word_count_dict, id_to_word,word_to_id, vocab_size, tokenized_lines =create_vocab(quotes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "50ea0eb3747944059b9c8b3a018a0fcd",
        "deepnote_cell_type": "markdown",
        "id": "GlX4ugx88_v7"
      },
      "source": [
        "If you have done the exercise correctly, you have `15333` tokens in the vocabulary, and the number of occurrences for `joey` is `1951` and for `central_perk` is `36`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "cell_id": "491f4672ae5c47508074df7a1d74a8a2",
        "deepnote_cell_type": "code",
        "id": "c8b31695"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of occurrences for joey: 2308\n",
            "number of occurrences for central perk: 37\n",
            "vocab size is: 17646\n"
          ]
        }
      ],
      "source": [
        "print(\"number of occurrences for joey:\",word_count_dict[word_to_id['joey']])\n",
        "print(\"number of occurrences for central perk:\",word_count_dict[word_to_id['central_perk']])\n",
        "print(\"vocab size is:\",vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "cell_id": "27c3b68d90fe4589a4b3e33b552603a9",
        "deepnote_cell_type": "code",
        "id": "b027280a"
      },
      "outputs": [],
      "source": [
        "def calculate_weight(cooccurrences, context_word_ids, center_word_id, side):\n",
        "    \"\"\"\n",
        "    Calculate the weight in the co-occurrence matrix based on the distance of a word\n",
        "    to the center word\n",
        "    sentence = [I, went, to , the, bank]\n",
        "    Let center word be \"to\" and window size =2\n",
        "    left_context =[I,went]\n",
        "    right_context = [the,bank]\n",
        "\n",
        "    Weights:\n",
        "    1/distance-> `went` and `the` have weight of 1 and `I` and `bank` have weight of 1/2\n",
        "    \"\"\"\n",
        "\n",
        "    if side == \"left_context\":\n",
        "        context_word_ids.reverse()\n",
        "\n",
        "      ## adjust the weight of the matrix to 1/distance between the center word and context word, where center word will act as the row and the context word is the column##\n",
        "\n",
        "    for idx, context_word_id in enumerate(context_word_ids):\n",
        "        distance = abs(idx + 1)\n",
        "        if distance == 0:\n",
        "            weight = 1\n",
        "        weight = 1 / distance\n",
        "        cooccurrences[center_word_id, context_word_id] += weight\n",
        "\n",
        "    return cooccurrences\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "e5ce97690ed84843aac576a50e5c7de3",
        "deepnote_cell_type": "markdown",
        "id": "MUF8zXO59P3b"
      },
      "source": [
        "The weight of first to second element on the example matrix should be `1.0`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "cell_id": "5b243d10b4704634b6fb6de59147e333",
        "deepnote_cell_type": "code",
        "id": "d7c32c0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "weight of id=0 to id=1 : 1.0\n"
          ]
        }
      ],
      "source": [
        "cooccurrences = sparse.lil_matrix((10, 10),dtype=np.float64)\n",
        "calculate_weight(cooccurrences, [1,3,5], 0, side=\"right_context\")\n",
        "print(\"weight of id=0 to id=1 :\",cooccurrences[0,1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "45d1ac1423a24de28cf91dfd34ffd516",
        "deepnote_cell_type": "markdown",
        "id": "0ede37e8"
      },
      "source": [
        "We build co-occurrence as a sparse matrix to speed up the computation. The original matrix is a square matrix in the size of the vocabulary. However, many words do not co-occur with one another and we do not need to store those elements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "cell_id": "3fc6b1e02c38406b96444da7ed45c2d7",
        "deepnote_cell_type": "code",
        "id": "d63f39e7"
      },
      "outputs": [],
      "source": [
        "def build_cooccur(corpus, window_size=3, min_count=5):\n",
        "    \"\"\"\n",
        "    Create a coocurrance matrix given a corpus\n",
        "    corpus: the list of tokenized lines form the corpus\n",
        "    window_size: how many words to right and left to consider\n",
        "\n",
        "    Returns the co-oocurrance sparse matrix\n",
        "    \"\"\"\n",
        "    vocab, id_to_word,word_to_id, vocab_size, tokenized_lines = create_vocab(corpus)\n",
        "\n",
        "    #sparse lil_matrix is optimized to operate on matrix which mostly has zeros.\n",
        "    cooccurrences = sparse.lil_matrix((vocab_size, vocab_size),dtype=np.float64)\n",
        "\n",
        "    for line in tokenized_lines:\n",
        "        word_ids = [word_to_id[word] for word in line]\n",
        "\n",
        "        for i, central_word_id in enumerate(word_ids):\n",
        "\n",
        "            # left_context_word_ids = [word_ids[j] for j in range(max(i - window_size, 0), i)]\n",
        "            left_context_word_ids = [word_ids[j] for j in range(max(i - window_size, 0), i)]\n",
        "\n",
        "            #right side context\n",
        "            # right_context_word_ids = [word_ids[j] for j in range(i, min(i + window_size, len(word_ids)))]\n",
        "            # right_context_word_ids = [j for j in range(i + 1, min(i + window_size, len(word_ids)))]\n",
        "            right_context_word_ids = [word_ids[j] for j in range(i + 1, min(i + window_size + 1, len(word_ids)))]\n",
        "\n",
        "\n",
        "            ###update the matrix based on the distance weights on both sides###\n",
        "            # Todo: pass over center_word_id or index? IF newly makes no sence with distances...\n",
        "            if i > 0:\n",
        "                cooccurrences = calculate_weight(cooccurrences, left_context_word_ids, central_word_id, side=\"left_context\")\n",
        "            if i < len(word_ids) - 1:\n",
        "                cooccurrences = calculate_weight(cooccurrences, right_context_word_ids, central_word_id, side=\"right_context\")\n",
        "\n",
        "    # go into the LiL-matrix to quickly iterate through all nonzero cells and filter out the ones with minimum count\n",
        "    cooccurrences_tuples = []\n",
        "    for i, (row, data) in enumerate(zip(cooccurrences.rows,cooccurrences.data)):\n",
        "        for j, d in zip(row, data):\n",
        "            if d >= min_count:\n",
        "                cooccurrences_tuples.append((i, j, d))\n",
        "\n",
        "      ## check if the min_count condition is statisfied and then add the elements as the tuple of (i,j,weight)###\n",
        "    return cooccurrences_tuples\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "596886637b814aa5abfe304002bf153b",
        "deepnote_cell_type": "markdown",
        "id": "f3aba5bd"
      },
      "source": [
        "Build a matrix with a window_size of 3 words, and the minimum number of times a word has to occur to be part of the matrix is 10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "cell_id": "55763935390b4d2eb5ad8614ebc4fcc1",
        "deepnote_cell_type": "code",
        "id": "69c198e0",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of phrases: 171526\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(2, 20, 16.16666666666667)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "matrix=build_cooccur(quotes, window_size=3, min_count=10)\n",
        "matrix[103]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "954044b6b6fa4a828697f4876885789f",
        "deepnote_cell_type": "markdown",
        "id": "82cc1256"
      },
      "source": [
        "### Subtask 3: Modelling and Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "ac6ae22537da467cb0fceb6e52e3606b",
        "deepnote_cell_type": "markdown",
        "id": "91764831"
      },
      "source": [
        "We initialize the weights for the context and center words and learn the vectors through backprop, using the GloVe cost function.\n",
        "Make sure you use the correct weighting schema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "cell_id": "b3212a7313b641d2bfe7e79fec1f059e",
        "deepnote_cell_type": "code",
        "id": "05251525"
      },
      "outputs": [],
      "source": [
        "# Random normal weights intialization\n",
        "np.random.seed(77)# we set a seed to have similar results\n",
        "def init_weights(vocab_size, hidden):\n",
        "     #Each word has a center word vector and a context vector.\n",
        "     # initilize the correct size vector\n",
        "    W_center = np.random.randn(vocab_size, hidden)\n",
        "    # initilize the correct size vector\n",
        "    b_center = np.random.randn(vocab_size)\n",
        "    # initilize the correct size vector\n",
        "    W_context = np.random.randn(vocab_size, hidden)\n",
        "    # initilize the correct size vector\n",
        "    b_context = np.random.randn(vocab_size)\n",
        "    return W_center, b_center, W_context, b_context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "e3329af4ab3442d9a0c078909df574ba",
        "deepnote_cell_type": "markdown",
        "id": "y-mcYRis9wet"
      },
      "source": [
        "keep track of `W_center[0,1]` as it should change based on backprop in the next cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "cell_id": "b2cfad807c304662a4a5290d4699bc4c",
        "deepnote_cell_type": "code",
        "id": "b2f31560"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "check the shapes to make sure the matrices have correct sizes:\n",
            "(100, 32)\n",
            "(100,)\n",
            "(100, 32)\n",
            "(100,)\n",
            "Look at the value of this element and how it changes with back prob:\n",
            "0.6615314728168009\n"
          ]
        }
      ],
      "source": [
        "W_center, b_center, W_context, b_context=init_weights(100, 32)\n",
        "print(\"check the shapes to make sure the matrices have correct sizes:\")\n",
        "print(W_center.shape)\n",
        "print(b_center.shape)\n",
        "print(W_context.shape)\n",
        "print(b_context.shape)\n",
        "print(\"Look at the value of this element and how it changes with back prob:\")\n",
        "print(W_center[0,1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "443396a70ae841bcafc004284780d337",
        "deepnote_cell_type": "markdown",
        "id": "gae189dl-ZZo"
      },
      "source": [
        "Write a training script for the GloVe model that goes over the entire co-occurrence matrix given a number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import sys\n",
        "\n",
        "def check_nan(update):\n",
        "    if math.isnan(update) or math.isinf(update):\n",
        "        sys.stderr.write(\"\\ncaught NaN in update\")\n",
        "        return 0.0\n",
        "    else:\n",
        "        return update\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "cell_id": "c6f041f3838540a893ebe7bc4d6e1add",
        "deepnote_cell_type": "code",
        "id": "8669b4fb"
      },
      "outputs": [],
      "source": [
        "# Back Propagation\n",
        "def back_prop(W_center, b_center, W_context, b_context, matrix, x_max,  vocab_size, learning_rate, alpha=2):\n",
        "    \"\"\"\n",
        "    W_center, b_center: weight and bias of the center word\n",
        "    W_context, b_context: weight and bias of the context word\n",
        "    vocab_size: vocabulary size\n",
        "    x_max: define our weighting function when computing the cost for two word pairs; see the GloVe paper for more\n",
        "    details.\n",
        "    matrix: coocurrance matrix\n",
        "    alpha: the power of x_max function\n",
        "    learning_rate: learning rate for gradient descent\n",
        "    \"\"\"\n",
        "\n",
        "    global_cost = 0\n",
        "    grad_squared_center = np.zeros_like(W_center)\n",
        "    grad_squared_context = np.zeros_like(W_context)\n",
        "    grad_squared_bias_center = np.zeros_like(b_center)\n",
        "    grad_squared_bias_context = np.zeros_like(b_context)\n",
        "\n",
        "    for i, j, cooccurrence in matrix:\n",
        "        weight = ((cooccurrence/x_max)**alpha if(cooccurrence < x_max) else 1)\n",
        "        # Compute inner component of cost function  J' = w_i^Tw_j + b_i + b_j - log(X_{ij})\n",
        "        # print(\"weight i transposed:\",W_context[j].T, \"\\n\", W_context[j])\n",
        "        diff_j_prime = np.dot(W_center[i], W_context[j]) + b_center[i] + b_context[j] - np.log(cooccurrence)\n",
        "\n",
        "        ##Compute cost J = f(X_{ij}) (J')^2##\n",
        "        cost_j = weight * (diff_j_prime**2)\n",
        "\n",
        "        # Check for NaN and Infinity values in the diffs\n",
        "        if np.isnan(diff_j_prime) or np.isinf(diff_j_prime):\n",
        "            print(f\"Encountered NaN/Infinity value in diff_j_prime, setting to 0.\")\n",
        "            continue\n",
        "        \n",
        "        global_cost += cost_j\n",
        "        ##Compute gradients for word vectors##\n",
        "        grad_center = diff_j_prime * W_context[j]\n",
        "        grad_context = diff_j_prime * W_center[i]\n",
        "\n",
        "        ##Compute gradients for bias terms##\n",
        "        grad_bias_center = diff_j_prime\n",
        "        grad_bias_context = diff_j_prime\n",
        "\n",
        "        # Accumulate squared gradients\n",
        "        grad_squared_center[i] += grad_center ** 2\n",
        "        grad_squared_context[j] += grad_context ** 2\n",
        "        grad_squared_bias_center[i] += grad_bias_center ** 2\n",
        "        grad_squared_bias_context[j] += grad_bias_context ** 2\n",
        "        \n",
        "        # Update weights with adaptive learning rates\n",
        "        W_center[i] -= learning_rate * grad_center / np.sqrt(grad_squared_center[i] + 1e-8)\n",
        "        W_context[j] -= learning_rate * grad_context / np.sqrt(grad_squared_context[j] + 1e-8)\n",
        "        b_center[i] -= learning_rate * grad_bias_center / np.sqrt(grad_squared_bias_center[i] + 1e-8)\n",
        "        b_context[j] -= learning_rate * grad_bias_context / np.sqrt(grad_squared_bias_context[j] + 1e-8)\n",
        "    return W_center, b_center, W_context, b_context, global_cost    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "478ebdea7aa6473a91dea52c82a0a5b3",
        "deepnote_cell_type": "markdown",
        "id": "sV9uZIgX95tZ"
      },
      "source": [
        "Based on the random seed, the value of `W_center[0,1]` should have changed due to backpropagation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "cell_id": "fa7b5d34c5684883a434925bcf5775ba",
        "deepnote_cell_type": "code",
        "id": "84a467a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cost: 0.20163369439311515\n",
            "changed value:\n",
            "0.674624772684653\n"
          ]
        }
      ],
      "source": [
        "test_matrix=[(0,1,1),(0,2,0.4),(0,3,0.9),(0,4,0.4)]\n",
        "W_center, b_center, W_context, b_context, global_cost  =back_prop(W_center, b_center, W_context, b_context, test_matrix, x_max=10,  vocab_size=100, learning_rate=0.01)\n",
        "print(\"cost:\",global_cost)\n",
        "print(\"changed value:\")\n",
        "print(W_center[0,1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "752879b8fcb64b16b7d578c04c5e911a",
        "deepnote_cell_type": "markdown",
        "id": "ULLmdXvC-QpX"
      },
      "source": [
        "\n",
        "Write a training script for the GloVe model that goes over the entire co-occurrence matrix given a number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "cell_id": "c2c68483bb034f458bf4c0a206ebb5c3",
        "deepnote_cell_type": "code",
        "id": "12dd535b"
      },
      "outputs": [],
      "source": [
        "def train_GloVe(matrix, vocab_size, epochs = 10, learning_rate = 0.0001, x_max = 10, hidden_dim=100):\n",
        "    \"\"\"\n",
        "    Train the glove model based the co-ocurrance matrix for a number of epochs\n",
        "    matrix: co-occcurance matrix\n",
        "    vocab_size: number of words in vocab\n",
        "    epochs: number of passes through the data\n",
        "    learning_rate: learning rate for back prop\n",
        "    x_max: parameter of the weighting function\n",
        "    hidden_dim: dimension of the vectors\n",
        "    \"\"\"\n",
        "    W_center, b_center, W_context, b_context = init_weights(vocab_size, hidden_dim)\n",
        "    for i in tqdm(range(epochs)):\n",
        "        ### perform backprop###\n",
        "        W_center, b_center, W_context, b_context, global_cost = back_prop(W_center, b_center, W_context, b_context, matrix, x_max,  vocab_size, learning_rate, alpha=0.75)\n",
        "        print(global_cost)\n",
        "    return W_center, W_context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "ef5e9f0c065f4623bcab6bb9f62a9587",
        "deepnote_cell_type": "markdown",
        "id": "e0116c62"
      },
      "source": [
        "Train the model with hidden dimension of `100` and learning rate of `0.001` for a `100` epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "cell_id": "50f8db5bd4414728a7b6f24c1f93e663",
        "deepnote_cell_type": "code",
        "id": "b2355c76"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|▍         | 4/100 [00:00<00:06, 14.81it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "695391.2052786889\n",
            "684992.7537767655\n",
            "674733.9577590675\n",
            "664597.3653146222\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  6%|▌         | 6/100 [00:00<00:06, 14.95it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "654594.4602677099\n",
            "644713.2477874254\n",
            "634954.6228464773\n",
            "625321.2992252586\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 12%|█▏        | 12/100 [00:00<00:05, 14.98it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "615828.5351777349\n",
            "606434.1477521994\n",
            "597163.4979821568\n",
            "588014.9247765286\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 16%|█▌        | 16/100 [00:01<00:05, 15.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "578976.7172235975\n",
            "570062.2269141058\n",
            "561260.5739807851\n",
            "552559.7179360389\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 20/100 [00:01<00:05, 15.11it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "543984.1961195192\n",
            "535512.60719379\n",
            "527131.9134109158\n",
            "518892.72447807644\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 24%|██▍       | 24/100 [00:01<00:05, 15.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "510748.7642707005\n",
            "502702.67004000075\n",
            "494775.3996178506\n",
            "486955.17169885425\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 28%|██▊       | 28/100 [00:01<00:04, 14.98it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "479212.0677417723\n",
            "471584.79303952877\n",
            "464049.2861284379\n",
            "456634.22249332047\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|███       | 30/100 [00:02<00:04, 15.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "449285.34033387544\n",
            "442054.8502836274\n",
            "434914.20985491364\n",
            "427864.67738798907\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 36%|███▌      | 36/100 [00:02<00:04, 14.99it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "420911.20883322577\n",
            "414051.8204455353\n",
            "407277.3018898857\n",
            "400603.99442245776\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 40/100 [00:02<00:03, 15.08it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "394009.04012812016\n",
            "387520.35268648854\n",
            "381106.2645080377\n",
            "374792.65874949115\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 44%|████▍     | 44/100 [00:02<00:03, 15.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "368560.89619892323\n",
            "362417.34578515234\n",
            "356354.43995288236\n",
            "350380.1964251968\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 48%|████▊     | 48/100 [00:03<00:03, 15.15it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "344494.18588228856\n",
            "338672.5431952512\n",
            "332947.1453251629\n",
            "327304.8713940591\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 52%|█████▏    | 52/100 [00:03<00:03, 15.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "321726.437233444\n",
            "316254.1728214573\n",
            "310839.6217880669\n",
            "305523.28926754766\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 56%|█████▌    | 56/100 [00:03<00:02, 15.13it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "300258.5106281622\n",
            "295086.36665040086\n",
            "289979.9242760956\n",
            "284941.4080667635\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 60/100 [00:03<00:02, 15.09it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "279988.2364366063\n",
            "275089.70664991654\n",
            "270261.8900562369\n",
            "265521.29881672363\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 64%|██████▍   | 64/100 [00:04<00:02, 15.10it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "260846.1466895673\n",
            "256236.095929909\n",
            "251695.6272926168\n",
            "247225.24104283462\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 68%|██████▊   | 68/100 [00:04<00:02, 15.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "242805.97492645853\n",
            "238472.3019289845\n",
            "234180.66652521794\n",
            "229971.5696943434\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|███████   | 70/100 [00:04<00:01, 15.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "225808.01986992804\n",
            "221719.54625260847\n",
            "217694.75252083325\n",
            "213732.3997872285\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 74%|███████▍  | 74/100 [00:04<00:01, 14.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "209818.14847991237\n",
            "205966.58333975816\n",
            "202185.4737216321\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 78%|███████▊  | 78/100 [00:05<00:01, 14.75it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "198458.10778947335\n",
            "194776.17385811434\n",
            "191166.7262109617\n",
            "187590.3009418233\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 82%|████████▏ | 82/100 [00:05<00:01, 14.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "184097.12119379535\n",
            "180638.38970916916\n",
            "177263.4864369638\n",
            "173924.7968949926\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 86%|████████▌ | 86/100 [00:05<00:00, 14.77it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "170634.1438174592\n",
            "167407.85532640078\n",
            "164216.65662184288\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 90%|█████████ | 90/100 [00:06<00:00, 14.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "161087.42300284415\n",
            "158007.7679174811\n",
            "154954.13111260865\n",
            "151985.1151092053\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 94%|█████████▍| 94/100 [00:06<00:00, 15.02it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "149048.2955693023\n",
            "146155.8150476256\n",
            "143316.63109788622\n",
            "140488.40148462914\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 96%|█████████▌| 96/100 [00:06<00:00, 14.93it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "137769.81542708204\n",
            "135051.47021843924\n",
            "132402.24414495454\n",
            "129768.70291453031\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:06<00:00, 14.97it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "127205.58707190656\n",
            "124658.86655508664\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "## your code ##\n",
        "W_center, W_context = train_GloVe(matrix, vocab_size, epochs = 100, learning_rate = 0.001, x_max = 10, hidden_dim=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "319bd85a153148019e6f2969113b540b",
        "deepnote_cell_type": "markdown",
        "id": "770701f4"
      },
      "source": [
        "As you can see by looking at the loss, the model still needs more time to converge to a minimum.\n",
        "However, we keep the training short and keeping in mind that the vectors can improve we look at some examples.\n",
        "Take the average, transpose, and normalize the matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "cell_id": "2659cbaf2f734cd5963ec7def348c3d1",
        "deepnote_cell_type": "code",
        "id": "b272ab8a"
      },
      "outputs": [],
      "source": [
        "from numpy.linalg import norm\n",
        "# take the average of the learned vector as the final vector\n",
        "W = np.add(W_center, W_context)/2\n",
        "W = W.T\n",
        "W = W/norm(W)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "cell_id": "fbce3b449b0047ad9963f7228ed07b7c",
        "deepnote_cell_type": "code",
        "id": "334aff38"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(100, 17646)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "W.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "ad6c996ac4654bc980ed30aeeccbf329",
        "deepnote_cell_type": "markdown",
        "id": "d106c990"
      },
      "source": [
        "Lets create a dictionary that points from a word to its vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "cell_id": "1baffd8b1d6d442192df53c608bf1a4d",
        "deepnote_cell_type": "code",
        "id": "db81b66e"
      },
      "outputs": [],
      "source": [
        "# Generates word to word embedding dictionary\n",
        "word_to_vector = {}\n",
        "for word in word_to_id.keys():\n",
        "    word_to_vector[word] = W[:, word_to_id[word]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "1f32d7269f324785a7d12d88a3bec63e",
        "deepnote_cell_type": "markdown",
        "id": "11c86eeb"
      },
      "source": [
        "### Subtask 4: Compare to Skip-gram\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "746e6f74e7fd44689682d5f2fce4e22d",
        "deepnote_cell_type": "markdown",
        "id": "e8fd8763"
      },
      "source": [
        "Let's compute the similarities for the same words in Task 1 to compare the results with word2vec. This time you need to implement the similarity function, based on the dot product. To get to the topk you need to sort the elements based on their similarity.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "cell_id": "647f386dbe9a4ec6bb9a8d99066ef5cb",
        "deepnote_cell_type": "code",
        "id": "a2f3f722"
      },
      "outputs": [],
      "source": [
        "from numpy import dot\n",
        "\n",
        "def most_similar(word_vector,all_vectors,id_to_word, topk):\n",
        "    \"\"\"\n",
        "    function to find the topk most similar words to a word vector\n",
        "    word_vector: vector of the search word\n",
        "    all_vectors: all word vectors\n",
        "    id_to_word: dictionary from id to words\n",
        "    topk: number of elements to return\n",
        "    \"\"\"\n",
        "    ### find the topk most similar words to a given word vector ##\n",
        "    # Use cosine similarity to find the most similar words\n",
        "    cosine_sim = dot(word_vector, all_vectors)\n",
        "    most_similar_ids = np.argsort(cosine_sim)[-topk:]\n",
        "    most_similar_words = [id_to_word[idx] for idx in most_similar_ids]\n",
        "    word = most_similar_words[::-1]\n",
        "    return word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "cell_id": "e870506ae79346e6adb785da9daf9791",
        "deepnote_cell_type": "code",
        "id": "53c1a84a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['central_perk', 'stability', 'hideous', 'loser', 'journal']"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "most_similar(word_to_vector[\"central_perk\"],W,id_to_word,5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "cell_id": "7ed647ee59b14c628d57d282ce177598",
        "deepnote_cell_type": "code",
        "id": "2c8e8519"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['joey', 'tingle', 'pees', 'godeh', 'crossing']"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "most_similar(word_to_vector[\"joey\"],W,id_to_word,5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "3ad3c6cb031048f8aa82e449444bdc32",
        "deepnote_cell_type": "markdown",
        "id": "6bdcd5f5"
      },
      "source": [
        "Compute the similarity between the `('rachel', 'mrs_green')`, `('smelly_cat', 'song')` and `('ross', 'spaceship')`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pairwise_sim(word1, word2, word_to_vector):\n",
        "    \"\"\"\n",
        "    function to find the similarity between two words\n",
        "    word1, word2: the two words\n",
        "    word_to_vector: dictionary from word to vector\n",
        "    \"\"\"\n",
        "    ### find the similarity between two words ##\n",
        "    # Use cosine similarity to find the most similar words\n",
        "    cosine_sim = dot(word_to_vector[word1], word_to_vector[word2])\n",
        "    return cosine_sim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "cell_id": "77ec7c44bbde45b8bbd245d0e6446c57",
        "deepnote_cell_type": "code",
        "id": "e77f01a5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.172752475994363e-05"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## your code ##\n",
        "pairwise_sim(\"rachel\", \"mrs_green\", word_to_vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "cell_id": "8d15aeceee034d7b8cde8099bc78ea58",
        "deepnote_cell_type": "code",
        "id": "d1def847"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-3.777437328050534e-06"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## your code ##\n",
        "pairwise_sim('smelly_cat', 'song', word_to_vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "cell_id": "4ef6f86219ea4a72a680dabd7feadd46",
        "deepnote_cell_type": "code",
        "id": "933ca3ac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4.106331097953352e-06"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## your code ##\n",
        "pairwise_sim('ross', 'spaceship', word_to_vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "b0b9ace0e3014f5097c109c6bfe3a78d",
        "deepnote_cell_type": "markdown",
        "id": "ecace919"
      },
      "source": [
        "If you see your results are not as meaningful as the gensim model, do not be discouraged. With better optimization and longer training the results should improve. If you have time play around a bit more with your model and see if you can generate more meaningful vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown",
        "id": "9LbSjC5oz21o"
      },
      "source": [
        "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=2995dafc-2409-4d4e-859d-95843714f594' target=\"_blank\">\n",
        "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
        "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "deepnote": {},
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "66515ddb3eea49f3911f054f2c6427ef",
    "deepnote_persisted_session": {
      "createdAt": "2023-10-30T16:42:27.986Z"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
