{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "be9f7653",
      "metadata": {
        "id": "be9f7653"
      },
      "source": [
        "**Heidelberg University**\n",
        "\n",
        "**Data Science  Group**\n",
        "    \n",
        "Prof. Dr. Michael Gertz  \n",
        "\n",
        "Ashish Chouhan, Satya Almasian, John Ziegler, Jayson Salazar, Nicolas Reuter\n",
        "    \n",
        "January 16, 2024\n",
        "    \n",
        "Natural Language Processing with Transformers\n",
        "\n",
        "Winter Semster 2023/2024     \n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "258e9648",
      "metadata": {
        "id": "258e9648"
      },
      "source": [
        "# **Assignment 4: Question Answering**\n",
        "**Due**: Monday, January 29, 2024, 2pm, via [Moodle](https://moodle.uni-heidelberg.de/course/view.php?id=19251)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc27ad9e",
      "metadata": {
        "id": "fc27ad9e"
      },
      "source": [
        "### **Submission Guidelines**\n",
        "\n",
        "- Solutions need to be uploaded as a **single** Jupyter notebook. You will find several pre-filled code segments in the notebook, your task is to fill in the missing cells.\n",
        "- For the written solution, use LaTeX in markdown inside the same notebook. Do **not** hand in a separate file for it.\n",
        "- Download the .zip file containing the dataset but do **not** upload it with your solution.\n",
        "- It is sufficient if one person per group uploads the solution to Moodle, but make sure that the full names of all team members are given in the notebook.\n",
        "\n",
        "***\n",
        "Member Name: Ye Tao, Yong Wu, Ziwei Liu, Guangdeng Liang"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HETm7VsBkmLq",
      "metadata": {
        "id": "HETm7VsBkmLq"
      },
      "source": [
        "## **Task 1: Retrieval Augmented Generation (RAG)** ( 4.5 + 3 + 4 + 3 + 1.5 = 16 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ODkKBIRkrfe",
      "metadata": {
        "id": "0ODkKBIRkrfe"
      },
      "source": [
        "In this task, we look at using the open source `Llama-13b-chat` model for creating a RAG system. You must first apply for access to Llama 2 models via [this](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) form (access is typically granted within a few hours). etrieval augmented generation you also need to request to use the model on Hugging Face by going to the [model](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf) card. ***Note that the emails you provide for your Hugging Face account must match the email you used to request Llama 2.***\n",
        "\n",
        "The final piece that you need is a Hugging Face authentication token. You can find such a token by going to the `setting` in your Hugging Face profile, under the `Access Token` menu you can generate a new token.\n",
        "\n",
        "To store the document you will need a free Pinecone [API key](https://app.pinecone.io/).\n",
        "Make sure you have these pieces ready before starting to work on this task.\n",
        "\n",
        "----\n",
        "When ready, let's start by downloading the necessary packages.\n",
        "\n",
        "It is advised to proceed with this notebook with a GPU (if you are on Colab make sure that a GPU environment is activated.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yO1OwuHTMo75",
      "metadata": {
        "id": "yO1OwuHTMo75"
      },
      "source": [
        "Place all the access tokens in the `.env` file and upload it to the working directory (if you are running this notebook locally, you can change the path to fit your working directory). Please use the following format:\n",
        "\n",
        "\n",
        "```\n",
        "HF_AUTH= \"Hugging Face Authentication Key\"\n",
        "PINECONE_API_KEY=\"Pincone API Key\"\n",
        "PINECONE_ENVIRONMENT=\"Pinecone Environment\"\n",
        "```\n",
        "\n",
        "Run the cell below to load the access tokens into the environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1Dxt-e1bg73b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Dxt-e1bg73b",
        "outputId": "b91f00d8-5d17-43f7-b489-c909389b327a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-dotenv in /root/miniconda3/envs/nlp/lib/python3.10/site-packages (1.0.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "EqmnxgpALbVV",
      "metadata": {
        "id": "EqmnxgpALbVV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# load environment variables from .env file\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yE3EOlhTrKo6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yE3EOlhTrKo6",
        "outputId": "5f91e08c-d9e4-4a1e-ed89-6c3950c35736"
      },
      "outputs": [],
      "source": [
        "%pip install litellm\n",
        "%pip install -qU trulens_eval pydantic fastapi kaleido python-multipart uvicorn cohere openai tiktoken \"llama-index\"\n",
        "%pip install transformers\n",
        "%pip install sentence-transformers\n",
        "%pip install pinecone-client\n",
        "%pip install datasets\n",
        "%pip install accelerate\n",
        "%pip install einops\n",
        "%pip install langchain\n",
        "%pip install xformers\n",
        "%pip install bitsandbytes\n",
        "%pip install matplotlib seaborn tqdm\n",
        "%pip install chromadb\n",
        "%pip install evaluate\n",
        "%pip install rouge_score\n",
        "%pip install bert_score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dLDAPMVST2Ds",
      "metadata": {
        "id": "dLDAPMVST2Ds"
      },
      "source": [
        "\n",
        "\n",
        "## Subtask 1.1: Data Preparation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b9odZyFLDin",
      "metadata": {
        "id": "9b9odZyFLDin"
      },
      "source": [
        "We need a collection of documents to perform our retrieval on. To make it closer to your final project, you will be downloading and using a subset of the LangChain documentation. We get some of the `.html` files located on the site. The code below will download all HTML files from the links on the webpage into a `docs` directory. `-l1` limits the download to only the first level of depth.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "JXe86ZKVFfrm",
      "metadata": {
        "id": "JXe86ZKVFfrm"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qiodFLkaLUsF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiodFLkaLUsF",
        "outputId": "6cc4f77d-07df-452a-8ddf-4789002dec29"
      },
      "outputs": [],
      "source": [
        "!wget -r -l1 -A.html -P docs https://api.python.langchain.com/en/stable/langchain_api_reference.html"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5DWa9M6JLb8O",
      "metadata": {
        "id": "5DWa9M6JLb8O"
      },
      "source": [
        " The docs are going to be used as input text for answering questions that a normal language model might not be aware of (LangChain docs is not necessarily part of its training data of Llama2). We can use LangChain itself to process these docs. Use the [ReadTheDocsLoader](https://python.langchain.com/docs/integrations/document_loaders/readthedocs_documentation) to load the docs from the `docs` folder.\n",
        "\n",
        " At the time of creating this notebook, there  `423` documents were downloaded. However, since the documentation is being updated regularly this number might be different for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "Nd8ufORKLVy0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "Nd8ufORKLVy0",
        "outputId": "b8c5f0aa-4932-4a39-e432-11bbc7268964"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "417"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.document_loaders import ReadTheDocsLoader\n",
        "#### your code ####\n",
        "loader = ReadTheDocsLoader(\"docs\")\n",
        "docs = loader.load()\n",
        "#### your code ####\n",
        "len(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yKs0_OrBMQ7M",
      "metadata": {
        "id": "yKs0_OrBMQ7M"
      },
      "source": [
        "Let's take a look at one of the documents. You see that LangChain has created a `Document` object. Look at the example below and fill in the cells to print out the text content and URL of the page (the URL of the page should starts with `https://`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vcpuudNRLV7k",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcpuudNRLV7k",
        "outputId": "885ceb52-228c-498f-90af-2a94a220b29d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(page_content='langchain_nvidia_trt 0.0.1¶\\nlangchain_nvidia_trt.llms¶\\nClasses¶\\nllms.StreamingResponseGenerator(client,\\xa0...)\\nA Generator that provides the inference results from an LLM.\\nllms.TritonTensorRTError\\nBase exception for TritonTensorRT.\\nllms.TritonTensorRTLLM\\nTRTLLM triton models.\\nllms.TritonTensorRTRuntimeError\\nRuntime error for TritonTensorRT.', metadata={'source': 'docs/api.python.langchain.com/en/stable/nvidia_trt_api_reference.html'})"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs[10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5Q24oWkGM3IP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Q24oWkGM3IP",
        "outputId": "6467b7cc-7606-4742-b52c-18031a3bcbd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "langchain_nvidia_trt 0.0.1¶\n",
            "langchain_nvidia_trt.llms¶\n",
            "Classes¶\n",
            "llms.StreamingResponseGenerator(client, ...)\n",
            "A Generator that provides the inference results from an LLM.\n",
            "llms.TritonTensorRTError\n",
            "Base exception for TritonTensorRT.\n",
            "llms.TritonTensorRTLLM\n",
            "TRTLLM triton models.\n",
            "llms.TritonTensorRTRuntimeError\n",
            "Runtime error for TritonTensorRT.\n",
            "https://api.python.langchain.com/en/stable/nvidia_trt_api_reference.html\n"
          ]
        }
      ],
      "source": [
        "#### your code ####\n",
        "page_content=docs[10].page_content\n",
        "page_url=docs[10].metadata['source']\n",
        "page_url = page_url.replace(\"docs/\", \"https://\")\n",
        "#### your code ####\n",
        "print(page_content)\n",
        "print(page_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OfZKdZJsMeyj",
      "metadata": {
        "id": "OfZKdZJsMeyj"
      },
      "source": [
        "As you can imagine the documents can be long and if multiple of them are required as context to answer questions, we need to take the document lengths into account.\n",
        "This is due to the fact that language models do not have unlimited context span. In our case, we plan to use Llama2 for this project, where the maximum token limit is 4096. This limit is not only the input but also takes the generated output into account, moreover, you need to leave room for the query and instructions as well. Therefore, it is important to chunk the longer documents into smaller-sized fragments.\n",
        "\n",
        "Based on your use case and how many contexts you plan to feed into the model the length of these fragments will differ.\n",
        "In this case, we choose to assign 2000 tokens to context and choose to generate the answer from 5 context fragments, which leaves us with 400 tokens per context fragment as the maximum chunk size.\n",
        "\n",
        "To count the number of tokens in a chunk, we need to load the correct tokenizer for Llama2. Fill the code cell below to load the correct tokenizer and use it to complete the function that counts the number of tokens per given chunk.\n",
        "\n",
        "**Hint:** you need to use your Hugging Face authentication token to load the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "WjOHBqXLLWDA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "WjOHBqXLLWDA",
        "outputId": "e7495f39-bac1-4078-cc0c-a8e8c682889d"
      },
      "outputs": [],
      "source": [
        "#If you get an error here during the first import from the `transformers` package, restart the kernel and try again.\n",
        "#### your code ####\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-13b-chat-hf\", token=os.getenv('HF_AUTH'))\n",
        "\n",
        "#### your code ####"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "G5GutpFdLWFu",
      "metadata": {
        "id": "G5GutpFdLWFu"
      },
      "outputs": [],
      "source": [
        "def token_len(text):\n",
        "  #### your code ####\n",
        "    return len(tokenizer.encode(text))\n",
        "    #### your code ####"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2pdG2JeGPfcS",
      "metadata": {
        "id": "2pdG2JeGPfcS"
      },
      "source": [
        "Count the number of tokens for all documents and use it to compute minimum, maximum, and average token count statistics across all documents. Depending on how the documentation is updated by the time you run the cell below the numbers might slightly differ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HdAdFPYyLWIM",
      "metadata": {
        "id": "HdAdFPYyLWIM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Min: 49\n",
            "Avg: 4093.021582733813\n",
            "Max: 38563\n"
          ]
        }
      ],
      "source": [
        "#### your code ####\n",
        "token_counts = [token_len(doc.page_content) for doc in docs]\n",
        "min_tokens = min(token_counts)\n",
        "avg_tokens = sum(token_counts) / len(token_counts)\n",
        "max_tokens = max(token_counts)\n",
        "#### your code ####\n",
        "print(f\"\"\"Min: {min_tokens}\n",
        "Avg: {avg_tokens}\n",
        "Max: {max_tokens}\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iz9rYYRtMo2N",
      "metadata": {
        "id": "iz9rYYRtMo2N"
      },
      "source": [
        "Now we will use LangChain's built-in chunking functionality to split the text into smaller chunks. LangChain offers a variety of text splitters that you can check out [here](https://api.python.langchain.com/en/latest/langchain_api_reference.html#module-langchain.text_splitter).\n",
        "Use the general-purpose splitter that splits text by recursively looking at characters. Use this class to split the text into 400 token-sized chunks, where the length of each chunk is computed based on the `token_len` function. The length is not the only criterion for splitting, if any of these separators `'\\n\\n', '\\n', ' ', ''` is encountered, we will have a new chunk.\n",
        "Since splitting only based on maximum length might result in incoherent chunks for every consecutive chunk, let the chunk overlap by 50 tokens. This way,  we preserve some of the previous context while chunking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0_RaaGMPPUd9",
      "metadata": {
        "id": "0_RaaGMPPUd9"
      },
      "outputs": [],
      "source": [
        "#### your code ####\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size =400,\n",
        "    length_function =token_len,\n",
        "    chunk_overlap =50,\n",
        "    separators=['\\n\\n', '\\n', ' ', '']\n",
        ")\n",
        "#### your code ####"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kS6uNIzpOD-I",
      "metadata": {
        "id": "kS6uNIzpOD-I"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "26"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chunks = text_splitter.split_text(docs[100].page_content)\n",
        "len(chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-r29RUJ7PUg6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "-r29RUJ7PUg6",
        "outputId": "eba37f81-550e-43dd-913d-041fdf163c57"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "316"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "token_len(chunks[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SGaeBXo8ONLj",
      "metadata": {
        "id": "SGaeBXo8ONLj"
      },
      "source": [
        "The next step is to apply the splitting function to all the documents in our corpus and to save our chunks in a logical way. We also want to assign a unique ID to each chunk so we know which part of the documentation they come from. In the end, the corpus should be transformed into a list of dictionaries of the following format:\n",
        "\n",
        "\n",
        "```\n",
        "[\n",
        "    {\n",
        "        \"id\": \"glossary-0\",\n",
        "        \"text\": \"first chunk of the document glossary\",\n",
        "        \"source\": \"https://langchain.readthedocs.io/en/latest/glossary.html\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"glossary-1\",\n",
        "        \"text\": \"second chunk of glossary\",\n",
        "        \"source\": \"https://langchain.readthedocs.io/en/latest/glossary.html\"\n",
        "    }\n",
        "    ...\n",
        "]\n",
        "```\n",
        "\n",
        "Construct the IDs by taking the name of the page before the suffix `.html` and appending a chronological number indicating which chunk it is.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pBsNJBUNPUjC",
      "metadata": {
        "id": "pBsNJBUNPUjC"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "52af9eefde2845bb9c39cc415ef59d40",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/417 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "6062"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "documents = []\n",
        "\n",
        "for doc in tqdm(docs):\n",
        "  #### your code ####\n",
        "    url = doc.metadata['source'].replace(\"docs/\", \"https://\")\n",
        "    uid = url.split('/')[-1].replace('.html', '')\n",
        "    chunks = text_splitter.split_text(doc.page_content)\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        documents.append({\n",
        "            \"id\": f\"{uid}-{i}\",\n",
        "            \"text\": chunk,\n",
        "            \"source\": url\n",
        "        })\n",
        "\n",
        "  #### your code ####\n",
        "len(documents) # once again this value might differ based on how the LangChain documentation is updated"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Hnx_s1bBQFQM",
      "metadata": {
        "id": "Hnx_s1bBQFQM"
      },
      "source": [
        "For the next steps, we require a `DataFrame`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "V64EmH9bLWNO",
      "metadata": {
        "id": "V64EmH9bLWNO"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>langchain_api_reference-0</td>\n",
              "      <td>langchain 0.1.3¶\\nlangchain.agents¶\\nAgent is ...</td>\n",
              "      <td>https://api.python.langchain.com/en/latest/lan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>langchain_api_reference-1</td>\n",
              "      <td>Base Single Action Agent class.\\nagents.agent....</td>\n",
              "      <td>https://api.python.langchain.com/en/latest/lan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>langchain_api_reference-2</td>\n",
              "      <td>[Deprecated]  Chat Agent.[Deprecated] Chat Age...</td>\n",
              "      <td>https://api.python.langchain.com/en/latest/lan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>langchain_api_reference-3</td>\n",
              "      <td>[Deprecated]  Agent for the MRKL chain.[Deprec...</td>\n",
              "      <td>https://api.python.langchain.com/en/latest/lan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>langchain_api_reference-4</td>\n",
              "      <td>Parses a message into agent action/finish.\\nag...</td>\n",
              "      <td>https://api.python.langchain.com/en/latest/lan...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                          id  \\\n",
              "0  langchain_api_reference-0   \n",
              "1  langchain_api_reference-1   \n",
              "2  langchain_api_reference-2   \n",
              "3  langchain_api_reference-3   \n",
              "4  langchain_api_reference-4   \n",
              "\n",
              "                                                text  \\\n",
              "0  langchain 0.1.3¶\\nlangchain.agents¶\\nAgent is ...   \n",
              "1  Base Single Action Agent class.\\nagents.agent....   \n",
              "2  [Deprecated]  Chat Agent.[Deprecated] Chat Age...   \n",
              "3  [Deprecated]  Agent for the MRKL chain.[Deprec...   \n",
              "4  Parses a message into agent action/finish.\\nag...   \n",
              "\n",
              "                                              source  \n",
              "0  https://api.python.langchain.com/en/latest/lan...  \n",
              "1  https://api.python.langchain.com/en/latest/lan...  \n",
              "2  https://api.python.langchain.com/en/latest/lan...  \n",
              "3  https://api.python.langchain.com/en/latest/lan...  \n",
              "4  https://api.python.langchain.com/en/latest/lan...  "
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "data = pd.DataFrame(documents)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CGbju_bkOQoL",
      "metadata": {
        "id": "CGbju_bkOQoL"
      },
      "source": [
        "#### ${\\color{red}{Comments\\ 1.1}}$\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
        "\n",
        "\n",
        "```\n",
        "✅ Point distribution ✅\n",
        "- 0.5 point if the documents are correctly loaded with `ReadTheDocsLoader`.\n",
        "    - 0.5 / 0.5 Points since the function is exactly the same\n",
        "- 0.5 point if the page content and URL is extracted from the `Document` object.\n",
        "    - 0.5 / 0.5 Points. The implementation is exact the one in the sample solution.\n",
        "- 0.5 point if the tokenizer is correctly initialized.\n",
        "    - 0.5 / 0.5 Points. The loading function accesses the environment variable directly rather than saving it to a variable beforehand, but the functionality is the same\n",
        "- 0.25 point if the `token_len` function is correct.\n",
        "    - 0.25 / 0.25 Points. The function is the same a inm the sample solution\n",
        "- 0.75 point to compute the minimum, maximum and average length of all documents.\n",
        "    - 0.75 / 0.75 Points. The actual values differ (probably due to a later point of execution) but the computation is correct. \n",
        "- 1 point for using the correct text splitter and using the correct parameters. For each mistake, deduct 0.25 point.\n",
        "    - 1/ 1 Points. The Splitter is initialized correctly and the application of it is also correct.\n",
        "- 1 point for converting the documents into chunks and list of dictionaries.\n",
        "    - 0.75 / 1 Points. The function is practically correct, but the source field in the dataframe is not as in the sample solution,as it still contains the https part and not the docs/ part\n",
        "\n",
        "Overall:\n",
        "\n",
        "4.25 / 4.5 Points\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ end⚠️}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VTrg2hMrO0vF",
      "metadata": {
        "id": "VTrg2hMrO0vF"
      },
      "source": [
        "## Subtask 1.2: Document Embedding Pipeline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "q_DSEsWIT9eI",
      "metadata": {
        "id": "q_DSEsWIT9eI"
      },
      "source": [
        "In this task, we initialize the embedding pipeline to transform the chunks into vector embeddings using Hugging Face and LangChain. These embeddings are used for similarity search between the query and the chunks to retrieve the most relevant chunks.\n",
        "  We will use the `sentence-transformers/all-MiniLM-L6-v2` model for embedding, which is a rather small model that you can easily run on Colab. Initialize the model using `HuggingFaceEmbeddings` to use Hugging Face via Langchain. The encoding batch size should be 32, and make sure that the model is placed on the correct device, otherwise, this can take a long time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "DYpEw-IKUTqK",
      "metadata": {
        "id": "DYpEw-IKUTqK"
      },
      "outputs": [],
      "source": [
        "from torch import cuda\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "import os\n",
        "import pinecone\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "HI3CJ5bbUTsT",
      "metadata": {
        "id": "HI3CJ5bbUTsT"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n"
          ]
        }
      ],
      "source": [
        "embedding_model = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "device = 'cuda:0'\n",
        "docs = [\n",
        "    \"An example document\",\n",
        "    \"A second document as an example\"\n",
        "]\n",
        "### your code ###\n",
        "embed_model = HuggingFaceEmbeddings(\n",
        "    model_name=embedding_model,\n",
        "    model_kwargs={'device': device},\n",
        "    encode_kwargs={'batch_size': 32}\n",
        ")\n",
        "### your code ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_U73Hj93V0_-",
      "metadata": {
        "id": "_U73Hj93V0_-"
      },
      "source": [
        "Embed the example documents using the model you created and check the output.\n",
        "The output should be a list of lists, containing the embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9Qt3SDKoUvKx",
      "metadata": {
        "id": "9Qt3SDKoUvKx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of docs: 2\n",
            "dimension of docs: 384\n"
          ]
        }
      ],
      "source": [
        "### your code ###\n",
        "embeddings = embed_model.embed_documents(docs)\n",
        "### your code ###\n",
        "print(\"number of docs:\",len(embeddings))\n",
        "print(\"dimension of docs:\",len(embeddings[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2D8sr-g9W8cw",
      "metadata": {
        "id": "2D8sr-g9W8cw"
      },
      "source": [
        "Now we use the embedding pipeline created above to store the embeddings in a Pinecone vector index. First, lets setup the Pinecone environment, collect your API key and environment name from the environment variables, and initiate Pinecone with them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tycv_q6RW8tO",
      "metadata": {
        "id": "tycv_q6RW8tO"
      },
      "outputs": [],
      "source": [
        "### your code ###\n",
        "pinecone = pinecone.Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\"))\n",
        "### your code ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_9tvVVQ7X2pV",
      "metadata": {
        "id": "_9tvVVQ7X2pV"
      },
      "source": [
        "Initialize the index `rag-assignment` inside Pinecone. Use the cosine similarity as similarity metric. Keep in mind that if you run this multiple times on a free tier, where only one index is allowed, you need to remove the index created to make room for a new one (Pinecone index gets archived automatically after 14 days of inactivity)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "g_ZA_2J5XyTD",
      "metadata": {
        "id": "g_ZA_2J5XyTD"
      },
      "outputs": [],
      "source": [
        "index_name = 'rag-assignment'\n",
        "### your code ###\n",
        "from pinecone import PodSpec\n",
        "\n",
        "if index_name not in pinecone.list_indexes().names():\n",
        "    pinecone.create_index(\n",
        "        name=index_name,\n",
        "        dimension=len(embeddings[0]),\n",
        "        metric='cosine',\n",
        "        spec=PodSpec(\n",
        "            environment=\"gcp-starter\"\n",
        "        )\n",
        "    )\n",
        "### your code ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rxfs9gshYfKx",
      "metadata": {
        "id": "rxfs9gshYfKx"
      },
      "source": [
        "Lets take a look at the index you created. As of now the index should be empty but have the correct embedding dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gNnN9mEyYWHB",
      "metadata": {
        "id": "gNnN9mEyYWHB"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'dimension': 384,\n",
              " 'index_fullness': 0.0,\n",
              " 'namespaces': {},\n",
              " 'total_vector_count': 0}"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "index_name = 'rag-assignment'\n",
        "index = pinecone.Index(index_name)\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "woQKBXfobZMg",
      "metadata": {
        "id": "woQKBXfobZMg"
      },
      "source": [
        "Process the dataset in batches of `32` and push the vectors to the Pinecone index. Your index should include the IDs and embeddings for each chunk. As metadata, pass the original text as `text` and the URL as `source` (no need to add the `https`). We use this metadata later to retrieve the original text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "K8vjCSVKa8Y6",
      "metadata": {
        "id": "K8vjCSVKa8Y6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 190/190 [01:30<00:00,  2.10it/s]\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "\n",
        "for i in tqdm(range(0, len(data), batch_size)):\n",
        "    ### your code ###\n",
        "    batch_docs = documents[i:i+batch_size]\n",
        "    ids = [doc['id'] for doc in batch_docs]\n",
        "    texts = [doc['text'] for doc in batch_docs]\n",
        "    embeds = embed_model.embed_documents(texts)\n",
        "    vectors = []\n",
        "    for id, embed, doc in zip(ids, embeds, batch_docs):\n",
        "        vector_data = {\n",
        "            \"id\": id,\n",
        "            \"values\": embed,  # Convert numpy array to list\n",
        "            \"metadata\": {\n",
        "                \"text\": doc['text'], \n",
        "                \"source\": doc['source'].replace('https://', '')\n",
        "            }\n",
        "        }\n",
        "        vectors.append(vector_data)\n",
        "    index.upsert(vectors=vectors)\n",
        "    ### your code ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kz2UZVj8ff_L",
      "metadata": {
        "id": "kz2UZVj8ff_L"
      },
      "source": [
        "Now if we look at the index statistics we should have vectors of dimension `384`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cH9Zq6azfgJm",
      "metadata": {
        "id": "cH9Zq6azfgJm"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'dimension': 384,\n",
              " 'index_fullness': 0.0601,\n",
              " 'namespaces': {'': {'vector_count': 6010}},\n",
              " 'total_vector_count': 6010}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d671bac4",
      "metadata": {},
      "source": [
        "'total_vector_count': 6010 less than len(documents): 6062 because of duplicates  \n",
        "Number of duplicate rows: 104\n",
        "Checked the data and found it was https://api.python.langchain.com/en/latest/langchain_api_reference.html duplicated.  \n",
        "This is split into 52 chunks, which means there are 52 duplicate items not in the index.  \n",
        "So 6010 is correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c75327d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of duplicate rows: 104\n"
          ]
        }
      ],
      "source": [
        "duplicates = data[data.duplicated(subset='id', keep=False)]\n",
        "print(f\"Number of duplicate rows: {len(duplicates)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NK-2Scv8qqYV",
      "metadata": {
        "id": "NK-2Scv8qqYV"
      },
      "source": [
        "#### ${\\color{red}{Comments\\ 1.2}}$\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
        "\n",
        "\n",
        "```\n",
        "✅ Point distribution ✅\n",
        "- 0.5 point if `HuggingFaceEmbeddings` is correclty initialized.\n",
        "    - 0.5 / 0.5 Points. It is correctly initialized, but the encode kwargs are missing the device option. This does not yield different results but might take longer in execution.\n",
        "- 0.5 point if examples are correctly processed with `HuggingFaceEmbeddings`.\n",
        "    - 0.5 / 0.5 Points. The examples yield the correct results.\n",
        "- 0.5 point if Pinecone is initialized with the enviroment variables.\n",
        "    - 0.25 / 0.5 Points. Pincone is initialized using another method and not using the environment variable for the environment name. \n",
        "- 0.5 if the Pinecone index is correctly created with the metric specified.\n",
        "    - 0.5 / 0.5 Points. Pinecone index is initiallized using the cosine similarity. \n",
        "- 1 point if the passages are converted to vectors and are pushed into Pinecone.\n",
        "    1 / 1 Points. It seems like the function does practically the same even though the list comprehension is defined a little bit different\n",
        "\n",
        "\n",
        "Overall:\n",
        "\n",
        "1.75 / 2 Points\n",
        "```\n",
        "\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ end⚠️}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bVrpkiJhHAF",
      "metadata": {
        "id": "5bVrpkiJhHAF"
      },
      "source": [
        "## Subtask 1.3: Text Generation Pipeline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZPykfVaHkIkO",
      "metadata": {
        "id": "ZPykfVaHkIkO"
      },
      "source": [
        "So far we have our index ready and a way to find the most similar chunks to our query. Now, we need a way to generate the answer from the retrieved chunks. For this purpose, we use the `text-generation` pipeline from Hugging Face (refer to the Hugging Face [tutorial](https://moodle.uni-heidelberg.de/pluginfile.php/1286642/mod_resource/content/1/HuggingFace.ipynb)) and load it into LangChain using a wrapper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "CL_PhWM3lEnj",
      "metadata": {
        "id": "CL_PhWM3lEnj"
      },
      "outputs": [],
      "source": [
        "from torch import cuda, bfloat16\n",
        "import os\n",
        "import transformers\n",
        "model_id = 'meta-llama/Llama-2-13b-chat-hf'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "A3l34T5T3eEN",
      "metadata": {
        "id": "A3l34T5T3eEN"
      },
      "source": [
        "Quantization techniques reduce memory and computational costs by representing weights and activations with lower-precision data types like 8-bit integers (int8). This enables loading larger models you normally wouldn’t be able to fit into memory, and thus speeds up inference.\n",
        "To make the process of model quantization more accessible, Hugging Face has seamlessly integrated with the [Bitsandbytes](https://huggingface.co/docs/accelerate/usage_guides/quantization) library.\n",
        "\n",
        "Define a config from `Bitsandbytes` that enables 4-bit quantization and set the nested quantization to `true`. This changes the datatype from float 32 (default) to normalized float 4 datatype to contain 4 bits of information.\n",
        "Additionally, add a compute type to store weights in 4-bits, but the computation to happen in 16-bit (bfloat16).\n",
        "Moreover, set the `bnb_4bit_use_double_quant` to true, which uses a second quantization after the first one to save an additional 0.4 bits per parameter.\n",
        "Refer to [here](https://huggingface.co/docs/transformers/main_classes/quantization) for more information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "mXNTODoOlFjX",
      "metadata": {
        "id": "mXNTODoOlFjX"
      },
      "outputs": [],
      "source": [
        "  ### your code ###\n",
        "bitsAndBites_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_compute_dtype=bfloat16,\n",
        "    bnb_4bit_use_double_quant=True)\n",
        "  ### your code ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Wea-kVMF4Kvf",
      "metadata": {
        "id": "Wea-kVMF4Kvf"
      },
      "source": [
        "Use your Hugging Face token to load the correct model configuration using the `transformers` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "7U47CyIk4EUz",
      "metadata": {
        "id": "7U47CyIk4EUz"
      },
      "outputs": [],
      "source": [
        "### your code ###\n",
        "model_config = transformers.AutoConfig.from_pretrained(model_id, token=os.getenv('HF_AUTH'))\n",
        "### your code ###\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V8CNl7G1SsUC",
      "metadata": {
        "id": "V8CNl7G1SsUC"
      },
      "source": [
        "Load the model for text generation (pay attention to the model type) using the configuration file you have defined, with the specified quantization, and set the `trust_remote_code` flag to `true`. Another flag that is useful for large mode is  `device_map=\"auto\"`. By setting this flag, Accelerate will determine where to put each layer to maximize the use of GPUs and offload the rest on the CPU, or even the hard drive if you don’t have enough GPU RAM (or CPU RAM).\n",
        "\n",
        "It will take a while for the model to download."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "TUzuGsv61cBA",
      "metadata": {
        "id": "TUzuGsv61cBA"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "70ffd653ce434bb8a848e7cbf5dcd274",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded \n"
          ]
        }
      ],
      "source": [
        "#Loading the model will take some time, (roughly 5 min)\n",
        "### your code ###\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    config=model_config,\n",
        "    quantization_config=bitsAndBites_config,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    token=os.getenv('HF_AUTH'))\n",
        "### your code ###\n",
        "model.eval()\n",
        "print(f\"Model loaded \")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-brElhysTZVZ",
      "metadata": {
        "id": "-brElhysTZVZ"
      },
      "source": [
        "You can even check the memory footprint of your model using the `get_memory_footprint` method.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zWDTgyKhlLLQ",
      "metadata": {
        "id": "zWDTgyKhlLLQ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7083970560"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.get_memory_footprint()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ViFoo0vlSFMp",
      "metadata": {
        "id": "ViFoo0vlSFMp"
      },
      "source": [
        "The next thing we need to do is initialize a `text-generation` pipeline with Hugging Face that uses the Llama2 model to generate some text, given some input. We will then use this pipeline inside LangChain to build our question-answering system.\n",
        "`text-generation` pipeline generates text from a language model conditioned on a given input. The pipeline is similar to other Hugging Face pipelines and requires two things that we must initialize:\n",
        "\n",
        "1.   A language model, in this case, it will be `meta-llama/Llama-2-13b-chat-hf`.\n",
        "2.   A tokenizer for the language model.\n",
        "\n",
        "LangChain expects the full-text outputs, therefore set the `return_full_text` to true. You can also pass additional generation parameters to the model.\n",
        "Since we want the questions to be answered mainly based on the retrieved chunks, let's set the model temperature to a low value of 0.01 to reduce randomness. Additionally, add a repetition penalty of 1.1 to stop the model from repeating itself and the maximum number of generation tokens to 512."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "1aFv-9-lPCJO",
      "metadata": {
        "id": "1aFv-9-lPCJO"
      },
      "outputs": [],
      "source": [
        "### your code ###\n",
        "generate_text = transformers.pipeline(\n",
        "    task='text-generation',\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=True,\n",
        "    temperature=0.01,\n",
        "    repetition_penalty=1.1,\n",
        "    max_new_tokens=512)\n",
        "\n",
        "### your code ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZvdSEw9PZ-n9",
      "metadata": {
        "id": "ZvdSEw9PZ-n9"
      },
      "source": [
        "We provide the language model a general question to make sure our pipeline is working correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FpuCkc77RF53",
      "metadata": {
        "id": "FpuCkc77RF53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'generated_text': 'Explain to me the difference between alligator and crocodile.\\nAlligators and crocodiles are both large, carnivorous reptiles that live in wetlands and rivers, but there are several key differences between them. Here are some of the main differences:\\n\\n1. Appearance: Alligators have a wider, rounder snout compared to crocodiles, which have a longer, thinner snout. Alligators also have a more rounded body shape and shorter legs than crocodiles.\\n2. Habitat: Alligators are found only in freshwater environments such as lakes, rivers, and swamps, while crocodiles can be found in both freshwater and saltwater environments.\\n3. Geographic range: Alligators are only found in the southeastern United States and China, while crocodiles are found in many parts of the world, including Africa, Asia, Australia, and the Americas.\\n4. Behavior: Alligators are generally less aggressive than crocodiles and tend to avoid confrontations with humans. Crocodiles, on the other hand, are known for their aggressive behavior and have been responsible for many human attacks and fatalities.\\n5. Nesting habits: Alligators build mounds of vegetation and mud to lay their eggs, while crocodiles dig holes in the sand or mud to lay their eggs.\\n6. Jaw structure: Alligators have a stronger bite force than crocodiles, but crocodiles have a more powerful jaw muscle that allows them to exert more force when biting.\\n7. Diet: Both alligators and crocodiles are carnivores, but alligators tend to eat more fish and smaller animals, while crocodiles prefer larger prey like buffalo, deer, and even humans.\\n8. Lifespan: Alligators typically live for 30-50 years in the wild, while crocodiles can live for up to 70 years in captivity.\\n\\nThese are just a few of the differences between alligators and crocodiles. While they may look similar at first glance, they are distinct species with unique characteristics and habits.'}]\n"
          ]
        }
      ],
      "source": [
        "sample_input=\"Explain to me the difference between alligator and crocodile.\"\n",
        "### your code ###\n",
        "generated_text=generate_text(sample_input)\n",
        "### your code ###\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2QtViqTBSsPS",
      "metadata": {
        "id": "2QtViqTBSsPS"
      },
      "source": [
        "Use the LangChain Hugging Face wrapper, as subset of [LLM chain](https://python.langchain.com/docs/modules/chains/foundational/llm_chain) to create an interface for the text generation pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "6-spabNmRMM2",
      "metadata": {
        "id": "6-spabNmRMM2"
      },
      "outputs": [],
      "source": [
        "### your code ###\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "llm = HuggingFacePipeline(pipeline=generate_text)\n",
        "### your code ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9EFZBGNgTM20",
      "metadata": {
        "id": "9EFZBGNgTM20"
      },
      "source": [
        "To confirm that it works the same way, use the sample input to generate text using the llm chain. The input should be passed as the `prompt` to the language model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rDToa1YOTFEX",
      "metadata": {
        "id": "rDToa1YOTFEX"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'\\nAlligators and crocodiles are both large, carnivorous reptiles that live in wetlands and rivers, but there are several key differences between them. Here are some of the main differences:\\n\\n1. Appearance: Alligators have a wider, rounder snout compared to crocodiles, which have a longer, thinner snout. Alligators also have a more rounded body shape and shorter legs than crocodiles.\\n2. Habitat: Alligators are found only in freshwater environments such as lakes, rivers, and swamps, while crocodiles can be found in both freshwater and saltwater environments.\\n3. Geographic range: Alligators are only found in the southeastern United States and China, while crocodiles are found in many parts of the world, including Africa, Asia, Australia, and the Americas.\\n4. Behavior: Alligators are generally less aggressive than crocodiles and tend to avoid confrontations with humans. Crocodiles, on the other hand, are known for their aggressive behavior and have been responsible for many human attacks and fatalities.\\n5. Nesting habits: Alligators build mounds of vegetation and mud to lay their eggs, while crocodiles dig holes in the sand or mud to lay their eggs.\\n6. Jaw structure: Alligators have a stronger bite force than crocodiles, but crocodiles have a more powerful jaw muscle that allows them to exert more force when biting.\\n7. Diet: Both alligators and crocodiles are carnivores, but alligators tend to eat more fish and smaller animals, while crocodiles prefer larger prey like buffalo, antelope, and even humans.\\n8. Lifespan: Alligators typically live for 30-50 years in the wild, while crocodiles can live for up to 70 years in captivity.\\n\\nThese are just a few of the differences between alligators and crocodiles. While they may look similar at first glance, they are distinct species with unique characteristics and habits.'"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "### your code ###\n",
        "llm(prompt=sample_input)\n",
        "### your code ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mn7-zQxeVhWF",
      "metadata": {
        "id": "mn7-zQxeVhWF"
      },
      "source": [
        "#### ${\\color{red}{Comments\\ 1.3}}$\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
        "\n",
        "\n",
        "```\n",
        "- 1 point the parameters for `bitsAndBites_config` are correct.\n",
        "- 0.5 point the configuration file is correctly initlized.\n",
        "- 1 point the generation model is correct.\n",
        "- 0.5 point question is answered using the Hugging Face pipeline.\n",
        "- 0.5 point the LLM chain containig the Hugging Face pipeline is correct.\n",
        "- 0.5 point question is answered using LLM chain.\n",
        "-----\n",
        "4 points\n",
        "```\n",
        "\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ end⚠️}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VRwTtnQEWq5h",
      "metadata": {
        "id": "VRwTtnQEWq5h"
      },
      "source": [
        "## Subtask 1.4: Question Answering Chain\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wr9QzQvYXDBS",
      "metadata": {
        "id": "wr9QzQvYXDBS"
      },
      "source": [
        "For Retrieval Augmented Generation (RAG) in LangChain, we need to initialize either a `RetrievalQA` or `RetrievalQAWithSourcesChain` object.\n",
        "\n",
        "`RetrievalQA` is a method for question-answering tasks, utilizing an index to retrieve relevant documents or text chunks, it is suitable for straightforward Q&A applications.\n",
        "\n",
        "`RetrievalQAWithSourcesChain` is an extension of RetrievalQA that chains together multiple sources of information, providing context and the source for answers.\n",
        "\n",
        " For both of these, we need an LLM and a Pinecone index. For LangChain to be able to use the Pinecone index, we need to initialize it through the LangChain vector store.\n",
        "\n",
        " **Hint**: You need to explicitly tell the vector storage where to find the original text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RQIR_Gb6Wrfs",
      "metadata": {
        "id": "RQIR_Gb6Wrfs"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain_community/vectorstores/pinecone.py:75: UserWarning: Passing in `embedding` as a Callable is deprecated. Please pass in an Embeddings object instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from langchain.vectorstores import Pinecone\n",
        "### your code ###\n",
        "vectorstore = Pinecone(index, embed_model.embed_query, \"text\")\n",
        "### your code ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mS9d51dyptJd",
      "metadata": {
        "id": "mS9d51dyptJd"
      },
      "source": [
        "Let's try a query that is specific to the LangChain documentation and see which chunks are relevant. Use the vector storage defined above to find the top-3 chunks related to the given query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sQeWNPDqXdJe",
      "metadata": {
        "id": "sQeWNPDqXdJe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content='langchain 0.1.3¶\\nlangchain.agents¶\\nAgent is a class that uses an LLM to choose a sequence of actions to take.\\nIn Chains, a sequence of actions is hardcoded. In Agents,\\na language model is used as a reasoning engine to determine which actions\\nto take and in which order.\\nAgents select and use Tools and Toolkits for actions.\\nClass hierarchy:\\nBaseSingleActionAgent --> LLMSingleActionAgent\\n                          OpenAIFunctionsAgent\\n                          XMLAgent\\n                          Agent --> <name>Agent  # Examples: ZeroShotAgent, ChatAgent\\nBaseMultiActionAgent  --> OpenAIMultiFunctionsAgent\\nMain helpers:\\nAgentType, AgentExecutor, AgentOutputParser, AgentExecutorIterator,\\nAgentAction, AgentFinish\\nClasses¶\\nagents.agent.Agent\\n[Deprecated]  Agent that calls the language model and deciding the action.\\nagents.agent.AgentExecutor\\nAgent that is using tools.\\nagents.agent.AgentOutputParser\\nBase class for parsing agent output into agent action/finish.\\nagents.agent.BaseMultiActionAgent\\nBase Multi Action Agent class.\\nagents.agent.BaseSingleActionAgent\\nBase Single Action Agent class.\\nagents.agent.ExceptionTool\\nTool that just returns the query.\\nagents.agent.LLMSingleActionAgent', metadata={'source': 'api.python.langchain.com/en/stable/langchain_api_reference.html'}),\n",
              " Document(page_content='langchain.agents.agent.Agent¶\\nclass langchain.agents.agent.Agent[source]¶\\nBases: BaseSingleActionAgent\\n[Deprecated]  Agent that calls the language model and deciding the action.\\nThis is driven by an LLMChain. The prompt in the LLMChain MUST include\\na variable called “agent_scratchpad” where the agent can put its\\nintermediary work.[Deprecated] Agent that calls the language model and deciding the action.\\nThis is driven by an LLMChain. The prompt in the LLMChain MUST include\\na variable called “agent_scratchpad” where the agent can put its\\nintermediary work.\\nNotes\\nDeprecated since version 0.1.0: Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\\nCreate a new model by parsing and validating input data from keyword arguments.\\nRaises ValidationError if the input data cannot be parsed to form a valid model.\\nparam allowed_tools: Optional[List[str]] = None¶\\nparam llm_chain: LLMChain [Required]¶\\nparam output_parser: AgentOutputParser [Required]¶\\nasync aplan(intermediate_steps: List[Tuple[AgentAction, str]], callbacks: Optional[Union[List[BaseCallbackHandler], BaseCallbackManager]] = None, **kwargs: Any) → Union[AgentAction, AgentFinish][source]¶', metadata={'source': 'api.python.langchain.com/en/stable/agents/langchain.agents.agent.Agent.html'}),\n",
              " Document(page_content='langchain.agents.loading.load_agent¶\\nlangchain.agents.loading.load_agent(path: Union[str, Path], **kwargs: Any) → Union[BaseSingleActionAgent, BaseMultiActionAgent][source]¶\\n[Deprecated]  Unified method for loading an agent from LangChainHub or local fs.\\nParameters\\npath – Path to the agent file.\\n**kwargs – Additional keyword arguments passed to the agent executor.\\npath – Path to the agent file.\\n**kwargs – Additional keyword arguments passed to the agent executor.\\nReturns\\nAn agent executor.[Deprecated] Unified method for loading an agent from LangChainHub or local fs.\\nReturns\\nAn agent executor.\\nNotes\\nDeprecated since version 0.1.0.', metadata={'source': 'api.python.langchain.com/en/stable/agents/langchain.agents.loading.load_agent.html'})]"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = 'what is a LangChain Agent?'\n",
        "### your code ###\n",
        "vectorstore.similarity_search(\n",
        "    query,  # our search query\n",
        "    k=3  # return 3 most relevant docs\n",
        ")\n",
        "### your code ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gEMooqALXqOt",
      "metadata": {
        "id": "gEMooqALXqOt"
      },
      "source": [
        "Now use the `vectorstore` and `llm` to initialize the `RetrievalQA` object, which showcases question answering over an index. `RetrievalQA` is a document chain, these are useful for summarizing documents, answering questions about documents, extracting information from documents, and more. All such chains operate with 4 different chain types:\n",
        "\n",
        "\n",
        "1.   `stuff`: it takes a list of documents, inserts them all into a prompt, and passes that prompt to an LLM.\n",
        "2.   `refine`: it constructs a response by looping over the input documents and iteratively updating its answer. It is well-suited for tasks that require analyzing more documents than can fit in the model’s context.\n",
        "3. `map_reduce`:  it first applies an LLM chain to each document individually (the Map step), treating the chain output as a new document. It then passes all the new documents to a separate combined documents chain to get a single output (the Reduce step).\n",
        "4. `map_re_rank`: it runs an initial prompt on each document that not only tries to complete a task but also gives a score for how certain it is in its answer. The highest-scoring response is returned.\n",
        "\n",
        "For this assignment, we focus only on the first type. Make sure to set the `verbose` to `true`, so we can see the different stages of processing that happens while answering a question (you might need to set this parameter more than once). As mentioned before, we want our retrieve to input top-5 most similiar chunks to the query to generate an answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lYnUrFnaXqU0",
      "metadata": {
        "id": "lYnUrFnaXqU0"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "### your code ###\n",
        "\n",
        "rag_pipeline = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vectorstore.as_retriever(k=5),\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "### your code ###\n",
        "query='what is a LangChain Agent?'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "O4Mo0JNMqSC7",
      "metadata": {
        "id": "O4Mo0JNMqSC7"
      },
      "source": [
        "First, we try to answer the question only using Llama2. As you see the answer is not convincing as it does not have access to the LangChain documentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jGk2ZsUVYiJ0",
      "metadata": {
        "id": "jGk2ZsUVYiJ0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n\\nA LangChain Agent is an intelligent agent that uses natural language processing (NLP) and machine learning (ML) techniques to assist users in finding relevant information on the web. It is designed to help users navigate the vast amount of information available online by providing personalized recommendations and answers to their questions.\\n\\nThe name \"LangChain\" refers to the idea of linking together different languages and knowledge sources to create a comprehensive and coherent view of the world. The agent is able to understand and respond to user queries in multiple languages, and it can draw upon a wide range of sources, including text, images, videos, and other forms of media, to provide accurate and relevant results.\\n\\nSome of the key features of a LangChain Agent include:\\n\\n1. Natural Language Processing (NLP): The agent is able to understand and interpret natural language queries, allowing users to ask questions in everyday language.\\n2. Machine Learning (ML): The agent uses ML algorithms to learn from user interactions and improve its performance over time.\\n3. Multi-lingual support: The agent can understand and respond to user queries in multiple languages, making it accessible to a global audience.\\n4. Knowledge graph integration: The agent can draw upon a knowledge graph to provide more detailed and accurate information about specific topics.\\n5. Personalization: The agent can provide personalized recommendations based on user preferences and interests.\\n\\nOverall, a LangChain Agent is designed to be a powerful tool for helping users find relevant information on the web, and it has the potential to revolutionize the way we interact with technology.'"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "W8U5Vce0qdch",
      "metadata": {
        "id": "W8U5Vce0qdch"
      },
      "source": [
        "Now use the Pipeline from above and see how the answer changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1KaQyXKQYn57",
      "metadata": {
        "id": "1KaQyXKQYn57"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "' A LangChain Agent is a piece of software that uses a Language Model (LLM) to perform tasks. It is a class that uses an LLM to choose a sequence of actions to take. In Chains, a sequence of actions is hardcoded. In Agents, a language model is used as a reasoning engine to determine which actions to take and in which order.'"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "### your code ###\n",
        "rag_pipeline.run(query)\n",
        "### your code ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pJ2JxOL4aqM2",
      "metadata": {
        "id": "pJ2JxOL4aqM2"
      },
      "source": [
        "#### ${\\color{red}{Comments\\ 1.4}}$\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
        "\n",
        "\n",
        "```\n",
        "- 0.5 point correctly initialize a vector store.\n",
        "- 1 point performing the top-3 similiarity search.\n",
        "- 1 point correct initialization of the `rag_pipeline` with all the parameters. If the verbose is not set correctly remove only 0.25.\n",
        "- 0.5 correct use of the pipeline to answer the question.\n",
        "-----\n",
        "3 points\n",
        "```\n",
        "\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ end⚠️}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "inIFkOziTce3",
      "metadata": {
        "id": "inIFkOziTce3"
      },
      "source": [
        "## Subtask 1.5: Conversational Retrieval Chain\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "btxcJlEITiDt",
      "metadata": {
        "id": "btxcJlEITiDt"
      },
      "source": [
        "We can also extend our retrieval chain to be able to remember the previous questions and answer the current question by looking at the previous context.\n",
        "The important part of a conversational model is conversation memory, which transforms the stateless language model to be able to remember previous interactions, e.g., similiar to ChatGPT. In this subtask, we will use LangChain to create a conversational memory.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DYHPVo-9TiGU",
      "metadata": {
        "id": "DYHPVo-9TiGU"
      },
      "source": [
        "To implement the memory we use `ConversationalRetrievalChain`.\n",
        "This chain takes in chat history (a list of messages) and new questions and then returns an answer to that question. The algorithm for this chain consists of three parts:\n",
        "\n",
        "1. Use the chat history and the new question to create a new question that contains the information from the previous context.\n",
        "\n",
        "2. This new question is passed to the retriever and relevant documents are returned.\n",
        "\n",
        "3. The retrieved documents are passed to an LLM to generate a final response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GLv_d-RYyNYL",
      "metadata": {
        "id": "GLv_d-RYyNYL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "langchain 0.1.3¶\n",
            "langchain.agents¶\n",
            "Agent is a class that uses an LLM to choose a sequence of actions to take.\n",
            "In Chains, a sequence of actions is hardcoded. In Agents,\n",
            "a language model is used as a reasoning engine to determine which actions\n",
            "to take and in which order.\n",
            "Agents select and use Tools and Toolkits for actions.\n",
            "Class hierarchy:\n",
            "BaseSingleActionAgent --> LLMSingleActionAgent\n",
            "                          OpenAIFunctionsAgent\n",
            "                          XMLAgent\n",
            "                          Agent --> <name>Agent  # Examples: ZeroShotAgent, ChatAgent\n",
            "BaseMultiActionAgent  --> OpenAIMultiFunctionsAgent\n",
            "Main helpers:\n",
            "AgentType, AgentExecutor, AgentOutputParser, AgentExecutorIterator,\n",
            "AgentAction, AgentFinish\n",
            "Classes¶\n",
            "agents.agent.Agent\n",
            "[Deprecated]  Agent that calls the language model and deciding the action.\n",
            "agents.agent.AgentExecutor\n",
            "Agent that is using tools.\n",
            "agents.agent.AgentOutputParser\n",
            "Base class for parsing agent output into agent action/finish.\n",
            "agents.agent.BaseMultiActionAgent\n",
            "Base Multi Action Agent class.\n",
            "agents.agent.BaseSingleActionAgent\n",
            "Base Single Action Agent class.\n",
            "agents.agent.ExceptionTool\n",
            "Tool that just returns the query.\n",
            "agents.agent.LLMSingleActionAgent\n",
            "\n",
            "langchain.agents.agent.Agent¶\n",
            "class langchain.agents.agent.Agent[source]¶\n",
            "Bases: BaseSingleActionAgent\n",
            "[Deprecated]  Agent that calls the language model and deciding the action.\n",
            "This is driven by an LLMChain. The prompt in the LLMChain MUST include\n",
            "a variable called “agent_scratchpad” where the agent can put its\n",
            "intermediary work.[Deprecated] Agent that calls the language model and deciding the action.\n",
            "This is driven by an LLMChain. The prompt in the LLMChain MUST include\n",
            "a variable called “agent_scratchpad” where the agent can put its\n",
            "intermediary work.\n",
            "Notes\n",
            "Deprecated since version 0.1.0: Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
            "Create a new model by parsing and validating input data from keyword arguments.\n",
            "Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
            "param allowed_tools: Optional[List[str]] = None¶\n",
            "param llm_chain: LLMChain [Required]¶\n",
            "param output_parser: AgentOutputParser [Required]¶\n",
            "async aplan(intermediate_steps: List[Tuple[AgentAction, str]], callbacks: Optional[Union[List[BaseCallbackHandler], BaseCallbackManager]] = None, **kwargs: Any) → Union[AgentAction, AgentFinish][source]¶\n",
            "\n",
            "langchain.agents.loading.load_agent¶\n",
            "langchain.agents.loading.load_agent(path: Union[str, Path], **kwargs: Any) → Union[BaseSingleActionAgent, BaseMultiActionAgent][source]¶\n",
            "[Deprecated]  Unified method for loading an agent from LangChainHub or local fs.\n",
            "Parameters\n",
            "path – Path to the agent file.\n",
            "**kwargs – Additional keyword arguments passed to the agent executor.\n",
            "path – Path to the agent file.\n",
            "**kwargs – Additional keyword arguments passed to the agent executor.\n",
            "Returns\n",
            "An agent executor.[Deprecated] Unified method for loading an agent from LangChainHub or local fs.\n",
            "Returns\n",
            "An agent executor.\n",
            "Notes\n",
            "Deprecated since version 0.1.0.\n",
            "\n",
            "langchain.agents.structured_chat.base.create_structured_chat_agent¶\n",
            "langchain.agents.structured_chat.base.create_structured_chat_agent(llm: BaseLanguageModel, tools: Sequence[BaseTool], prompt: ChatPromptTemplate) → Runnable[source]¶\n",
            "Create an agent aimed at supporting tools with multiple inputs.\n",
            "Parameters\n",
            "llm – LLM to use as the agent.\n",
            "tools – Tools this agent has access to.\n",
            "prompt – The prompt to use. See Prompt section below for more.\n",
            "Returns\n",
            "A Runnable sequence representing an agent. It takes as input all the same input\n",
            "variables as the prompt passed in does. It returns as output either an\n",
            "AgentAction or AgentFinish.\n",
            "Examples\n",
            "from langchain import hub\n",
            "from langchain_community.chat_models import ChatOpenAI\n",
            "from langchain.agents import AgentExecutor, create_structured_chat_agent\n",
            "prompt = hub.pull(\"hwchase17/structured-chat-agent\")\n",
            "model = ChatOpenAI()\n",
            "tools = ...\n",
            "agent = create_structured_chat_agent(model, tools, prompt)\n",
            "agent_executor = AgentExecutor(agent=agent, tools=tools)\n",
            "agent_executor.invoke({\"input\": \"hi\"})\n",
            "# Using with chat history\n",
            "from langchain_core.messages import AIMessage, HumanMessage\n",
            "agent_executor.invoke(\n",
            "\n",
            "Question: what is a LangChain Agent?\n",
            "Helpful Answer:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import ConversationalRetrievalChain\n",
        "chat_history = []\n",
        "\n",
        "### your code ###\n",
        "qa_conversation =  ConversationalRetrievalChain.from_llm(\n",
        "    llm,\n",
        "    vectorstore.as_retriever(k=5),\n",
        "    verbose=True\n",
        ")\n",
        "result = qa_conversation({\"question\": query, \"chat_history\": chat_history})\n",
        "### your code ###\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S6HxivGwyv2x",
      "metadata": {
        "id": "S6HxivGwyv2x"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' A LangChain Agent is a piece of software that uses a Language Model (LLM) to perform tasks. It is a class that inherits from the BaseSingleActionAgent or BaseMultiActionAgent classes, and it has a number of methods for selecting and using Tools and Toolkits to perform actions. The Agent selects and uses Tools and Toolkits for actions.'"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result[\"answer\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yd6tqKhg24dL",
      "metadata": {
        "id": "yd6tqKhg24dL"
      },
      "source": [
        "Change the chat history to contain the previous question and answer pair and ask a follow-up question.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36vj0R9Fyyd1",
      "metadata": {
        "id": "36vj0R9Fyyd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: what is a LangChain Agent?\n",
            "Assistant:  A LangChain Agent is a piece of software that uses a Language Model (LLM) to perform tasks. It is a class that inherits from the BaseSingleActionAgent or BaseMultiActionAgent classes, and it has a number of methods for selecting and using Tools and Toolkits to perform actions. The Agent selects and uses Tools and Toolkits for actions.\n",
            "Follow Up Input: What are tools and toolkits?\n",
            "Standalone question:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "langchain 0.1.3¶\n",
            "langchain.agents¶\n",
            "Agent is a class that uses an LLM to choose a sequence of actions to take.\n",
            "In Chains, a sequence of actions is hardcoded. In Agents,\n",
            "a language model is used as a reasoning engine to determine which actions\n",
            "to take and in which order.\n",
            "Agents select and use Tools and Toolkits for actions.\n",
            "Class hierarchy:\n",
            "BaseSingleActionAgent --> LLMSingleActionAgent\n",
            "                          OpenAIFunctionsAgent\n",
            "                          XMLAgent\n",
            "                          Agent --> <name>Agent  # Examples: ZeroShotAgent, ChatAgent\n",
            "BaseMultiActionAgent  --> OpenAIMultiFunctionsAgent\n",
            "Main helpers:\n",
            "AgentType, AgentExecutor, AgentOutputParser, AgentExecutorIterator,\n",
            "AgentAction, AgentFinish\n",
            "Classes¶\n",
            "agents.agent.Agent\n",
            "[Deprecated]  Agent that calls the language model and deciding the action.\n",
            "agents.agent.AgentExecutor\n",
            "Agent that is using tools.\n",
            "agents.agent.AgentOutputParser\n",
            "Base class for parsing agent output into agent action/finish.\n",
            "agents.agent.BaseMultiActionAgent\n",
            "Base Multi Action Agent class.\n",
            "agents.agent.BaseSingleActionAgent\n",
            "Base Single Action Agent class.\n",
            "agents.agent.ExceptionTool\n",
            "Tool that just returns the query.\n",
            "agents.agent.LLMSingleActionAgent\n",
            "\n",
            "langchain.agents.load_tools.get_all_tool_names¶\n",
            "langchain.agents.load_tools.get_all_tool_names() → List[str][source]¶\n",
            "Get a list of all possible tool names.\n",
            "\n",
            "langchain.agents.agent_toolkits.vectorstore.base.create_vectorstore_agent¶\n",
            "langchain.agents.agent_toolkits.vectorstore.base.create_vectorstore_agent(llm: BaseLanguageModel, toolkit: VectorStoreToolkit, callback_manager: Optional[BaseCallbackManager] = None, prefix: str = 'You are an agent designed to answer questions about sets of documents.\\nYou have access to tools for interacting with the documents, and the inputs to the tools are questions.\\nSometimes, you will be asked to provide sources for your questions, in which case you should use the appropriate tool to do so.\\nIf the question does not seem relevant to any of the tools provided, just return \"I don\\'t know\" as the answer.\\n', verbose: bool = False, agent_executor_kwargs: Optional[Dict[str, Any]] = None, **kwargs: Any) → AgentExecutor[source]¶\n",
            "Construct a VectorStore agent from an LLM and tools.\n",
            "Parameters\n",
            "llm (BaseLanguageModel) – LLM that will be used by the agent\n",
            "toolkit (VectorStoreToolkit) – Set of tools for the agent\n",
            "callback_manager (Optional[BaseCallbackManager], optional) – Object to handle the callback [ Defaults to None. ]\n",
            "prefix (str, optional) – The prefix prompt for the agent. If not provided uses default PREFIX.\n",
            "verbose (bool, optional) – If you want to see the content of the scratchpad. [ Defaults to False ]\n",
            "\n",
            "Convert a LangChain message to a dictionary.\n",
            "adapters.openai.convert_messages_for_finetuning(...)\n",
            "Convert messages to a list of lists of dictionaries for fine-tuning.\n",
            "adapters.openai.convert_openai_messages(messages)\n",
            "Convert dictionaries representing OpenAI messages to LangChain format.\n",
            "langchain_community.agent_toolkits¶\n",
            "Agent toolkits contain integrations with various resources and services.\n",
            "LangChain has a large ecosystem of integrations with various external resources\n",
            "like local and remote file systems, APIs and databases.\n",
            "These integrations allow developers to create versatile applications that combine the\n",
            "power of LLMs with the ability to access, interact with and manipulate external\n",
            "resources.\n",
            "When developing an application, developers should inspect the capabilities and\n",
            "permissions of the tools that underlie the given agent toolkit, and determine\n",
            "whether permissions of the given toolkit are appropriate for the application.\n",
            "See [Security](https://python.langchain.com/docs/security) for more information.\n",
            "Classes¶\n",
            "agent_toolkits.ainetwork.toolkit.AINetworkToolkit\n",
            "Toolkit for interacting with AINetwork Blockchain.\n",
            "agent_toolkits.amadeus.toolkit.AmadeusToolkit\n",
            "Toolkit for interacting with Amadeus which offers APIs for travel.\n",
            "agent_toolkits.azure_cognitive_services.AzureCognitiveServicesToolkit\n",
            "Toolkit for Azure Cognitive Services.\n",
            "\n",
            "Question:  What are tools and toolkits used for in a LangChain Agent?\n",
            "\n",
            "Please help me with this! Thank you!\n",
            "Helpful Answer:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "follow_up=\"What are tools and toolkits?\"\n",
        "### your code ###\n",
        "chat_history.append((query, result[\"answer\"]))\n",
        "result = qa_conversation({\"question\": follow_up, \"chat_history\": chat_history})\n",
        "### your code ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EvlY2MtH0xo3",
      "metadata": {
        "id": "EvlY2MtH0xo3"
      },
      "source": [
        "This is the previous context that was fed in alongside the new question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09zxWnojzKEu",
      "metadata": {
        "id": "09zxWnojzKEu"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('what is a LangChain Agent?',\n",
              "  ' A LangChain Agent is a piece of software that uses a Language Model (LLM) to perform tasks. It is a class that inherits from the BaseSingleActionAgent or BaseMultiActionAgent classes, and it has a number of methods for selecting and using Tools and Toolkits to perform actions. The Agent selects and uses Tools and Toolkits for actions.')]"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_history"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DSlQYeWG04Gj",
      "metadata": {
        "id": "DSlQYeWG04Gj"
      },
      "source": [
        "The current question is answered by knowing that the tools and toolkits are referring to a LangChain Agent, which was part of the previous question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5FP4_WAbzMSQ",
      "metadata": {
        "id": "5FP4_WAbzMSQ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nTools and toolkits are used to give the agent the capability to access, interact with, and manipulate external resources such as local and remote file systems, APIs, and databases.'"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result['answer']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LQUW_iFs3q3m",
      "metadata": {
        "id": "LQUW_iFs3q3m"
      },
      "source": [
        "#### ${\\color{red}{Comments\\ 1.5}}$\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
        "\n",
        "\n",
        "```\n",
        "- 1 point to correctly initialize the converstional chain.\n",
        "- 0.5 point for updating the history and asking a follow up question.\n",
        "---- \n",
        "1.5 points\n",
        "```\n",
        "\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ end⚠️}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0KDwOSuNPBe-",
      "metadata": {
        "id": "0KDwOSuNPBe-"
      },
      "source": [
        "## **Task 2: Advanced RAG Techniques and Evaluation (4 + 5 = 9 points)**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cN6pjlW7PBhQ",
      "metadata": {
        "id": "cN6pjlW7PBhQ"
      },
      "source": [
        "Now that you have successfully implemented your first RAG system, we dive into more advanced techniques and learn how to evaluate your methods using metrics you learned during the lecture. We focus on evaluation with an already annotated dataset. To this end, we load a small subset of [NarrativeQA](https://huggingface.co/datasets/narrativeqa), which is an English-language dataset of stories and corresponding questions designed to test reading comprehension, especially on long documents. We only load 30 samples from the data, as you will see in the upcoming sections, answer generation takes quite some time. In actual setting, it is advised to use a much larger set to obtain statistically significant results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "wM2dodC2PAlr",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "02df61aaed3948fea4a75f7489e31b30",
            "c49337d86bb04c81a519b1ef8e307e68",
            "73b3a406b17b4fc89ec0ab999b38ee30",
            "9c6cae46d48141b3bcd579303e41a328",
            "ee8f1ab0801048cf84e7e9073c1210da"
          ]
        },
        "id": "wM2dodC2PAlr",
        "outputId": "1a67506e-37fc-4495-f1fc-64d552bfcd7e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"satyaalmasian/narrativeqa_subset\",split=\"train[:30]\")\n",
        "len(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "N0HDF2M1fz4D",
      "metadata": {
        "id": "N0HDF2M1fz4D"
      },
      "source": [
        "Since we already used our free index in Pinecone for the previous task, we use Chroma, an open-source vector database, instead. As opposed to Pinecone, Chroma creates a collection on your machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "hZ8kqj95hqvo",
      "metadata": {
        "id": "hZ8kqj95hqvo"
      },
      "outputs": [],
      "source": [
        "from langchain.docstore.document import Document\n",
        "documents=[ doc[\"text\"] for doc in dataset[\"document\"]]\n",
        "questions=[quest for quest in dataset[\"question\"]]\n",
        "answers=[ans for ans in dataset[\"answers\"]]\n",
        "documents=list(set(documents))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "fj0X3PqHIFpQ",
      "metadata": {
        "id": "fj0X3PqHIFpQ"
      },
      "outputs": [],
      "source": [
        "docs= [Document(page_content=doc, metadata={\"source\": \"local\"}) for doc in documents]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3BDIuYQkJB0U",
      "metadata": {
        "id": "3BDIuYQkJB0U"
      },
      "source": [
        "The number of documents is smaller  than the number of questions and answers and each document is used as a reference for multiple questions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "FbWqRg12JLJn",
      "metadata": {
        "id": "FbWqRg12JLJn",
        "outputId": "879bd11d-a3d6-4d38-e3d2-bbb8b1239720"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n",
            "30\n"
          ]
        }
      ],
      "source": [
        "print(len(docs))\n",
        "print(len(questions))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cD69GYOjjJuk",
      "metadata": {
        "id": "cD69GYOjjJuk"
      },
      "source": [
        "### Subtask 2.1: Build Contextual Compression in LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "D-OyjUQGhGPq",
      "metadata": {
        "id": "D-OyjUQGhGPq"
      },
      "source": [
        "Let's split our documents using the TextSplitter from Task 1 and embed them inside the Chroma database with the embedding model of the previous task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "M7NGs-RzhQa7",
      "metadata": {
        "id": "M7NGs-RzhQa7"
      },
      "outputs": [],
      "source": [
        "### your code ###\n",
        "all_splits = text_splitter.split_documents(docs)\n",
        "### your code ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "hHknAFVBf5ia",
      "metadata": {
        "id": "hHknAFVBf5ia"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "### your code ###\n",
        "vectordb = Chroma.from_documents(all_splits, embed_model)\n",
        "retriever = vectordb.as_retriever()\n",
        "### your code ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "RbSBLyg364Ie",
      "metadata": {
        "id": "RbSBLyg364Ie"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fist question in the set: Why do more students tune into Mark's show?\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[Document(page_content=\"Reporter #2 - Are you on drugs?\\n\\nPaige - Arrrgh. Talk Hard. Arrrrrgh.\\n\\nMark - I've got a lot of homework I'm gonna take off alright.\\n\\nMarla - Mark I know why your really going home. It's because you wanna listen to that \\nshow tonight don't you?\\n\\n<Play Peter Murphy>\\n\\n<Nora goes to Marks house where she finds him burning his Happy Harry Hardon \\nletters>\\n\\nNora - Hi! What are you doing? You having fun?\\n\\nMark - Yeah.\\n\\nNora - Hey, look I took some of these off the wall for you. I mistakingly thought you \\nmight want them.\\n\\nMark - Thanks.\\n\\nNora - So I guess you're not going on tonight.\\n\\nMark - Brilliant.\\n\\nNora - Is this all just a game to you. You know you can't just shout fire in a theatre and \\nwalk out. You have a responsibility for the people who believe in you. What is this? \\nC'mon say something, say anything. Open your mouth and say get the hell out of here \\nbitch.\\n\\nMark - I can't.\\n\\nNora - You can't what?\\n\\nMark - I can't talk.\\n\\nNora - Sure you can talk.\\n\\nMark - I can't talk to you.\", metadata={'source': 'local'}),\n",
              " Document(page_content='Mark - What the hell is wrong.\\n\\nNora - I just got expelled.\\n\\nMark - What the hell are you talking about.\\n\\nNora - I\\'m failing Math.\\n\\nMark - They can\\'t kick you out for that.\\n\\nNora - I\\'ve been cutting lessons.\\n\\nMark - Well that just deserves a suspension right.\\n\\nNora - Well then I said \"Fuck You\" to Creswood. You should have seen her face, she was \\nso happy she said \"Thank You\"\\n\\nMark - This school sucks. Jesus Christ!\\n\\nNora - This is why I don\\'t even care anymore. Look just leave it alone. There\\'s nothing \\nyou can do about it. <Nora runs off>\\n\\nJan - Hunter! Hunter wait a minute. I just wanted to say good bye and good luck.\\n\\nMark - Why?\\n\\nJan - I was fired, I made a mistake. I thought I could change things, I forgot you don\\'t \\nrock the boat.\\n\\nMark - Yeah especially when you\\'re in it.\\n\\nJan - Hey, chin up.\\n\\n<Staff room>\\n\\nBrian - Loretta what the hell is going on here.\\n\\nCreswood - It\\'s the trouble makers, you can\\'t run a top school with trouble makers in the \\nmix.\\n\\nBrian - Okay, so what exactly is a trouble maker.', metadata={'source': 'local'}),\n",
              " Document(page_content=\"Mark - Dad it's not exactly the best school in the district. There are some problems with \\nit.\\n\\nBrian - You don't rock the boat especially when you're sitting in it. Any way we should \\nget going, I don't want to be late.\\n\\nMarla! - C'mon Mark it's your fathers big meeting.\\n\\n<The PTA. meeting>\\n\\nCreswood - Good evening on be half of myself and the staff at Hubert Humphrey High I \\nwish to thank you for turning out in such numbers, I congratulate you on your concern. \\nNow before we begin I would like to introduce our new school commissioner, fresh from \\nseveral educational triumphs on the east coast, Brian Hunter. Before I introduce the rest \\nof our speakers for this evening.\\n\\nPTA. Parent #1 - Excuse me Mrs Creswood, can we just skip the preliminaries and find \\nout what you're doing about all this.\\n\\nCreswood - Well when I introduce Mr Deaver he'll talk about our twenty four hour hot \\nline.\\n\\nPTA. Parent #1 - Wait a minute, the kids who need the most help like those with drug \\nproblems, they don't go in for all that.\\n\\nPTA. Parent #2 - I know kids. I mean they just wanna be happy.\\n\\nPTA. Parent #3 - Frankly, this radio person is the whole problem. Are we going to allow \\nthis guy to be heard by anyone who turns a dial.\", metadata={'source': 'local'}),\n",
              " Document(page_content=\"Mark - Just save it for the masses.\\n\\nBrian Hunter - Mark, they've got twelve hundred students down there. Surely some of \\nthem\\nhave gotta be cool.\\n\\nMark - Look the deal is I get decent grades and you guys leave me alone.\\n\\n<Back at Hupert Humphrey>\\n\\nJanie - Okay so who is this guy?\\n\\nNora - I don't know, nobody knows who he is, but he really hates this school so I guess \\nhe goes here.\\n\\nJanie - But all the guys that go here are geeks.\\n\\nNora - Maybe not my dear! Later\\n\\nJanie - Later?\\n\\n<English Class>\\n\\nJan Emerson - And so then the logi cars questioned the few remaining death spurs more \\nand more they began to fade away until there was nothing left of them and they \\ndisappeared from the face of the earth.......... Hmm, pretty good hey? Leading with your \\nheart, not your mind. I wondered if you would tell us what you were thinking when you \\nwrote this?\\n\\nMark - I just wrote it late last night.\\n\\nJan - That's obvious it's practically a night book. Mark, I was hoping you'd share your \\nfeelings about it. <Bell rings> Saved by the bell. Don't think If I didn't read your \\ncomposition it won't be read. Mark! We're looking for new writers for The Clarion. Don't \\nbe embarrassed of your talent.\", metadata={'source': 'local'})]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"Fist question in the set:\",questions[2]['text'])\n",
        "r_docs = retriever.get_relevant_documents(questions[2]['text'])\n",
        "r_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5YIitJ47Km7p",
      "metadata": {
        "id": "5YIitJ47Km7p"
      },
      "source": [
        "First, make a simple RAG pipeline that works on top of the Chroma retriever. This retriever should be similar to the previous task. However, since we want to use it for a large number of questions, remove the `verbose` parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "CTp5BOapKami",
      "metadata": {
        "id": "CTp5BOapKami"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "### your code ###\n",
        "rag_simple = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever\n",
        ")\n",
        "### your code ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "I8EzQFiNRja9",
      "metadata": {
        "id": "I8EzQFiNRja9"
      },
      "source": [
        "We look at an example question and compare the answer by RAG to the gold answer from the dataset. Note that the answers can contain multiple lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "afGjILYMKfCK",
      "metadata": {
        "id": "afGjILYMKfCK"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'query': \"Why do more students tune into Mark's show?\",\n",
              " 'result': \" Because he's not afraid to speak his mind and talk about real issues that matter to teens.\"}"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rag_simple(questions[2]['text']) #ignore the warning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "jvjAzg42R2WE",
      "metadata": {
        "id": "jvjAzg42R2WE"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'text': 'Mark talks about what goes on at school and in the community.',\n",
              "  'tokens': ['Mark',\n",
              "   'talks',\n",
              "   'about',\n",
              "   'what',\n",
              "   'goes',\n",
              "   'on',\n",
              "   'at',\n",
              "   'school',\n",
              "   'and',\n",
              "   'in',\n",
              "   'the',\n",
              "   'community',\n",
              "   '.']},\n",
              " {'text': 'Because he has a thing to say about what is happening at his school and the community.',\n",
              "  'tokens': ['Because',\n",
              "   'he',\n",
              "   'has',\n",
              "   'a',\n",
              "   'thing',\n",
              "   'to',\n",
              "   'say',\n",
              "   'about',\n",
              "   'what',\n",
              "   'is',\n",
              "   'happening',\n",
              "   'at',\n",
              "   'his',\n",
              "   'school',\n",
              "   'and',\n",
              "   'the',\n",
              "   'community',\n",
              "   '.']}]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "answers[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mNXD6PaUSlLE",
      "metadata": {
        "id": "mNXD6PaUSlLE"
      },
      "source": [
        "Apply the `rag_simple` pipeline to all the question in your corpus and accumulate the answers. **It should take around 10 minutes on a T4 GPU on Colab**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "uyTM0QD2KzwW",
      "metadata": {
        "id": "uyTM0QD2KzwW"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "simple_answers=[]\n",
        "### your code ###\n",
        "for question in questions:\n",
        "    simple_answers.append(rag_simple(question['text'])['result'])\n",
        "### your code ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0q3EbAw_hV5h",
      "metadata": {
        "id": "0q3EbAw_hV5h"
      },
      "source": [
        "Libraries such as LangChain and [Llamaindex](https://www.llamaindex.ai/) provide a variety of retrieval strategies for building a RAG system. In this subtask, you will use one of these variations called **contextual compression**. This method aims to extract only the relevant information from documents, reducing the need for expensive language model calls and improving response quality. Contextual compression consists of two parts:\n",
        "\n",
        "\n",
        "1.  **Base retriever:** retrieves the initial set of documents based on the query. This is similar to the retriever from the previous task.\n",
        "2.   **Document compressor:** processes these documents to extract the relevant content. We use `LLMChainExtractor`, which will iterate over the initially returned documents and extract from each only the content that is relevant to the query.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "RJ0UzPIVPAoc",
      "metadata": {
        "id": "RJ0UzPIVPAoc"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor,LLMChainFilter\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "### your code ###\n",
        "\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_retriever=retriever,\n",
        "    base_compressor=LLMChainExtractor.from_llm(llm),\n",
        ")\n",
        "### your code ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qMPaRwoeuvDX",
      "metadata": {
        "id": "qMPaRwoeuvDX"
      },
      "source": [
        "Let's take a look at an example of compression retriever works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "AK9xhW8JPAqo",
      "metadata": {
        "id": "AK9xhW8JPAqo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fist question in the set: Why do more students tune into Mark's show?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[Document(page_content='* \"Why do more students tune into Mark\\'s show?\"\\n* \"Mark\\'s show\"\\n* \"students\"', metadata={'source': 'local'}),\n",
              " Document(page_content='* Nora got expelled\\n* Nora has been cutting lessons\\n* Creswood is mentioned as a staff member', metadata={'source': 'local'}),\n",
              " Document(page_content='* \"the kids who need the most help like those with drug problems\"\\n* \"this radio person\"', metadata={'source': 'local'}),\n",
              " Document(page_content='* \"more students\"\\n* \"tune into Mark\\'s show\"\\n* \"decent grades\"\\n* \"leave me alone\"', metadata={'source': 'local'})]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"Fist question in the set:\",questions[2]['text'])\n",
        "compressed_docs = compression_retriever.get_relevant_documents(questions[2]['text'])\n",
        "compressed_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eWAPRPqCS3WP",
      "metadata": {
        "id": "eWAPRPqCS3WP"
      },
      "source": [
        "Look at the output and try out several different questions by yourself. Does the compressed output make sense?  \n",
        "- For QUESTION compression, I think it works. For example, as currently printed out, Mark\\'s show and students are duplicated. This should be good for retriever.\n",
        "- For document compression, the quality of compression varies. Some sentences effectively capture the main idea, while others lose key context.\n",
        "\n",
        "Compare this to the previous **simple** approach. Which one, in your opinion, is better?\n",
        "- Depends on the quality of the compression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1GVulJfSu3lt",
      "metadata": {
        "id": "1GVulJfSu3lt"
      },
      "source": [
        "Finally, we use the new retriever with the Llama2 model from the previous task to create the context compressor RAG pipeline. The code below should be similiar to what you did in the previous task. Once again, make sure to turn off the `verbose` argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "PKu2Dhr1PAtM",
      "metadata": {
        "id": "PKu2Dhr1PAtM"
      },
      "outputs": [],
      "source": [
        "### your code ###\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "rag_compressor = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=compression_retriever\n",
        ")\n",
        "### your code ###\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "DDtrO4z2pAD9",
      "metadata": {
        "id": "DDtrO4z2pAD9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'query': \"Why do more students tune into Mark's show?\",\n",
              " 'result': ' Based on the information provided, it seems that Mark\\'s show may be popular among students who are struggling with issues such as drug problems or poor academic performance. The mention of Nora getting expelled and cutting lessons suggests that these students may be seeking help or support from Mark\\'s show. Additionally, the phrase \"the kids who need the most help\" and the reference to \"those with drug problems\" suggest that Mark\\'s show may be providing a valuable resource for these students.'}"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rag_compressor(questions[2]['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8PfY6ya8z-1T",
      "metadata": {
        "id": "8PfY6ya8z-1T"
      },
      "source": [
        "Now we can use the pipeline to generate answers for all the questions in our dataset. **It should take around 20 minutes on a T4 GPU on Colab.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "eofiL90APAvl",
      "metadata": {
        "id": "eofiL90APAvl"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "compressor_answers=[]\n",
        "### your code ###\n",
        "for question in questions:\n",
        "    compressor_answers.append(rag_compressor(question['text'])['result'])\n",
        "### your code ###\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ck8Ktt6Zns5",
      "metadata": {
        "id": "6ck8Ktt6Zns5"
      },
      "source": [
        "#### ${\\color{red}{Comments\\ 2.1}}$\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
        "\n",
        "\n",
        "```\n",
        "- 0.5 point the text is correctly split.\n",
        "- 1 point initializing Chroma db as a retreiever and feeding the documents.\n",
        "- 0.5 point simple RAG pipline.\n",
        "- 0.25 point generating answers with simple RAG.\n",
        "- 1 point the correct compressor retriever.\n",
        "- 0.5 point compressor RAG pipline.\n",
        "- 0.25 point generating answers with compressor RAG.\n",
        "-----\n",
        "4 points\n",
        "```\n",
        "\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ end⚠️}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RlA3IDsn0Oxa",
      "metadata": {
        "id": "RlA3IDsn0Oxa"
      },
      "source": [
        "### Subtask 2.2. Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "E274c-Yc0gAr",
      "metadata": {
        "id": "E274c-Yc0gAr"
      },
      "source": [
        "Since we have access to ground truth answers, we can use various evaluation metrics from the literature. In this task, we explore three metrics:\n",
        "\n",
        "\n",
        "1.   **BLEU:** BLEU score stands for Bilingual Evaluation Understudy and is a precision-based metric developed\n",
        "for evaluating machine translation. BLEU scores a candidate by computing the\n",
        "number of n-grams in the candidate that also appear\n",
        "in a reference. The n can vary, in this task we compute for n=4.\n",
        "2.   **ROUGE:** ROUGE score stands for Recall-Oriented Understudy for Gisting Evaluation and is an F-measure metric designed for\n",
        "evaluating translation and summarization. There are a number of variants of ROUGE.\n",
        "3. **BERTScore:** BERTScore first obtains BERT representation of each word in the candidate and reference by feeding the candidate\n",
        "and reference through a BERT model separately.\n",
        "An alignment is then computed between candidate\n",
        "and reference words by computing pairwise cosine\n",
        "similarity. This alignment is then aggregated in to\n",
        "precision and recall scores before being aggregated\n",
        "into a (modified) F1 score that is weighted using\n",
        "inverse-document-frequency values.\n",
        "\n",
        "Luckily, Hugging Face has an implementation for all these metrics. Use the `evaluate` library to load the metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VT57xstfUtAL",
      "metadata": {
        "id": "VT57xstfUtAL"
      },
      "source": [
        "Use the loaded metrics to compare the RAG pipelines from the previous subtask."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "M4SU_Z4vPAxy",
      "metadata": {
        "id": "M4SU_Z4vPAxy"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "### your code ###\n",
        "bleu = evaluate.load('bleu')\n",
        "rouge = evaluate.load('rouge')\n",
        "bertscore = evaluate.load('bertscore')\n",
        "### your code ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ET3Ns8NSQ73F",
      "metadata": {
        "id": "ET3Ns8NSQ73F"
      },
      "source": [
        "As seen in the previous subtask, the answers can contain multiple lines. To be able to compare the output of our systems to the gold answers, merge the multiple answers into a single string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "6ITKpzWkQ_hJ",
      "metadata": {
        "id": "6ITKpzWkQ_hJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30\n"
          ]
        }
      ],
      "source": [
        "answers_merged=[]\n",
        "### your code ###\n",
        "for answer_list in answers:\n",
        "    # Join all texts in the answer list into a single string\n",
        "    merged_answer = ' '.join([ans['text'] for ans in answer_list])\n",
        "    answers_merged.append(merged_answer)\n",
        "\n",
        "### your code ###\n",
        "print(len(answers_merged))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IOlhI3iV1p9M",
      "metadata": {
        "id": "IOlhI3iV1p9M"
      },
      "source": [
        "Compute the BLUE score for the simple RAG and compressor RAG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "Z9PgmMfLPA0a",
      "metadata": {
        "id": "Z9PgmMfLPA0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Simple system:\n",
            "{'bleu': 0.0, 'precisions': [0.11329305135951662, 0.004746835443037975, 0.0016611295681063123, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 2.2364864864864864, 'translation_length': 662, 'reference_length': 296}\n",
            "Compressor:\n",
            "{'bleu': 0.0, 'precisions': [0.09887005649717515, 0.008849557522123894, 0.0030864197530864196, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 2.391891891891892, 'translation_length': 708, 'reference_length': 296}\n"
          ]
        }
      ],
      "source": [
        "### your code ###\n",
        "bleu_simple = bleu.compute(predictions=simple_answers, references=answers_merged)\n",
        "bleu_compressor = bleu.compute(predictions=compressor_answers, references=answers_merged)\n",
        "### your code ###\n",
        "print(\"Simple system:\")\n",
        "print(bleu_simple)\n",
        "print(\"Compressor:\")\n",
        "print(bleu_compressor)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BZI2_Jfqrgrc",
      "metadata": {
        "id": "BZI2_Jfqrgrc"
      },
      "source": [
        "What does the elements below in the output of the BLEU impelementation in Hugging Face mean? (do not copy and paste the documentation but write the implications behind each element!).\n",
        "\n",
        "\n",
        "\n",
        "1.   **precisions:** `your answer`\n",
        "2.   **brevity_penalty:** `your answer`\n",
        "3.   **translation_length:** `your answer`\n",
        "4.   **reference_length:** `your answer`\n",
        "5.   **length_ratio:** `your answer`\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "G-usgncEr7i9",
      "metadata": {
        "id": "G-usgncEr7i9"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "\n",
        "1.   **precisions:** This represents the precision scores for different n-gram lengths, usually ranging from 1 to 4. Precision in this context measures how many of the n-grams in the machine-translated text are also found in the reference translations. Higher precision scores indicate a greater overlap with the reference texts, suggesting better translation quality. However, precision alone can favor shorter translations, which is why BLEU also incorporates a brevity penalty.\n",
        "2.   **brevity_penalty:** The brevity penalty addresses the limitation of precision by penalizing overly short translations. If the machine-generated translation is shorter than the reference texts, the brevity penalty will be less than 1, reducing the overall BLEU score. This encourages translations to not only be accurate at the n-gram level but also to match the length of the reference texts more closely.\n",
        "3.   **translation_length:**   This is the total length of the machine-translated text. It's used to calculate the brevity penalty and to understand the scale of the translation task. In general, longer texts pose more challenges for maintaining consistency and accuracy throughout.\n",
        "4.   **reference_length:**  This refers to the length of the reference translations that the machine-generated text is being compared against. It's crucial for calculating the brevity penalty and for establishing a baseline for the length that a high-quality translation should achieve.\n",
        "5. **length_ratio:** This is the ratio of the translation length to the reference length. It provides a quick insight into how the length of the translated text compares to the reference. A ratio close to 1 indicates that the translation length closely matches the reference length, which is ideal in many contexts. Ratios significantly above or below 1 might suggest that the translation is respectively verbose or too concise, potentially affecting readability and fidelity to the original meaning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "Ot-jQzvqPA3J",
      "metadata": {
        "id": "Ot-jQzvqPA3J"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Simple system:\n",
            "{'rouge1': 0.11591846505131298, 'rouge2': 0.009312169312169312, 'rougeL': 0.10273551724919033, 'rougeLsum': 0.10339797821345781}\n",
            "Compressor:\n",
            "{'rouge1': 0.10965082465623792, 'rouge2': 0.014217006806238507, 'rougeL': 0.09819283588949361, 'rougeLsum': 0.09786033247272342}\n"
          ]
        }
      ],
      "source": [
        "### your code ###\n",
        "rouge_simple = rouge.compute(predictions=simple_answers, references=answers_merged)\n",
        "rouge_compressor = rouge.compute(predictions=compressor_answers, references=answers_merged)\n",
        "### your code ###\n",
        "print(\"Simple system:\")\n",
        "print(rouge_simple)\n",
        "print(\"Compressor:\")\n",
        "print(rouge_compressor)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IFE7RI-HsSGq",
      "metadata": {
        "id": "IFE7RI-HsSGq"
      },
      "source": [
        "What is the difference in variants of ROUGE (ROUGE-N, ROUGE-L, ROUGE-SUM)?\n",
        "\n",
        "`your answer`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IJmntC2_sfkx",
      "metadata": {
        "id": "IJmntC2_sfkx"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "ROUGE measures the similarity between the machine-generated summary and the reference summaries using overlapping n-grams, word sequences that appear in both the machine-generated summary and the reference summaries. The most common n-grams used are unigrams, bigrams, and trigrams. ROUGE score calculates the recall of n-grams in the machine-generated summary by comparing them to the reference summaries.\n",
        "\n",
        "**ROUGE-N:** This variant focuses on the overlap of n-grams (contiguous sequences of 'n' words) between the generated text and the reference texts. The most common is ROUGE-1 (for unigrams) and ROUGE-2 (for bigrams), which measure the overlap of single words and two-word phrases, respectively. ROUGE-N is useful for assessing the surface similarity of the generated text to the references, emphasizing the importance of choosing the right words and phrases.\n",
        "\n",
        "**ROUGE-L:**This variant measures the Longest Common Subsequence (LCS) between the generated text and the reference texts. Unlike ROUGE-N, which requires exact matches of n-gram sequences, ROUGE-L considers the longest sequence of words that appears in both the generated and reference texts in the same order, allowing for non-contiguous matches. This can better capture the fluency and the overall structure of the content, as it reflects the ability to preserve significant phrases even if the exact wording differs.\n",
        "\n",
        "**ROUGE-S:** This is an extension of ROUGE that is used specifically for evaluating the summary content by considering multiple reference summaries and aggregating their individual ROUGE scores. It's particularly useful in scenarios where there are multiple valid ways to summarize a given text, and it allows for a more comprehensive evaluation by taking into account the variability among reference summaries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "zSF5zMOyPA5I",
      "metadata": {
        "id": "zSF5zMOyPA5I"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Simple system:\n",
            "{'precision': 0.8405957301457723, 'recall': 0.8556291083494822, 'f1': 0.8479224026203156}\n",
            "Compressor:\n",
            "{'precision': 0.8360168476899464, 'recall': 0.851251111427943, 'f1': 0.8434217433134715}\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "bertscore_simple_averaged={}\n",
        "bertscore_compressor_averaged={}\n",
        "### your code ###\n",
        "results_simple = bertscore.compute(predictions=simple_answers, references=answers_merged, lang=\"en\")\n",
        "bertscore_simple_averaged['precision'] = np.mean(results_simple['precision'])\n",
        "bertscore_simple_averaged['recall'] = np.mean(results_simple['recall'])\n",
        "bertscore_simple_averaged['f1'] = np.mean(results_simple['f1'])\n",
        "results_compressor = bertscore.compute(predictions=compressor_answers, references=answers_merged, lang=\"en\")\n",
        "bertscore_compressor_averaged['precision'] = np.mean(results_compressor['precision'])\n",
        "bertscore_compressor_averaged['recall'] = np.mean(results_compressor['recall'])\n",
        "bertscore_compressor_averaged['f1'] = np.mean(results_compressor['f1'])\n",
        "### your code ###\n",
        "print(\"Simple system:\")\n",
        "print(bertscore_simple_averaged)\n",
        "print(\"Compressor:\")\n",
        "print(bertscore_compressor_averaged)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LfiuFxqQVrHa",
      "metadata": {
        "id": "LfiuFxqQVrHa"
      },
      "source": [
        "Which model works better?\n",
        "- In terms of results, the Simple system is a little better."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbIA8Cg3Zq6g",
      "metadata": {
        "id": "bbIA8Cg3Zq6g"
      },
      "source": [
        "#### ${\\color{red}{Comments\\ 2.2}}$\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
        "\n",
        "\n",
        "```\n",
        "- 0.5 point loading the metrics.\n",
        "- 0.5 point parsing the answers.\n",
        "- 0.5 point computation of BLEU.\n",
        "- 0.25 *5 = 1.25 points  meaning of each part of BLEU score.\n",
        "- 0.5 point computation of ROUGE.\n",
        "- 0.25 *3= 0.75 point  variants for ROUGE.\n",
        "- 1 point computation of BERTScore.\n",
        "-----\n",
        "3.5 points\n",
        "```\n",
        "\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ end⚠️}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a3d2023",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
